# Bright Run LoRA Training Data Platform - User Journey Document
**Version:** 1.0
**Date:** 12-15-2025
**Category:** LoRA Fine-Tuning Training Data Platform User Journey
**Product Abbreviation:** pipeline

**Source References:**
- Seed Story: `pmc/product/00-pipeline-seed-story.md`
- Overview Document: `pmc/product/01-pipeline-overview.md`
- User Stories: `pmc/product/02-pipeline-user-stories.md`

---

## Executive Summary

### Product Vision Alignment

The Bright Run LoRA Training Data Platform User Journey maps the complete experience of transforming subject matter expertise into production-ready AI training data. This journey enables users to create custom Llama 3 70B LoRA models that demonstrably improve emotional intelligence in financial advisory conversations, turning raw knowledge into proven AI solutions.

The user journey progresses through six distinct stages, each delivering measurable value while building toward the complete solution. From initial project setup through final model validation, users experience a guided workflow that requires no prior AI/technical expertise while producing enterprise-grade training datasets.

### Key User Personas Overview

| Persona | Primary Journey Stages | Key Goals |
|---------|----------------------|-----------|
| AI Engineer / Technical Lead | Stages 1-6 | Configure training, monitor progress, validate quality |
| Business Owner / Founder | Stages 1, 5-6 | Prove ROI, justify pricing, close premium deals |
| Quality Analyst / QA Team | Stages 4-6 | Validate model quality, ensure brand consistency |
| Budget Manager / Operations | Stages 1, 6 | Control costs, track spending, forecast budget |

### Journey Scope and Boundaries

**In Scope:**
- Training job configuration and initiation (one-click training)
- Real-time progress monitoring with loss curves and metrics
- Automatic checkpoint recovery for spot instances
- Model quality validation (perplexity, emotional intelligence, brand voice)
- Cost tracking and budget management
- Model artifact delivery and deployment packages

**Out of Scope (MVP):**
- User authentication and data isolation
- Multi-tenant access controls
- Encryption and security features
- Advanced hyperparameter tuning UI
- Multi-GPU distributed training
- Custom validation dataset uploads

### Success Definition

The user journey succeeds when:
- **95%+ training success rate** - Jobs complete on first or second attempt
- **40%+ emotional intelligence improvement** - Measurable quality gains vs baseline
- **±15% cost predictability** - Accurate estimates before job start
- **<10 minute configuration time** - From login to training start
- **12-hour average training time** - Efficient execution on H100 GPUs

### Value Progression Story for Proof-of-Concept

1. **Stage 1-2**: User establishes project context and ingests training content with immediate visibility into data quality
2. **Stage 3**: User explores and organizes knowledge structures, understanding dataset composition
3. **Stage 4**: User configures training with expert presets, receiving accurate cost/time estimates
4. **Stage 5**: User monitors training in real-time, with automatic recovery from interruptions
5. **Stage 6**: User validates quality with objective metrics and downloads proven, deployable models

---

## User Persona Definitions

### Persona 1: AI Engineer / Technical Lead

**Role and Responsibilities:**
- Configure training jobs (hyperparameters, GPU selection, cost estimation)
- Monitor training progress (loss curves, metrics, completion estimates)
- Handle failures (diagnose errors, retry with adjustments)
- Download and validate model artifacts
- Generate client-ready validation reports

**Technical Proficiency Level:** Intermediate - familiar with AI/ML concepts but not necessarily LoRA experts

**Goals and Motivations:**
- Reduce manual GPU setup time from 40 hours to <10 minutes
- Achieve 95%+ training success rate without intervention
- Gain weekend freedom through automated notifications
- Deliver proven models with measurable quality improvements

**Pain Points and Frustrations:**
- Manual Docker/CUDA/PyTorch setup complexity
- Zero visibility during 12-20 hour training runs
- OOM errors requiring guess-and-check debugging
- Spot instance interruptions losing all progress
- Cost unpredictability leading to surprise bills

**Success Criteria:**
- Configure and start training in <10 minutes
- Monitor progress with real-time loss curves
- Recover from errors with actionable guidance
- Download deployment-ready artifacts with confidence

**AI Knowledge Level:** Understands model training concepts; needs guidance on LoRA-specific parameters

---

### Persona 2: Business Owner / Founder

**Role and Responsibilities:**
- Price and sell trained model solutions (not just datasets)
- Demonstrate ROI to clients with validation reports
- Manage training budget and profitability
- Differentiate from competitors in sales conversations

**Technical Proficiency Level:** Business-focused - understands AI value, not implementation details

**Goals and Motivations:**
- Transform $5k dataset sales into $15k-30k model deliveries
- Win competitive bids with "proven 40% improvement" claims
- Close first $20k trained model deal within 8 weeks
- Maintain 80%+ profit margins on training services

**Pain Points and Frustrations:**
- Cannot prove dataset quality to skeptical clients
- Loses deals to "show me it works" objections
- Leaves 3-5x revenue on table selling raw datasets
- Budget anxiety from unpredictable training costs

**Success Criteria:**
- Access validation reports showing measurable improvements
- Quote clients with ±15% cost accuracy
- Share before/after comparisons with prospects
- Track ROI per client project

**AI Knowledge Level:** Conceptual understanding; needs executive-friendly explanations

---

### Persona 3: Quality Analyst / QA Team

**Role and Responsibilities:**
- Run validation benchmarks (perplexity, emotional intelligence, catastrophic forgetting)
- Compare baseline vs trained model outputs
- Evaluate brand voice alignment (personality consistency)
- Generate client-ready quality reports

**Technical Proficiency Level:** Intermediate - understands quality metrics, may not code

**Goals and Motivations:**
- Quantify quality improvements objectively
- Prevent regression or catastrophic forgetting
- Ensure brand voice consistency across outputs
- Build confidence in delivery quality

**Pain Points and Frustrations:**
- No systematic validation framework
- Subjective "this feels better" assessments
- Risk of delivering models that lost capabilities
- Cannot quantify brand voice alignment

**Success Criteria:**
- Run automated validation suites
- Review metrics dashboards at a glance
- Generate PDF reports for clients
- Approve/reject models based on thresholds

**AI Knowledge Level:** Understands evaluation metrics; needs clear quality rubrics

---

### Persona 4: Budget Manager / Operations

**Role and Responsibilities:**
- Control training costs and prevent overages
- Forecast monthly GPU spending
- Track costs per client/project
- Set and enforce budget limits

**Technical Proficiency Level:** Business/operations - focuses on financial metrics

**Goals and Motivations:**
- Prevent surprise GPU bills
- Maintain cost predictability
- Optimize spot vs on-demand usage
- Demonstrate training ROI

**Pain Points and Frustrations:**
- Unpredictable training costs
- No visibility into spending until bills arrive
- Difficulty attributing costs to projects
- Cannot forecast monthly capacity

**Success Criteria:**
- View monthly spending dashboards
- Receive alerts at 80%/95% budget thresholds
- Export cost reports for accounting
- Block jobs exceeding budget

**AI Knowledge Level:** Limited; focuses on cost/value metrics

---

## 1. Discovery & Project Initialization

```
STAGE 1: Discovery & Project Initialization
Purpose: Establish project context and prepare for training job creation
User Persona(s): AI Engineer (primary), Business Owner (secondary)
Entry Criteria: User has access to the platform with training file(s) available
Exit Criteria: Training job created with all configuration parameters set
Success Metrics: Job creation in <10 minutes; valid configuration with cost estimate
User Value Delivered: Clear understanding of project scope, costs, and timeline
Proof-of-Concept Demonstration: Shows intuitive one-click training setup
```

### 1.1 Training File Selection & Review

**UJ1.1.1: Browse Available Training Files**

* Description: I want to see all my available training files with key metadata so that I can select the right dataset for training.
* Impact Weighting: Operational Efficiency
* Priority: High
* User Stories: US1.1.1
* Tasks: [T-1.1.1]
* User Journey Acceptance Criteria:
  - GIVEN: I am on the training jobs page
  - WHEN: I click "Create New Training Job"
  - THEN: I see a dropdown listing all training files with name, conversation count, and creation date
  - AND: Each file shows quality score badge (High/Medium/Low based on scaffolding completion)

Technical Notes: Populated from `training_files` table with enrichment status filter
Data Requirements: Training file ID, name, conversation_count, quality_score, created_at
Error Scenarios: No training files available - display "Create your first training file" prompt
Performance Criteria: File list loads in <2 seconds
User Experience Notes: Files sorted by most recent first; search filter available

---

**UJ1.1.2: Review Training File Details**

* Description: I want to see detailed metadata about a training file before selecting it so that I understand what content will be used for training.
* Impact Weighting: Operational Efficiency
* Priority: High
* User Stories: US1.1.1
* Tasks: [T-1.1.2]
* User Journey Acceptance Criteria:
  - GIVEN: I am viewing the training file dropdown
  - WHEN: I click on a training file name
  - THEN: I see an expanded panel showing:
    - Total conversations: 242
    - Total training pairs: 1,567
    - Scaffolding distribution (personas, emotional arcs, topics)
    - Human review count and approval rate
    - Average empathy/clarity/appropriateness scores
  - AND: A "Select This File" button becomes available

Technical Notes: Metadata aggregated from training_file_conversations join
Data Requirements: Conversation distribution stats, quality metrics
Error Scenarios: File metadata unavailable - show warning with refresh option
Performance Criteria: Metadata panel expands in <500ms
User Experience Notes: Visual charts for scaffolding distribution; color-coded quality indicators

---

**UJ1.1.3: Confirm Training File Eligibility**

* Description: I want the system to validate that my selected training file meets minimum requirements so that I don't start training with insufficient data.
* Impact Weighting: Risk Mitigation
* Priority: High
* User Stories: US1.1.1
* Tasks: [T-1.1.3]
* User Journey Acceptance Criteria:
  - GIVEN: I have selected a training file
  - WHEN: The system validates eligibility
  - THEN: I see a green checkmark if the file meets requirements:
    - Minimum 50 conversations
    - All conversations enriched
    - Valid JSON format
    - Training pairs exist
  - AND: If requirements not met, I see specific guidance on what to fix

Technical Notes: Validation runs on file selection; results cached for 5 minutes
Data Requirements: Enrichment status, format validation, conversation count
Error Scenarios: File fails validation - display specific missing requirements
Performance Criteria: Validation completes in <1 second
User Experience Notes: Non-technical language for validation messages; direct links to fix issues

---

### 1.2 Hyperparameter Configuration

**UJ1.2.1: Select Training Preset**

* Description: I want to choose from proven configuration presets with clear explanations so that I don't need to understand complex hyperparameters.
* Impact Weighting: Ease of Use / Success Rate
* Priority: High
* User Stories: US1.1.2
* Tasks: [T-1.2.1]
* User Journey Acceptance Criteria:
  - GIVEN: I have selected a valid training file
  - WHEN: I proceed to configuration
  - THEN: I see three preset cards:
    - **Conservative**: "Best for first training runs" - 8-10 hours, ~$25-30
    - **Balanced**: "Production-ready for most cases" - 12-15 hours, ~$50-60
    - **Aggressive**: "Maximum quality" - 18-20 hours, ~$80-100
  - AND: Each card shows risk level and historical success rate
  - AND: Balanced preset is selected by default

Technical Notes: Presets defined in configuration constants; success rates from historical data
Data Requirements: Preset definitions (r, lr, epochs, batch_size), success rate history
Error Scenarios: N/A - presets always available
Performance Criteria: Instant selection response
User Experience Notes: Tooltip icons explain each hyperparameter in simple terms; "Learn more" link to documentation

---

**UJ1.2.2: Select GPU Instance Type**

* Description: I want to choose between cost-efficient spot instances and guaranteed on-demand instances so that I can balance cost vs reliability.
* Impact Weighting: Cost Efficiency / Risk Management
* Priority: High
* User Stories: US1.1.3
* Tasks: [T-1.2.2]
* User Journey Acceptance Criteria:
  - GIVEN: I have selected a training preset
  - WHEN: I view GPU options
  - THEN: I see two options clearly explained:
    - **Spot Instance** (recommended): $2.49/hr, 70% cheaper, 10-30% interruption risk, automatic recovery
    - **On-Demand Instance**: $7.99/hr, guaranteed completion, no interruptions
  - AND: Selecting spot shows "95%+ success rate with automatic checkpoint recovery"
  - AND: Cost estimate updates immediately when switching

Technical Notes: Prices from RunPod API; interruption rate from historical data
Data Requirements: GPU pricing, historical interruption statistics
Error Scenarios: RunPod pricing unavailable - show last known prices with notice
Performance Criteria: Cost estimate updates in <100ms
User Experience Notes: Visual comparison chart; recommendation badge on spot instance

---

**UJ1.2.3: View Real-Time Cost Estimate**

* Description: I want to see estimated cost and duration update as I change settings so that I understand the impact of my choices before committing.
* Impact Weighting: Cost Transparency / Budget Control
* Priority: High
* User Stories: US1.2.1
* Tasks: [T-1.2.3]
* User Journey Acceptance Criteria:
  - GIVEN: I am configuring a training job
  - WHEN: I change any configuration setting
  - THEN: The cost estimate panel updates within 1 second showing:
    - Estimated duration: 12-15 hours
    - Estimated cost: $50-60 (spot) or $120-140 (on-demand)
    - Accuracy disclaimer: "±15% based on historical data"
  - AND: A warning appears if cost exceeds $100

Technical Notes: Calculation: (dataset_size × epochs × time_per_epoch) × hourly_rate
Data Requirements: Dataset size, selected configuration, GPU pricing
Error Scenarios: Calculation error - show "Unable to estimate" with retry option
Performance Criteria: Updates in <1 second on any config change
User Experience Notes: Cost range displayed (min-max) to set expectations; visual indicator if within budget

---

### 1.3 Job Creation & Confirmation

**UJ1.3.1: Add Job Documentation**

* Description: I want to add a job name and notes so that I can remember my reasoning when reviewing results later.
* Impact Weighting: Organization / Knowledge Sharing
* Priority: Medium
* User Stories: US1.3.1
* Tasks: [T-1.3.1]
* User Journey Acceptance Criteria:
  - GIVEN: I have configured hyperparameters and GPU type
  - WHEN: I proceed to finalization
  - THEN: I see fields for:
    - Job name (auto-populated as "[File Name] - [Preset] - [Date]")
    - Description (optional, 500 characters)
    - Notes (optional, 2000 characters)
    - Tags (multi-select: experiment, production, client-delivery, test)
  - AND: All fields are editable before submission

Technical Notes: Job name must be unique within user's jobs
Data Requirements: Job metadata fields
Error Scenarios: Duplicate job name - auto-append timestamp
Performance Criteria: Form validation instant
User Experience Notes: Markdown supported in notes; tag suggestions based on history

---

**UJ1.3.2: Review Configuration Summary**

* Description: I want to review all settings before starting training so that I can catch mistakes before wasting budget.
* Impact Weighting: Risk Mitigation / Cost Control
* Priority: High
* User Stories: US1.3.2
* Tasks: [T-1.3.2]
* User Journey Acceptance Criteria:
  - GIVEN: I click "Review & Start Training"
  - WHEN: The confirmation modal appears
  - THEN: I see a complete summary:
    - Training file details (name, conversations, quality)
    - Configuration (preset, all hyperparameters)
    - GPU selection (type, pricing tier, hourly rate)
    - Cost estimate (duration, cost range, budget impact)
    - Warnings section (if any configuration concerns)
  - AND: A confirmation checklist appears:
    - [ ] I have reviewed the configuration
    - [ ] I understand the estimated cost
    - [ ] I have budget approval if required

Technical Notes: Checklist must be complete to enable "Start Training"
Data Requirements: All configuration values, budget status
Error Scenarios: Budget exceeded - block with clear message
Performance Criteria: Summary loads in <500ms
User Experience Notes: Clear visual hierarchy; warnings highlighted in yellow/red

---

**UJ1.3.3: Start Training Job**

* Description: I want to start training with one click after confirmation so that I can proceed without additional steps.
* Impact Weighting: Ease of Use / Productivity
* Priority: High
* User Stories: US1.3.2
* Tasks: [T-1.3.3]
* User Journey Acceptance Criteria:
  - GIVEN: I have completed the confirmation checklist
  - WHEN: I click "Start Training"
  - THEN: The system:
    - Creates job in database with status "queued"
    - Begins GPU provisioning immediately
    - Displays job ID and expected start time (within 5 minutes)
    - Redirects to job monitoring dashboard
  - AND: I receive confirmation: "Training job created! GPU provisioning in progress..."

Technical Notes: Async job creation; RunPod API called after database insert
Data Requirements: Complete job configuration object
Error Scenarios: GPU unavailable - queue job with auto-retry
Performance Criteria: Job created and redirect in <2 seconds
User Experience Notes: Success animation; clear next steps shown

---

## 2. Content Ingestion & Automated Processing

```
STAGE 2: Content Ingestion & Automated Processing
Purpose: Process training data and prepare for model training execution
User Persona(s): AI Engineer (primary), System (automated)
Entry Criteria: Training job created and GPU provisioning initiated
Exit Criteria: Data preprocessing complete, model loaded and ready for training
Success Metrics: Preprocessing in <5 minutes; model loading in <15 minutes
User Value Delivered: Confirmation that training setup is correct and proceeding
Proof-of-Concept Demonstration: Shows automated pipeline handling complex setup
```

### 2.1 GPU Provisioning Status

**UJ2.1.1: View GPU Provisioning Progress**

* Description: I want to see GPU provisioning status so that I know when training will actually begin.
* Impact Weighting: Transparency / User Confidence
* Priority: High
* User Stories: US2.1.1, US2.1.2
* Tasks: [T-2.1.1]
* User Journey Acceptance Criteria:
  - GIVEN: I am on the job monitoring dashboard
  - WHEN: GPU provisioning is in progress
  - THEN: I see a status card showing:
    - Current stage: "Provisioning GPU Instance"
    - Progress indicator (animated)
    - Expected completion: "~2-5 minutes"
    - GPU type selected (H100 PCIe 80GB)
    - Pricing tier (Spot or On-Demand)
  - AND: Status updates automatically without page refresh

Technical Notes: Polling every 10 seconds or WebSocket connection
Data Requirements: RunPod provisioning status, GPU details
Error Scenarios: Provisioning timeout - offer retry or on-demand switch
Performance Criteria: Status updates visible within 10 seconds of change
User Experience Notes: Animated spinner during provisioning; helpful "What's happening" tooltip

---

**UJ2.1.2: Handle GPU Unavailability**

* Description: I want clear guidance when no GPUs are available so that I can decide whether to wait or switch to on-demand.
* Impact Weighting: User Experience / Flexibility
* Priority: High
* User Stories: US3.1.3
* Tasks: [T-2.1.2]
* User Journey Acceptance Criteria:
  - GIVEN: GPU provisioning fails or times out
  - WHEN: No spot GPUs are available
  - THEN: I see an informative modal:
    - Problem: "No H100 spot instances currently available"
    - Reason: "High demand in RunPod datacenter"
    - Options presented:
      1. Auto-retry every 5 minutes (up to 1 hour)
      2. Switch to on-demand (+$5/hr, immediate start)
      3. Cancel and try later
    - Historical data: "Typically available within 15-30 minutes"
  - AND: Selection triggers appropriate action immediately

Technical Notes: Track provisioning attempts; escalate after 3 failures
Data Requirements: RunPod availability, historical wait times
Error Scenarios: Datacenter outage - show status page link
Performance Criteria: Modal appears within 2 seconds of failure
User Experience Notes: Non-alarmist language; clear recommendation highlighted

---

### 2.2 Data Preprocessing Stage

**UJ2.2.1: View Preprocessing Progress**

* Description: I want to see data preprocessing progress so that I know my training file is being prepared correctly.
* Impact Weighting: Transparency / Troubleshooting
* Priority: Medium
* User Stories: US2.1.2
* Tasks: [T-2.2.1]
* User Journey Acceptance Criteria:
  - GIVEN: GPU has been provisioned successfully
  - WHEN: Preprocessing stage begins
  - THEN: I see the stage indicator update to:
    - Current stage: "Preprocessing" (highlighted)
    - Status: "Loading and tokenizing 242 conversations..."
    - Progress: 0% → 100% as processing completes
    - Duration estimate: "2-5 minutes"
  - AND: Completion shows "Preprocessing complete - 1,567 training pairs ready"

Technical Notes: Webhook events from container update status
Data Requirements: Preprocessing step, conversation count, pair count
Error Scenarios: Format error - display specific conversation with issue
Performance Criteria: Status updates within 30 seconds of progress
User Experience Notes: Sub-steps visible (loading, tokenizing, splitting)

---

**UJ2.2.2: Handle Dataset Format Errors**

* Description: I want specific guidance when my training data has format issues so that I can fix problems quickly.
* Impact Weighting: Debugging / Time Savings
* Priority: High
* User Stories: US3.1.2
* Tasks: [T-2.2.2]
* User Journey Acceptance Criteria:
  - GIVEN: Preprocessing detects a data format error
  - WHEN: Validation fails on a specific conversation
  - THEN: I see an error modal showing:
    - Problem: "Training data validation failed"
    - Specific error: "Conversation #47 (ID: conv_abc123) is missing required field 'target_response'"
    - Data sample: JSON snippet with error highlighted
    - How to fix: Step-by-step instructions
    - Quick action: "Open Conversation Editor" button
  - AND: Job status updates to "failed" with retry option after fix

Technical Notes: Detailed validation in preprocessing script
Data Requirements: Conversation ID, field name, line number
Error Scenarios: Multiple errors - show first 5 with "and X more"
Performance Criteria: Error details available within 5 seconds of failure
User Experience Notes: Non-technical language; visual highlighting of problem area

---

### 2.3 Model Loading Stage

**UJ2.3.1: View Model Loading Progress**

* Description: I want to see model loading progress so that I understand this 10-15 minute stage is normal.
* Impact Weighting: User Confidence / Reduced Anxiety
* Priority: Medium
* User Stories: US2.1.2
* Tasks: [T-2.3.1]
* User Journey Acceptance Criteria:
  - GIVEN: Preprocessing has completed successfully
  - WHEN: Model loading stage begins
  - THEN: I see the stage indicator update to:
    - Current stage: "Model Loading" (highlighted)
    - Status: "Loading Llama 3 70B model with 4-bit quantization..."
    - Progress: Estimated 10-15 minutes remaining
    - Note: "First load may take longer; cached for future runs"
  - AND: Sub-steps visible:
    - Downloading model weights (if not cached)
    - Applying quantization
    - Initializing LoRA adapters

Technical Notes: Model pre-cached on persistent volume when possible
Data Requirements: Cache status, download progress
Error Scenarios: Download failure - retry with alternate mirror
Performance Criteria: Progress indicator updates every 30 seconds
User Experience Notes: Educational tooltip explaining why this takes time

---

**UJ2.3.2: Confirm Training Ready**

* Description: I want clear confirmation when training is about to begin so that I can set expectations for the main training phase.
* Impact Weighting: User Confidence / Transition Clarity
* Priority: Medium
* User Stories: US2.1.1
* Tasks: [T-2.3.2]
* User Journey Acceptance Criteria:
  - GIVEN: Model loading has completed
  - WHEN: System is ready to begin training
  - THEN: I see a transition notification:
    - "Ready to Train"
    - Training file confirmed: 242 conversations
    - Model loaded: Llama 3 70B (4-bit quantized)
    - Configuration: Balanced preset, 3 epochs
    - Estimated training time: 12-15 hours
    - Estimated completion: [Date/Time]
  - AND: Stage indicator advances to "Training" automatically

Technical Notes: 3-second display before auto-advancing
Data Requirements: Configuration summary, time estimates
Error Scenarios: N/A - stage complete
Performance Criteria: Notification appears immediately after model load
User Experience Notes: Celebratory visual cue; notification sent if user navigates away

---

## 3. Knowledge Exploration & Intelligent Organization

```
STAGE 3: Knowledge Exploration & Intelligent Organization
Purpose: Execute training and provide real-time visibility into progress
User Persona(s): AI Engineer (primary), Budget Manager (monitoring)
Entry Criteria: Model loaded and training begins
Exit Criteria: Training epochs complete or job stopped
Success Metrics: Loss decreasing; GPU utilization >80%; cost within estimate
User Value Delivered: Confidence that training is proceeding correctly
Proof-of-Concept Demonstration: Shows professional-grade monitoring dashboard
```

### 3.1 Real-Time Training Monitoring

**UJ3.1.1: View Live Training Dashboard**

* Description: I want to see a live dashboard with training progress so that I know training is proceeding correctly.
* Impact Weighting: User Confidence / Transparency
* Priority: High
* User Stories: US2.1.1
* Tasks: [T-3.1.1]
* User Journey Acceptance Criteria:
  - GIVEN: Training has begun
  - WHEN: I am on the job monitoring page
  - THEN: I see a comprehensive dashboard with:
    - Progress header: "42% complete (Step 850 of 2000)"
    - Current epoch: "Epoch 2 of 3"
    - Elapsed time: "6h 23m"
    - Estimated remaining: "8h 15m"
    - Current loss: "0.342 (↓ from 0.521 at start)"
  - AND: All values update automatically every 60 seconds

Technical Notes: Polling or WebSocket for real-time updates
Data Requirements: Step, epoch, loss, time metrics from webhook
Error Scenarios: Connection lost - show "Reconnecting..." with manual refresh
Performance Criteria: Updates visible within 60 seconds of progress
User Experience Notes: Green indicators for healthy progress; amber for slow

---

**UJ3.1.2: View Live Loss Curves**

* Description: I want to see loss curves updating in real-time so that I can verify training is converging properly.
* Impact Weighting: Transparency / Quality Confidence
* Priority: High
* User Stories: US2.1.1
* Tasks: [T-3.1.2]
* User Journey Acceptance Criteria:
  - GIVEN: I am viewing the training dashboard
  - WHEN: Training metrics are being reported
  - THEN: I see a live loss curve chart showing:
    - Training loss (blue line) - decreasing over steps
    - Validation loss (orange line) - tracked after each epoch
    - X-axis: Training step number
    - Y-axis: Loss value
    - Zoom controls for recent steps
  - AND: Chart adds new data points every 60 seconds
  - AND: "Export as PNG" button available for reports

Technical Notes: Chart.js or Recharts with streaming data
Data Requirements: Loss values by step from training_metrics_history
Error Scenarios: No validation loss yet - show "Validation after epoch 1"
Performance Criteria: Chart updates within 60 seconds of new data
User Experience Notes: Smooth line interpolation; trend arrow indicator

---

**UJ3.1.3: View Current Metrics Table**

* Description: I want to see detailed current metrics so that I can assess training health at a glance.
* Impact Weighting: Transparency / Debugging
* Priority: Medium
* User Stories: US2.1.1
* Tasks: [T-3.1.3]
* User Journey Acceptance Criteria:
  - GIVEN: Training is active
  - WHEN: I view the metrics section
  - THEN: I see a metrics table showing:
    - Training loss: 0.342 (trend arrow: ↓)
    - Validation loss: 0.398 (if available)
    - Learning rate: 0.000182
    - Perplexity: 18.2 (if calculated)
    - GPU utilization: 87%
    - GPU memory: 76.8 GB / 80 GB
  - AND: Each metric has a tooltip explaining what it means

Technical Notes: Metrics from latest webhook event
Data Requirements: All training metrics from GPU container
Error Scenarios: Metric unavailable - show "Calculating..."
Performance Criteria: Metrics table updates with dashboard refresh
User Experience Notes: Color coding (green/amber/red) for metric health

---

### 3.2 Cost Tracking During Training

**UJ3.2.1: View Real-Time Cost Accumulation**

* Description: I want to see current cost updating as training progresses so that I can cancel if costs exceed expectations.
* Impact Weighting: Cost Control / Budget Awareness
* Priority: High
* User Stories: US7.1.1
* Tasks: [T-3.2.1]
* User Journey Acceptance Criteria:
  - GIVEN: Training is active
  - WHEN: I view the cost tracker panel
  - THEN: I see:
    - Estimated cost: "$45-55"
    - Current spend: "$22.18 (49% of estimate)"
    - Hourly rate: "$2.49/hr (spot)"
    - Projected final cost: "$47.32"
  - AND: Values update every 60 seconds
  - AND: Visual indicator changes:
    - Green: <80% of estimate
    - Yellow: 80-100% of estimate
    - Red: >100% of estimate

Technical Notes: Cost = elapsed_time × hourly_rate + interruption_overhead
Data Requirements: Elapsed time, GPU rate, interruption count
Error Scenarios: Rate unavailable - use last known rate
Performance Criteria: Cost updates synchronized with progress updates
User Experience Notes: Clear "Cancel Job" button if concerned about cost

---

**UJ3.2.2: Receive Cost Warning Alerts**

* Description: I want alerts if training costs exceed estimates so that I can take action before overspending.
* Impact Weighting: Risk Mitigation / Cost Control
* Priority: High
* User Stories: US7.1.1, US7.2.2
* Tasks: [T-3.2.2]
* User Journey Acceptance Criteria:
  - GIVEN: Training is active and cost is being tracked
  - WHEN: Cost exceeds 80% of estimate
  - THEN: I see an in-app alert:
    - Warning: "Job approaching cost estimate ($36 of $45)"
    - Action options: "Continue monitoring" or "Cancel job"
  - AND: At 100% of estimate: "Job exceeded estimate ($46 of $45). Review status."
  - AND: At 120% of estimate: "Job significantly over budget. Consider cancelling."

Technical Notes: Alert thresholds configurable in settings
Data Requirements: Current cost, estimate, threshold percentages
Error Scenarios: Alert delivery failed - retry on next update
Performance Criteria: Alert appears within 60 seconds of threshold breach
User Experience Notes: Non-intrusive banner; persists until acknowledged

---

### 3.3 Training Stage Progress

**UJ3.3.1: View Training Stage Indicators**

* Description: I want to see which stage training is in so that I understand the overall progress flow.
* Impact Weighting: User Experience / Clarity
* Priority: Medium
* User Stories: US2.1.2
* Tasks: [T-3.3.1]
* User Journey Acceptance Criteria:
  - GIVEN: A training job is active
  - WHEN: I view the progress area
  - THEN: I see a 4-stage progress bar:
    1. Preprocessing (complete) - 2 min
    2. Model Loading (complete) - 12 min
    3. Training (active, 42%) - 6h 23m elapsed
    4. Finalization (pending) - ~10 min estimated
  - AND: Current stage is highlighted with animation
  - AND: Completed stages show checkmarks with actual duration

Technical Notes: Stage definitions in constants; durations from events
Data Requirements: Stage status, start/end timestamps
Error Scenarios: Stage stuck - show warning after 2x expected duration
Performance Criteria: Stage updates within 30 seconds of transition
User Experience Notes: Proportional progress (training ~85% of total bar)

---

**UJ3.3.2: View Webhook Event Log**

* Description: I want to see a log of all training events so that I can diagnose issues and understand what happened.
* Impact Weighting: Debugging / Troubleshooting
* Priority: Medium
* User Stories: US2.1.3
* Tasks: [T-3.3.2]
* User Journey Acceptance Criteria:
  - GIVEN: I am on the job details page
  - WHEN: I click the "Event Log" tab
  - THEN: I see a chronological list of events:
    - Timestamp: "2025-12-15 14:23:42"
    - Event type: Status | Metrics | Warning | Error
    - Message: "Training started (GPU: H100 PCIe 80GB spot)"
    - Expandable payload (JSON)
  - AND: Events are color-coded by type
  - AND: Filter dropdown for event types
  - AND: Search box for keywords

Technical Notes: Events from training_webhook_events table
Data Requirements: Event timestamp, type, payload
Error Scenarios: Many events - paginate at 50 per page
Performance Criteria: Log loads in <2 seconds
User Experience Notes: Most recent events first; auto-scroll option

---

## 4. Training Data Generation & Expert Customization

```
STAGE 4: Training Data Generation & Expert Customization
Purpose: Handle training interruptions, errors, and job control
User Persona(s): AI Engineer (primary)
Entry Criteria: Training in progress with active monitoring
Exit Criteria: Issues resolved or job controlled appropriately
Success Metrics: 95%+ recovery success; actionable error guidance
User Value Delivered: Confidence that problems will be handled gracefully
Proof-of-Concept Demonstration: Shows enterprise-grade error handling
```

### 4.1 Spot Instance Interruption Handling

**UJ4.1.1: Automatic Checkpoint Recovery**

* Description: I want training to automatically recover from spot instance interruptions so that I don't lose progress.
* Impact Weighting: Reliability / Cost Efficiency
* Priority: High
* User Stories: US3.2.1
* Tasks: [T-4.1.1]
* User Journey Acceptance Criteria:
  - GIVEN: Training is running on a spot instance
  - WHEN: Spot instance is interrupted
  - THEN: The system automatically:
    1. Detects interruption within 30 seconds
    2. Updates status to "Recovering..."
    3. Provisions new spot instance
    4. Downloads latest checkpoint (saved every 100 steps)
    5. Resumes training from last checkpoint
    6. Updates status back to "Training"
  - AND: I see a notification: "Training interrupted at step 850. Auto-recovering..."
  - AND: Recovery completes within 10 minutes

Technical Notes: Checkpoints stored in Supabase Storage every 100 steps
Data Requirements: Latest checkpoint path, step number
Error Scenarios: 3 failed recoveries - offer on-demand switch
Performance Criteria: Recovery in <10 minutes
User Experience Notes: Dashboard shows "Interrupted 2x, resumed successfully"

---

**UJ4.1.2: View Interruption History**

* Description: I want to see how many times training was interrupted so that I can assess reliability.
* Impact Weighting: Transparency / Confidence
* Priority: Medium
* User Stories: US3.2.1
* Tasks: [T-4.1.2]
* User Journey Acceptance Criteria:
  - GIVEN: Training has experienced spot interruptions
  - WHEN: I view the job dashboard
  - THEN: I see an interruption badge: "Interrupted 2x (auto-recovered)"
  - AND: Clicking shows details:
    - Interruption 1: Step 450, recovered at step 450, 8 min downtime
    - Interruption 2: Step 850, recovered at step 850, 7 min downtime
    - Total downtime: 15 minutes
    - Additional cost: $0.62 (recovery overhead)
  - AND: This data is included in final job summary

Technical Notes: Interruption events logged with timestamps
Data Requirements: Interruption count, recovery details, downtime
Error Scenarios: Recovery failed - show which attempt failed
Performance Criteria: History available immediately
User Experience Notes: Reassuring language emphasizing automatic recovery

---

### 4.2 Error Handling & Retry

**UJ4.2.1: Receive Actionable Error Messages**

* Description: I want clear error messages with specific fixes so that I can retry successfully without guessing.
* Impact Weighting: User Experience / Success Rate
* Priority: High
* User Stories: US3.1.1
* Tasks: [T-4.2.1]
* User Journey Acceptance Criteria:
  - GIVEN: Training fails with an error
  - WHEN: I view the error details
  - THEN: I see an error modal with:
    - Problem: "Your configuration exceeded GPU memory capacity"
    - Likely cause: "batch_size=4 with r=32 requires ~92GB VRAM (80GB available)"
    - Suggested fixes:
      1. Reduce batch_size to 2 (recommended)
      2. Switch to Conservative preset (r=8)
      3. Reduce sequence length
    - Quick retry button: "Retry with batch_size=2"
    - Link to documentation
  - AND: Error type categorized (OOM, Format, Timeout, Network)

Technical Notes: Error patterns matched against known issues
Data Requirements: Error message, stack trace, configuration context
Error Scenarios: Unknown error - show generic guidance with support link
Performance Criteria: Error modal appears within 5 seconds of failure
User Experience Notes: Non-technical language; visual diff showing suggested change

---

**UJ4.2.2: One-Click Retry with Adjustments**

* Description: I want to retry failed jobs with suggested fixes applied automatically so that I don't reconfigure from scratch.
* Impact Weighting: Productivity / Time Savings
* Priority: High
* User Stories: US3.3.1, US3.3.2
* Tasks: [T-4.2.2]
* User Journey Acceptance Criteria:
  - GIVEN: A job has failed with suggested fixes
  - WHEN: I click "Retry with Suggested Fix"
  - THEN: The system:
    - Creates new job with previous configuration
    - Auto-applies suggested adjustment (e.g., batch_size: 4 → 2)
    - Shows diff of what changed
    - Prompts for confirmation
    - Starts new training upon confirmation
  - AND: New job notes include: "Retry of job_abc123 with batch_size reduced"

Technical Notes: Link new job to original for reference
Data Requirements: Original configuration, suggested adjustments
Error Scenarios: Suggested fix invalid - show manual retry option
Performance Criteria: New job created in <2 seconds
User Experience Notes: Clear before/after comparison; easy cancel option

---

### 4.3 Job Control Actions

**UJ4.3.1: Cancel Active Training Job**

* Description: I want to cancel training if I notice problems so that I can stop wasting money on a bad run.
* Impact Weighting: User Control / Cost Control
* Priority: High
* User Stories: US2.2.1
* Tasks: [T-4.3.1]
* User Journey Acceptance Criteria:
  - GIVEN: Training is active
  - WHEN: I click "Cancel Job"
  - THEN: I see a confirmation modal:
    - Warning: "This will permanently stop training and terminate the GPU instance"
    - Current status: "42% complete, $22.18 spent"
    - Reason dropdown: Loss not decreasing, Too expensive, Wrong configuration, Testing, Other
    - Confirmation: "I understand this cannot be undone"
  - AND: After confirmation:
    - Job status updates to "cancelled"
    - GPU terminated within 60 seconds
    - Final cost calculated
    - Notification: "Job cancelled. Final cost: $22.18"

Technical Notes: Cancellation sent to RunPod API immediately
Data Requirements: Job ID, current metrics, cost
Error Scenarios: GPU already terminated - update status only
Performance Criteria: Cancellation acknowledged in <5 seconds
User Experience Notes: Destructive action styling (red); required confirmation

---

**UJ4.3.2: Resume from Checkpoint Manually**

* Description: I want to manually resume a failed job from its last checkpoint with adjusted settings so that I don't waste progress.
* Impact Weighting: Cost Efficiency / Flexibility
* Priority: Medium
* User Stories: US3.2.2
* Tasks: [T-4.3.2]
* User Journey Acceptance Criteria:
  - GIVEN: A job has failed but checkpoints are available
  - WHEN: I click "Resume from Checkpoint"
  - THEN: I see a configuration modal showing:
    - "Resume from Step 850 (42% complete)"
    - Previous configuration (editable)
    - Suggested adjustments (if failure was config-related)
    - Updated cost estimate for remaining work
    - Option to switch GPU type
  - AND: After confirmation:
    - New job created linked to original
    - Training resumes from checkpoint
    - Status: "Resumed from job_abc123"

Technical Notes: Checkpoint validation before resume
Data Requirements: Checkpoint path, original configuration, step number
Error Scenarios: Checkpoint corrupted - show error and manual retry option
Performance Criteria: Checkpoint validation in <30 seconds
User Experience Notes: Clear indication of progress preservation

---

## 5. Collaborative Quality Control & Final Validation

```
STAGE 5: Collaborative Quality Control & Final Validation
Purpose: Complete training and validate model quality
User Persona(s): AI Engineer (primary), Quality Analyst (validation)
Entry Criteria: Training epochs complete
Exit Criteria: Model validated and artifacts available for download
Success Metrics: 40%+ EI improvement; 30%+ perplexity improvement; 95%+ knowledge retention
User Value Delivered: Proven model quality with objective metrics
Proof-of-Concept Demonstration: Shows measurable ROI and quality proof
```

### 5.1 Training Completion

**UJ5.1.1: View Training Completion Summary**

* Description: I want to see a completion summary so that I understand training results at a glance.
* Impact Weighting: User Satisfaction / Quick Assessment
* Priority: High
* User Stories: US2.1.1
* Tasks: [T-5.1.1]
* User Journey Acceptance Criteria:
  - GIVEN: Training has completed successfully
  - WHEN: I view the job dashboard
  - THEN: I see a completion summary card:
    - Status: "Completed Successfully"
    - Duration: 13.2 hours
    - Total steps: 2,000
    - Final training loss: 0.287 (started at 1.423)
    - Final validation loss: 0.312
    - Improvement: "80% loss reduction"
    - Total cost: $48.32
    - Spot interruptions: 2 (auto-recovered)
  - AND: Green success banner with celebratory visual

Technical Notes: Summary from final webhook event
Data Requirements: All final metrics, cost, timing
Error Scenarios: N/A - completion confirmed
Performance Criteria: Summary available immediately on completion
User Experience Notes: Share button for team notification

---

**UJ5.1.2: Receive Completion Notification**

* Description: I want to be notified when training completes so that I don't need to monitor the dashboard constantly.
* Impact Weighting: Productivity / Work-Life Balance
* Priority: High
* User Stories: US8.2.1
* Tasks: [T-5.1.2]
* User Journey Acceptance Criteria:
  - GIVEN: Training has completed (success or failure)
  - WHEN: Completion is detected
  - THEN: I receive notifications based on preferences:
    - Email: "Training Job Completed: [Job Name]"
      - Configuration summary
      - Final metrics
      - Quick action buttons: View Details, Download Adapters
    - Slack (if configured): Posted to designated channel
    - In-app: Banner notification
  - AND: Failure notifications include error details and retry options

Technical Notes: Notification preferences stored per user
Data Requirements: Job summary, user notification preferences
Error Scenarios: Email delivery failed - retry 3 times
Performance Criteria: Notifications sent within 60 seconds of completion
User Experience Notes: Weekend freedom - no need to check dashboard

---

### 5.2 Quality Validation

**UJ5.2.1: View Perplexity Improvement**

* Description: I want to see perplexity comparison (baseline vs trained) so that I can objectively measure training success.
* Impact Weighting: Quality Assurance / Client Proof
* Priority: High
* User Stories: US6.1.1
* Tasks: [T-5.2.1]
* User Journey Acceptance Criteria:
  - GIVEN: Training has completed
  - WHEN: I view the validation section
  - THEN: I see perplexity metrics:
    - Baseline (Llama 3 70B): 24.5
    - Trained (with LoRA): 16.8
    - Improvement: **31.4%** (green badge)
    - Interpretation: "31% lower perplexity = significantly better predictions"
  - AND: Quality badge displayed:
    - "Production Ready" (≥30% improvement)
    - "Acceptable" (20-29%)
    - "Needs Improvement" (<20%)

Technical Notes: Perplexity calculated on 20% held-out validation set
Data Requirements: Baseline perplexity, trained perplexity
Error Scenarios: Calculation failed - show "Unable to calculate" with manual option
Performance Criteria: Results available within 15 minutes of training completion
User Experience Notes: Bar chart visualization; comparison explanation

---

**UJ5.2.2: View Emotional Intelligence Benchmark**

* Description: I want to see emotional intelligence improvement scores so that I can prove the 40%+ improvement we claim.
* Impact Weighting: Client Proof / Sales Enablement
* Priority: High
* User Stories: US6.2.1
* Tasks: [T-5.2.2]
* User Journey Acceptance Criteria:
  - GIVEN: EI benchmark has been run
  - WHEN: I view the validation section
  - THEN: I see emotional intelligence scores:
    - Overall EI Score: 3.2/5 (baseline) → 4.5/5 (trained) = **41% improvement**
    - Sub-scores:
      - Empathy: 3.1 → 4.6 (48% improvement)
      - Clarity: 3.4 → 4.5 (32% improvement)
      - Appropriateness: 3.1 → 4.4 (42% improvement)
  - AND: Quality badge: "Exceptional EI" (≥40%), "Strong EI" (30-39%), etc.

Technical Notes: 50 test scenarios with human or LLM-as-judge evaluation
Data Requirements: Baseline and trained scores per scenario
Error Scenarios: Some scenarios failed - show partial results with note
Performance Criteria: Results available within 30 minutes of training completion
User Experience Notes: Visual comparison chart; example responses shown

---

**UJ5.2.3: View Catastrophic Forgetting Check**

* Description: I want to verify the model retained its original knowledge so that I don't deliver a model that forgot basics.
* Impact Weighting: Risk Mitigation / Quality Assurance
* Priority: High
* User Stories: US6.3.1
* Tasks: [T-5.2.3]
* User Journey Acceptance Criteria:
  - GIVEN: Forgetting detection test has been run
  - WHEN: I view the validation section
  - THEN: I see retention metrics:
    - Baseline accuracy: 87% (on 100 financial questions)
    - Trained accuracy: 85%
    - Retention rate: **98%** (85/87 = 97.7%)
    - Verdict: "Passed" (≥95% retention)
  - AND: Quality gate status:
    - ✓ Passed (≥95%)
    - ⚠ Warning (90-94%)
    - ✗ Failed (<90%)
  - AND: List of any regressed questions if retention <100%

Technical Notes: 100 financial knowledge questions in test suite
Data Requirements: Question responses, accuracy scores
Error Scenarios: Test incomplete - show partial results
Performance Criteria: Results available within 20 minutes
User Experience Notes: Reassurance messaging; detailed breakdown available

---

**UJ5.2.4: View Brand Voice Consistency**

* Description: I want to verify the trained model maintains brand voice so that clients receive on-brand AI.
* Impact Weighting: Brand Alignment / Client Satisfaction
* Priority: Medium
* User Stories: US6.4.1
* Tasks: [T-5.2.4]
* User Journey Acceptance Criteria:
  - GIVEN: Brand voice evaluation has been run
  - WHEN: I view the validation section
  - THEN: I see voice consistency metrics:
    - Overall consistency: 4.3/5 (**86% alignment**)
    - Per-characteristic breakdown (10 characteristics):
      - Warmth & Empathy: 4.5/5
      - Directness: 4.2/5
      - Education-First: 4.1/5
      - ... (remaining characteristics)
  - AND: Quality badge: "Strong Brand Alignment" (≥85%)
  - AND: Any characteristics below 3/5 are flagged

Technical Notes: Elena Morales voice rubric with 10 characteristics
Data Requirements: 30 responses scored on 10 characteristics
Error Scenarios: Evaluation incomplete - show available results
Performance Criteria: Results available within 30 minutes
User Experience Notes: Visual radar chart; example responses for each characteristic

---

### 5.3 Validation Report Generation

**UJ5.3.1: Generate Validation Report PDF**

* Description: I want to generate a professional PDF report with all validation results so that I can share proof with clients.
* Impact Weighting: Client Communication / Sales Enablement
* Priority: High
* User Stories: US4.2.2, US3.1.1 (from seed story)
* Tasks: [T-5.3.1]
* User Journey Acceptance Criteria:
  - GIVEN: All validation metrics are available
  - WHEN: I click "Generate Validation Report"
  - THEN: The system generates a PDF including:
    - Cover page: Job name, date, Bright Run branding
    - Executive summary (1 page, non-technical)
    - Training metrics (loss curves, configuration)
    - Validation results (perplexity, EI, retention, voice)
    - Before/after examples (10 best improvements)
    - Cost breakdown
    - Appendix with technical details
  - AND: PDF is ready for download in <30 seconds
  - AND: Shareable link available (30-day expiration)

Technical Notes: PDF generated server-side with charts embedded
Data Requirements: All metrics, examples, configuration
Error Scenarios: Generation timeout - retry with simplified version
Performance Criteria: PDF generated in <30 seconds
User Experience Notes: Preview before download; customization options

---

**UJ5.3.2: Share Validation Results**

* Description: I want to share validation results with stakeholders so that they can review without platform access.
* Impact Weighting: Collaboration / Client Communication
* Priority: Medium
* User Stories: US8.1.2
* Tasks: [T-5.3.2]
* User Journey Acceptance Criteria:
  - GIVEN: Validation results are available
  - WHEN: I click "Share Results"
  - THEN: I see sharing options:
    - Generate public link (view-only, 30-day expiration)
    - Send via email (enter recipient addresses)
    - Post to Slack channel
    - Copy summary to clipboard
  - AND: Shared view shows read-only validation dashboard
  - AND: Tracked: who viewed, when, how many times

Technical Notes: Public links use signed tokens for security
Data Requirements: Shareable URL, recipient info
Error Scenarios: Email delivery failed - show error with retry
Performance Criteria: Link generated instantly
User Experience Notes: Tracking for engagement insights

---

## 6. Synthetic Data Expansion & Value Amplification

```
STAGE 6: Synthetic Data Expansion & Value Amplification
Purpose: Deliver trained model artifacts and enable production deployment
User Persona(s): AI Engineer (primary), Client Integration Team (secondary)
Entry Criteria: Model validated and approved for delivery
Exit Criteria: Artifacts downloaded and deployment package delivered
Success Metrics: All artifacts available; deployment package complete
User Value Delivered: Production-ready model with everything needed for deployment
Proof-of-Concept Demonstration: Shows complete end-to-end value delivery
```

### 6.1 Model Artifact Downloads

**UJ6.1.1: Download LoRA Adapters**

* Description: I want to download trained LoRA adapters so that I can integrate them into production inference pipelines.
* Impact Weighting: Productivity / Time-to-Value
* Priority: High
* User Stories: US4.1.1
* Tasks: [T-6.1.1]
* User Journey Acceptance Criteria:
  - GIVEN: Training has completed successfully
  - WHEN: I click "Download Adapters"
  - THEN: I receive a ZIP file containing:
    - adapter_model.bin (200-500MB) - trained LoRA weights
    - adapter_config.json - configuration for loading
    - README.txt - quick integration instructions
    - training_summary.json - final metrics snapshot
  - AND: Download starts within 2 seconds
  - AND: Progress indicator for large file downloads
  - AND: Signed URL expires in 24 hours (security)

Technical Notes: Files stored in Supabase Storage model-artifacts bucket
Data Requirements: Adapter files, signed URL generation
Error Scenarios: Storage unavailable - show retry option
Performance Criteria: Download initiation in <2 seconds
User Experience Notes: Clear file descriptions; download count tracked

---

**UJ6.1.2: Export Training Metrics**

* Description: I want to export training metrics data so that I can analyze performance or include in reports.
* Impact Weighting: Reporting / Analysis
* Priority: Medium
* User Stories: US4.2.1
* Tasks: [T-6.1.2]
* User Journey Acceptance Criteria:
  - GIVEN: Training has completed
  - WHEN: I click "Export Metrics"
  - THEN: I see format options:
    - CSV (spreadsheet analysis)
    - JSON (programmatic access)
  - AND: Export includes:
    - Step number, epoch, training_loss, validation_loss
    - Learning rate, perplexity, GPU utilization
    - Timestamps for all data points
  - AND: Download starts immediately (no generation delay)

Technical Notes: Data from training_metrics_history table
Data Requirements: All historical metrics for job
Error Scenarios: Large dataset - paginated export option
Performance Criteria: Export prepared in <5 seconds
User Experience Notes: Format selection with preview sample

---

### 6.2 Deployment Package

**UJ6.2.1: Download Complete Deployment Package**

* Description: I want a complete deployment package so that I (or my client) can deploy the model without additional setup.
* Impact Weighting: Client Success / Support Reduction
* Priority: High
* User Stories: US4.3.1
* Tasks: [T-6.2.1]
* User Journey Acceptance Criteria:
  - GIVEN: Training is complete and validated
  - WHEN: I click "Download Deployment Package"
  - THEN: I receive a ZIP containing:
    - adapters/ folder (adapter_model.bin, adapter_config.json)
    - inference.py (runnable Python script)
    - requirements.txt (exact dependencies)
    - README.md (setup and usage instructions)
    - example_prompts.json (10 test prompts)
    - training_summary.json (metrics and configuration)
  - AND: Package works with: `pip install -r requirements.txt && python inference.py "prompt"`

Technical Notes: Templates filled with job-specific values
Data Requirements: All artifacts, configuration, examples
Error Scenarios: Template error - fallback to basic package
Performance Criteria: Package generated in <30 seconds
User Experience Notes: Test command shown; troubleshooting section

---

**UJ6.2.2: View Deployment Instructions**

* Description: I want clear deployment instructions so that I can integrate the model into various environments.
* Impact Weighting: Client Success / Adoption
* Priority: Medium
* User Stories: US4.3.1
* Tasks: [T-6.2.2]
* User Journey Acceptance Criteria:
  - GIVEN: I have downloaded the deployment package
  - WHEN: I view the README or online documentation
  - THEN: I see instructions for:
    - Local testing: Quick start with example command
    - GPU requirements: VRAM needed, recommended hardware
    - Cloud deployment: AWS, GCP, Azure examples
    - Integration: Loading adapters into existing pipelines
    - Troubleshooting: Common issues and solutions
  - AND: Code examples are copy-paste ready

Technical Notes: Documentation generated per job configuration
Data Requirements: Model specifications, GPU requirements
Error Scenarios: N/A - static content
Performance Criteria: Instant access
User Experience Notes: Tabbed interface for different deployment targets

---

### 6.3 Cost & Budget Finalization

**UJ6.3.1: View Final Cost Summary**

* Description: I want to see the complete cost breakdown so that I can track project profitability.
* Impact Weighting: Financial Planning / Transparency
* Priority: High
* User Stories: US7.3.2
* Tasks: [T-6.3.1]
* User Journey Acceptance Criteria:
  - GIVEN: Training is complete
  - WHEN: I view the cost section
  - THEN: I see a complete breakdown:
    - GPU compute cost: $47.00 (18.9 hrs @ $2.49/hr)
    - Spot interruption overhead: $1.32 (2 interruptions)
    - Storage costs: $0.00 (included)
    - Total cost: **$48.32**
    - Estimate variance: +2% (within ±15% target)
  - AND: Comparison to estimate: "Est: $45-55, Actual: $48.32"
  - AND: Cost attributed to client/project if tagged

Technical Notes: Final calculation after GPU terminated
Data Requirements: Elapsed time, rates, interruption data
Error Scenarios: Billing delayed - show "pending final calculation"
Performance Criteria: Final cost within 5 minutes of completion
User Experience Notes: Clear margin calculation if client tagged

---

**UJ6.3.2: View Monthly Budget Impact**

* Description: I want to see how this job affected my monthly budget so that I can plan future training.
* Impact Weighting: Budget Planning / Capacity Planning
* Priority: Medium
* User Stories: US7.2.1
* Tasks: [T-6.3.2]
* User Journey Acceptance Criteria:
  - GIVEN: Training is complete
  - WHEN: I view the budget impact section
  - THEN: I see:
    - Month-to-date spending: $487.32
    - This job: $48.32 (10% of monthly total)
    - Remaining budget: $12.68
    - Monthly limit: $500.00
    - Jobs completed this month: 12
  - AND: Link to full budget dashboard

Technical Notes: Aggregated from all jobs in current month
Data Requirements: Monthly spending, budget limit
Error Scenarios: N/A - calculation only
Performance Criteria: Instant display
User Experience Notes: Visual budget gauge; forecast if more jobs planned

---

### 6.4 Training Comparison & Templates

**UJ6.4.1: Compare with Previous Training Runs**

* Description: I want to compare this training run with previous ones so that I can identify the best configuration.
* Impact Weighting: Optimization / Data-Driven Decisions
* Priority: Medium
* User Stories: US5.1.1
* Tasks: [T-6.4.1]
* User Journey Acceptance Criteria:
  - GIVEN: I have completed multiple training runs
  - WHEN: I click "Compare with Other Jobs"
  - THEN: I can select 2-4 jobs for comparison
  - AND: I see side-by-side:
    - Overlaid loss curves on same chart
    - Metrics table (loss, perplexity, cost, duration)
    - Configuration differences highlighted
    - Winner recommendation: "Job 2: Best quality/cost ratio"
  - AND: Export comparison as PDF

Technical Notes: Data from all selected jobs' metrics
Data Requirements: Historical metrics, configuration
Error Scenarios: Jobs not comparable - show warning
Performance Criteria: Comparison loads in <3 seconds
User Experience Notes: Clear best-performer highlighting

---

**UJ6.4.2: Save Configuration as Template**

* Description: I want to save this successful configuration so that I can replicate it for future training.
* Impact Weighting: Team Efficiency / Best Practices
* Priority: Low
* User Stories: US5.2.2
* Tasks: [T-6.4.2]
* User Journey Acceptance Criteria:
  - GIVEN: Training completed successfully with good quality
  - WHEN: I click "Save as Template"
  - THEN: I can enter:
    - Template name (e.g., "Production Financial Advisory - High Quality")
    - Description of when to use this template
    - Tags for categorization
    - Visibility: Private or Team
  - AND: Template saves all hyperparameters and GPU settings
  - AND: Template appears in library for future jobs

Technical Notes: Template stored in configuration_templates table
Data Requirements: Full configuration, metadata
Error Scenarios: Duplicate name - prompt for unique name
Performance Criteria: Saved in <1 second
User Experience Notes: Suggestion based on job success metrics

---

## Cross-Stage Integration

### User Journey Flow

The user journey progresses through six connected stages, each building on the previous:

```
Stage 1: Discovery & Project Initialization
    ↓ (Training job created with valid configuration)
Stage 2: Content Ingestion & Automated Processing
    ↓ (GPU provisioned, data preprocessed, model loaded)
Stage 3: Knowledge Exploration & Intelligent Organization
    ↓ (Training monitored, costs tracked, issues detected)
Stage 4: Training Data Generation & Expert Customization
    ↓ (Interruptions handled, errors resolved, job controlled)
Stage 5: Collaborative Quality Control & Final Validation
    ↓ (Model validated, quality proven, reports generated)
Stage 6: Synthetic Data Expansion & Value Amplification
    ↓ (Artifacts downloaded, deployment package delivered)
→ Complete: Proven, deployable LoRA model with measurable improvements
```

### Value Amplification

| Stage | User Value Delivered | Cumulative Value |
|-------|---------------------|------------------|
| 1 | Clear project scope and cost estimate | Configuration in <10 min |
| 2 | Automated setup confirmation | No manual GPU/Python setup |
| 3 | Real-time visibility and control | Confidence during training |
| 4 | Automatic error recovery | 95%+ success rate |
| 5 | Objective quality proof | 40%+ improvement demonstrated |
| 6 | Production-ready artifacts | Complete deployment package |

### Development Efficiency

The stage sequence optimizes development by:
1. **Frontend-first** - Stages 1, 3, 5, 6 are UI-focused
2. **Backend infrastructure** - Stage 2, 4 are service-focused
3. **GPU integration** - Stages 2-4 require RunPod integration
4. **Validation suite** - Stage 5 requires benchmark development

### Data Flow Between Stages

| From Stage | Data Passed | To Stage |
|------------|------------|----------|
| 1 → 2 | Training job configuration | GPU provisioning |
| 2 → 3 | Preprocessing status, model ready signal | Training monitoring |
| 3 → 4 | Progress metrics, alerts | Error handling |
| 4 → 5 | Completion signal, final metrics | Validation |
| 5 → 6 | Validated model, quality scores | Artifact delivery |

### Progressive Enhancement

Each stage prepares users for increasing complexity:
- **Stages 1-2**: No AI knowledge required; guided setup
- **Stage 3**: Introduces loss curves; tooltips explain concepts
- **Stage 4**: Explains errors in plain language; suggests fixes
- **Stage 5**: Quality metrics with interpretation guidance
- **Stage 6**: Deployment instructions for various skill levels

---

## Acceptance Criteria Inventory

### Priority Summary

| Priority | Count | Description |
|----------|-------|-------------|
| High | 28 | Critical for core functionality |
| Medium | 18 | Important for complete experience |
| Low | 4 | Future enhancement candidates |

### Acceptance Criteria by Stage

| Stage | UJ Items | High Priority | Medium Priority | Low Priority |
|-------|----------|---------------|-----------------|--------------|
| 1. Discovery & Initialization | 8 | 6 | 2 | 0 |
| 2. Content Ingestion | 5 | 3 | 2 | 0 |
| 3. Knowledge Exploration | 6 | 4 | 2 | 0 |
| 4. Training Customization | 6 | 4 | 2 | 0 |
| 5. Quality Validation | 7 | 6 | 1 | 0 |
| 6. Value Amplification | 8 | 5 | 3 | 2 |

### Technical Risk Assessment

| Risk Area | Risk Level | Mitigation |
|-----------|-----------|------------|
| RunPod API integration | Medium | Fallback to manual commands |
| Checkpoint recovery | Medium | Conservative checkpoint frequency |
| Perplexity calculation | Low | Proven algorithm |
| PDF generation | Low | Multiple library options |
| Real-time updates | Medium | Polling fallback from WebSocket |

### Non-Technical User Impact Assessment

| Stage | User Complexity | Cognitive Load | Guidance Provided |
|-------|----------------|----------------|-------------------|
| 1 | Low | Minimal | Presets with explanations |
| 2 | Very Low | None (automated) | Status indicators only |
| 3 | Medium | Moderate | Tooltips, trend arrows |
| 4 | Medium | Moderate | Actionable error messages |
| 5 | Low | Minimal | Plain-language metrics |
| 6 | Low | Minimal | Copy-paste instructions |

---

## Implementation Guidance

### Suggested Development Sequence

**Phase 1: Foundation (Weeks 1-2)**
- Database schema for training jobs, metrics, artifacts
- TrainingService API layer
- API routes for job CRUD
- Basic job creation UI (Stage 1)

**Phase 2: GPU Integration (Weeks 2-3)**
- RunPod Docker container
- Webhook progress reporting
- GPU provisioning flow (Stage 2)
- Real-time monitoring UI (Stage 3)

**Phase 3: Error Handling (Week 3-4)**
- Checkpoint recovery system (Stage 4)
- Error classification and messaging (Stage 4)
- Retry mechanisms

**Phase 4: Validation (Weeks 4-6)**
- Perplexity calculation (Stage 5)
- EI benchmark suite (Stage 5)
- Forgetting detection (Stage 5)
- Report generation (Stage 5)

**Phase 5: Delivery (Week 6)**
- Artifact download system (Stage 6)
- Deployment package generation (Stage 6)
- Cost finalization (Stage 6)

### MVP vs Enhanced Features

**MVP (Phase 1-5):**
- One-click training job creation
- Real-time progress monitoring
- Automatic checkpoint recovery
- Perplexity validation
- Adapter download
- Basic deployment package

**Enhanced (Future):**
- EI benchmark suite (50 scenarios)
- Brand voice consistency rubric
- Full validation report PDF
- Configuration templates
- Training comparison tools
- API inference endpoint template

### Integration Points and Dependencies

| Component | Depends On | Required By |
|-----------|-----------|-------------|
| Job Creation UI | Training files table | All subsequent stages |
| RunPod Container | Supabase Storage | Training monitoring |
| Webhook Handler | Job database | Progress UI |
| Checkpoint Recovery | Supabase Storage | Error handling |
| Validation Suite | Completed training | Report generation |
| Artifact Storage | Supabase buckets | Download features |

### Development Sequencing Rationale

1. **Database first** - All features need job storage
2. **API before UI** - Backend stability enables frontend development
3. **Happy path before errors** - Confirm core flow works
4. **Monitoring before recovery** - Visibility enables debugging
5. **Validation after training** - Requires trained models
6. **Polish after core** - Templates and comparison are enhancements

---

## Document Purpose

1. Map complete user experience through progressive stages
2. Provide granular acceptance criteria enabling functional requirements development
3. Maintain user-centric focus while supporting technical implementation
4. Enable clear understanding of user needs and desired outcomes
5. Support progressive development following user journey sequence

## User Journey Guidelines

1. Each element focuses on user experience and value delivery
2. Acceptance criteria maintain user perspective throughout
3. Technical requirements support user experience objectives
4. User terminology preserved while enabling technical implementation
5. User journey enables validation against user needs and satisfaction
