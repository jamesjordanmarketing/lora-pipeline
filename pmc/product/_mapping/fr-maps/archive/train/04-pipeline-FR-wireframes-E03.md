# LoRA Pipeline - Functional Requirements
**Version:** 2.0.0  
**Date:** 12/16/2025  
**Category:** Design System Platform
**Product Abbreviation:** pipeline

**Source References:**
- Seed Story: `pmc\product\00-pipeline-seed-story.md`
- Overview Document: `pmc\product\01-pipeline-overview.md`
- User Stories: `pmc\product\02-pipeline-user-stories.md`


## 3. Error Handling & Recovery

- **FR3.1.1:** Out of Memory Error Handling
  * Description: System shall implement intelligent Out of Memory error detection, diagnosis, and resolution guidance by monitoring training logs for OOM indicators, calculating VRAM requirements based on configuration parameters, providing specific actionable recommendations with confidence-rated suggestions, offering one-click retry with automatically adjusted configurations, tracking OOM patterns to refine preset recommendations, and linking to educational resources explaining VRAM optimization strategies while building user understanding of memory constraints and prevention techniques through clear, non-technical communication.
  * Impact Weighting: Success Rate / User Experience / Learning
  * Priority: High
  * User Stories: US3.1.1
  * User Journey: UJ4.1.1 (Encountering OOM Errors), UJ4.1.2 (Understanding and Resolving OOM)
  * Tasks: [T-3.1.1]
  * User Story Acceptance Criteria:
    - Detect "OutOfMemoryError" or "CUDA out of memory" in training logs
    - Job status updates to "failed" with error type "OOM"
    - Error modal displays:
    - **Problem**: "Your configuration exceeded the 80GB VRAM capacity of the H100 GPU"
    - **Likely cause**: "batch_size=4 with 242 conversations and r=32 requires ~92GB VRAM"
    - **Suggested fixes**:
      1. Reduce batch_size to 2 (recommended)
      2. Switch to Conservative preset (r=8 instead of r=32)
      3. Reduce sequence length (if conversations are very long)
    - **Quick retry**: Button "Retry with batch_size=2" pre-fills configuration with suggested fix
    - Link to documentation: "Understanding VRAM Usage in LoRA Training"
    - Track OOM error frequency per configuration to improve preset recommendations
    - Example error message: "OutOfMemoryError: Your dataset + batch_size=4 + r=32 exceeds 80GB VRAM. Try batch_size=2 (Conservative preset) or contact support if issue persists."
  * Functional Requirements Acceptance Criteria:
    - **OOM Detection**: Webhook handler monitors training_webhook_events for error payloads, Pattern matching on error_message field for keywords: "OutOfMemoryError", "CUDA out of memory", "torch.cuda.OutOfMemoryError", "RuntimeError: CUDA error: out of memory", Additional check: GPU memory metrics show utilization >95% before crash, Confidence scoring: High confidence OOM (error message match + high GPU mem), Medium confidence (error message match only), Low confidence (suspicious crash pattern but no explicit OOM message)
    - **Job Status Update**: On OOM detection: UPDATE training_jobs SET status = 'failed', error_type = 'OutOfMemoryError', error_message = {full_error_text}, error_occurred_at = NOW(), failed_at_step = {current_step}, failed_at_epoch = {current_epoch}; INSERT error_analysis (job_id, error_type, detected_patterns, vram_calculation, suggested_fixes, confidence_level, created_at)
    - **VRAM Calculation Engine**: Estimate VRAM requirements using formula: base_model_memory_gb = 70 (Llama 3 70B in 4-bit quantization ‚âà 35-40GB), lora_adapters_memory_gb = (rank √ó num_target_modules √ó model_hidden_size √ó 4 bytes) / 1e9 ‚âà 2-8GB depending on rank, optimizer_state_memory_gb = lora_adapters_memory_gb √ó 2 (Adam optimizer stores momentum + variance), batch_memory_gb = batch_size √ó avg_conversation_tokens √ó 2 bytes √ó 1.5 (activations overhead) / 1e9, gradient_memory_gb = lora_adapters_memory_gb, total_estimated_vram = base_model + lora_adapters + optimizer_state + batch_memory + gradient_memory; Example calculation: Aggressive preset (r=32, batch=1, 7 target modules): base=40GB, lora=8GB, optimizer=16GB, batch=4GB, gradient=8GB, total‚âà76GB (within 80GB); Problem config (r=32, batch=4, 7 modules): total‚âà94GB (exceeds 80GB)
    - **Error Modal Design**: Full-screen modal with prominent error icon (üö´ red circle), non-dismissable (must use action buttons), Header: "Training Failed: Out of Memory", Subheader: "Your configuration requires more GPU memory than available"
    - **Problem Statement Section**: Large text box with red border, Icon: ‚ö†Ô∏è, Title: "What Happened:", Explanation: "Your training configuration exceeded the 80GB VRAM capacity of the H100 GPU. The model ran out of memory at step {step} during {stage} stage."
    - **Likely Cause Analysis**: Calculated VRAM breakdown displayed: "Estimated VRAM Requirements:", Base model (Llama 3 70B, 4-bit): 40GB, LoRA adapters (rank={r}): {X}GB, Optimizer state: {Y}GB, Training batch (size={batch}): {Z}GB, Gradients & activations: {W}GB, **Total Estimated: {total}GB** (red highlight if >80GB), Available GPU VRAM: 80GB, **Shortage: {shortage}GB over capacity**, Root cause highlighted: "Primary issue: batch_size={batch} with rank={r} requires {X}GB more than available"
    - **Suggested Fixes Section** (ranked by effectiveness): **Fix #1 (Recommended - 95% success rate)**: "Reduce batch_size from {current} to {suggested}" (calculated: next power of 2 that brings total VRAM <75GB), Impact: "Will reduce VRAM usage by {X}GB, bringing total to {new_total}GB (‚úì within capacity)", Trade-off: "Training will take ~{percentage}% longer but will complete successfully", Confidence badge: "95% success rate for this fix with similar configurations"; **Fix #2 (Alternative - 98% success rate)**: "Switch to Conservative preset (rank=8 instead of {current_rank})", Impact: "Reduces LoRA memory from {X}GB to {Y}GB, total VRAM: {new_total}GB", Trade-off: "Lower model capacity, may reduce quality by ~5-10% but much safer", Confidence: "98% success rate - most reliable option"; **Fix #3 (If needed - 80% success rate)**: "Reduce maximum sequence length from {current} to {suggested}", Impact: "Truncates very long conversations (>{suggested} tokens), reduces batch memory by {X}GB", Trade-off: "May lose context from longest conversations (affects ~{percentage}% of dataset)", Confidence: "80% success - only if conversations are unusually long"; **Fix #4 (Last resort)**: "Use a smaller model or contact support for custom configuration", Not actionable via UI, link to support
    - **Quick Retry Buttons**: Each suggested fix has corresponding action button: "Retry with batch_size={suggested}" (primary action, green button), "Retry with Conservative Preset" (secondary, blue), "Manually Adjust Configuration" (tertiary, gray - opens config form with pre-filled suggestions), Button click behavior: Clones current job configuration, Applies suggested parameter changes, Shows diff modal: "Configuration Changes: batch_size: 4 ‚Üí 2 (recommended fix for OOM)", User confirms: "Apply Changes & Retry Training", Creates new job with updated config, Original failed job remains visible in history as reference
    - **Educational Resources Section**: "Learn More" expandable section, Concise explanation: "Why did this happen?" "GPU VRAM (Video Memory) stores the model, training data, and calculations during training. Larger models, higher LoRA rank, and bigger batch sizes require more VRAM. The H100 GPU has 80GB capacity. Your configuration needed ~{X}GB.", Visual diagram: Simple bar chart showing VRAM breakdown (base model, LoRA, optimizer, batch, total with 80GB limit line), Link to documentation: "Understanding VRAM Requirements in LoRA Training" (opens in new tab), FAQ shortcuts: "How do I prevent OOM?", "What's the optimal batch_size?", "Should I use Conservative preset?"
    - **OOM Pattern Tracking**: System stores OOM events in error_analytics table: INSERT error_analytics (error_type, job_id, configuration_snapshot, vram_estimated, suggested_fix_applied, resolution_successful, user_id, timestamp); Aggregate analysis queries: SELECT preset, AVG(rank), AVG(batch_size), COUNT(*) as oom_count FROM error_analytics WHERE error_type = 'OOM' GROUP BY preset ORDER BY oom_count DESC; Identify high-risk configurations: "Aggressive preset with batch_size=4 has 42% OOM rate - recommend batch_size=2"; Refine preset recommendations: If >20% OOM rate for a preset/dataset size combination, update preset defaults: ALTER aggressive_preset SET default_batch_size = 2 WHERE conversation_count > 200
    - **Preset Recommendation Updates**: Quarterly review of OOM analytics, Adjust preset hyperparameters to reduce OOM rate, Target: <5% OOM rate across all presets, Example adjustments: "Aggressive preset: reduce default batch_size from 4 to 2 for datasets >200 conversations (reduced OOM rate from 18% to 3%)", Changelog: "Updated 2025-Q1: Aggressive preset batch size reduced based on OOM analysis"
    - **Proactive OOM Prevention**: During job configuration, before job starts, Calculate estimated VRAM using same formula, If estimated_vram > 78GB (97.5% of capacity, leaving 2.5% safety margin): Display warning modal: "‚ö†Ô∏è High OOM Risk: Your configuration is estimated to use {X}GB of 80GB VRAM (98%). High risk of Out of Memory error. Recommendations: {suggested fixes from above}", User options: "Adjust Configuration" (applies suggested fix), "Proceed Anyway" (acknowledge risk, create job), "Learn More" (documentation); If estimated_vram 75-78GB (94-97%): Display milder warning badge: "‚ö† Moderate OOM risk - consider reducing batch_size for safety"; Analytics tracking: How often users ignore warnings vs apply fixes, Success rate comparison: jobs with warnings vs jobs under threshold
    - **Post-OOM Support**: In error modal, additional support options: "Still having issues?" section: "Contact Support" button (pre-fills support ticket with job ID, configuration, error details), "Request Configuration Review" (expert team reviews config, suggests custom optimization), "Try Conservative Preset" (safest option, highest success rate), "Join Community Forum" (link to discussions about OOM optimization); Automated email follow-up (24 hours after OOM): "We noticed your training job failed with OOM. Here are resources: [link to guide], [link to preset comparison], [schedule consultation with AI engineer]"
    - **OOM Recovery Success Tracking**: For each retry after OOM fix applied, track outcome: If successful completion: INSERT oom_resolution_success (original_job_id, retry_job_id, fix_applied, success = true); Display success message: "‚úì Training completed successfully with suggested fix! Original config: batch_size=4, Fixed config: batch_size=2, Result: Successful completion in 14h 32m"; If retry also fails with OOM: Escalate to support, Different fix needed, More aggressive recommendation: "This configuration still exceeds VRAM. Try Conservative preset or contact support for custom solution"
    - **Team-wide OOM Insights**: Admin/manager dashboard shows: Team OOM rate: "12% of training jobs failed with OOM (last 30 days)", Most problematic configurations: "Aggressive preset + batch_size=4: 8 OOM failures", Suggested team action: "Update team training: Always start with Conservative or Balanced preset", Cost impact: "OOM failures cost ${X} in wasted GPU time (preventable with better configuration)", Training recommendation: "Provide LoRA hyperparameter training to team to reduce OOM rate from 12% to target <5%"

- **FR3.1.2:** Dataset Format Error Handling
  * Description: System shall implement comprehensive dataset format validation throughout the training pipeline lifecycle, detecting schema violations, missing fields, malformed JSON, encoding issues, and structural anomalies during both pre-flight validation (before job creation) and preprocessing stage (during training initiation). The error handling shall identify specific problematic conversations with line numbers and field names, display formatted data samples with error highlighting, provide step-by-step remediation guidance with deep links to editing interfaces, enable quick fixes for common issues, prevent invalid training file usage through pre-creation validation gates, and maintain data quality logs for continuous improvement while communicating technical issues in user-friendly language.
  * Impact Weighting: Debugging / Data Quality / Time Savings
  * Priority: High
  * User Stories: US3.1.2
  * User Journey: UJ4.2.1 (Dataset Format Validation), UJ4.2.2 (Fixing Data Errors)
  * Tasks: [T-3.1.2]
  * User Story Acceptance Criteria:
    - Detect dataset validation errors during preprocessing stage
    - Job status updates to "failed" with error type "Dataset Format Error"
    - Error modal displays:
    - **Problem**: "Training data validation failed during preprocessing"
    - **Specific error**: "Conversation #47 (ID: conv_abc123) is missing required field 'target_response'"
    - **Conversation details**: Show conversation metadata (persona, emotional_arc, topic)
    - **Data sample**: Display the malformed conversation JSON with error highlighted
    - **How to fix**:
      1. Go to conversation editor
      2. Fix missing field
      3. Regenerate training file
      4. Retry training job
    - **Quick action**: "Open Conversation Editor" button (deep link to conversation ID)
    - Validate training file schema before job creation to catch errors early
    - Prevent job creation if validation fails with clear error message
    - Example error: "DatasetFormatError: Training pair #47 missing 'target_response' field. Fix in conversation editor and regenerate training file."
  * Functional Requirements Acceptance Criteria:
    - **Pre-Flight Validation** (before job creation): When user selects training file in job configuration, trigger validation: GET /api/training-files/{file_id}/validate, Server downloads training file from storage, runs schema validation against expected format, Validation checks: JSON parse-ability (valid JSON syntax), Required top-level fields present: training_file_metadata, consultant_profile, conversations (array), metadata.total_conversations matches conversations.length, Each conversation has required fields: conversation_id, conversation_metadata, training_pairs (array), Each training_pair has: prompt_context, user_query, target_response, scaffolding_metadata; Validation response time limit: 10 seconds timeout for large files (>5MB), If validation fails: Display warning banner: "‚ö†Ô∏è Training file has {X} validation errors. Fix errors before creating job.", "View Errors" button opens detailed error list modal, Job creation blocked: "Create Training Job" button disabled with tooltip "Training file has validation errors", If validation passes: ‚úì badge displayed: "Training file validated successfully ({X} conversations, {Y} training pairs)", Job creation allowed
    - **Preprocessing Stage Validation** (during training start): GPU container downloads training file from storage during preprocessing stage, Runs same validation checks as pre-flight, Additional runtime checks: Tokenization compatibility (can tokenize all text with Llama 3 tokenizer), Token length validation (no conversation exceeds max_sequence_length=4096), Character encoding verification (UTF-8 valid, no corruption), Conversation uniqueness (no duplicate conversation_ids); If validation fails during preprocessing: Send webhook with validation errors, Job status ‚Üí 'failed', error_type ‚Üí 'DatasetFormatError', Stop processing immediately (do not start model loading), Store validation error details in job record
    - **Error Detection Granularity**: For each validation error, capture: error_type: "missing_field", "invalid_type", "malformed_json", "encoding_error", "empty_value", "duplicate_id", "token_overflow", affected_conversation_index: position in conversations array (0-based), affected_conversation_id: business key from conversation_metadata.id, field_path: JSON path to problematic field (e.g., "conversations[47].training_pairs[2].target_response"), error_message: human-readable description, suggested_fix: actionable remediation guidance, severity: "critical" (blocks training), "warning" (may cause issues), "info" (non-blocking)
    - **Error Modal Design**: Header: "Dataset Format Error", Icon: üìÑ‚ùå, Subheader: "Training file validation failed. {X} errors found.", Tab navigation: "Error Summary" (default), "All Errors" (list view), "Data Preview" (formatted JSON), "Fix Guide" (remediation steps)
    - **Error Summary Tab**: Displays first/critical error prominently, Card layout: **Problem**: "Training data validation failed during preprocessing", "Your training file contains invalid data that cannot be processed.", **Specific Error** (highlighted in red box): "Conversation #47 (ID: conv_abc123) is missing required field 'target_response'", "Location: conversations[46].training_pairs[0].target_response", **Affected Conversation Details**: Shows conversation metadata: "Conversation ID: conv_abc123", "Persona: Anxious Investor", "Emotional Arc: Triumph", "Topic: Retirement Planning", "Training Pairs: 8 pairs total, error in pair #1", **Error Impact**: "This conversation cannot be used for training. {X} other conversations validated successfully."
    - **Data Sample Display**: Formatted JSON viewer with syntax highlighting, Shows affected conversation object with context (10 lines before/after error), Error location highlighted with red background and arrow indicator, Example display: ```json {...abbreviated context...}, "training_pairs": [, {, "prompt_context": "Elena discussing retirement...",, "user_query": "How much should I save?",, >>> "target_response": null, <<< ERROR: Missing required field, "scaffolding_metadata": {...}, }, {...rest of array...}] ```, Copy button: "Copy Conversation JSON" for debugging
    - **All Errors Tab**: Paginated list view if multiple errors (25 errors per page), Each error as collapsible card: Card header: "{error_type}: Conversation #{index} (ID: {id})", Card body: Field path, Error message, Suggested fix; Sort options: "By conversation number", "By error type", "By severity", Filter: "Show critical only", "Show all", Export button: "Export Errors as CSV" (for bulk fixing)
    - **Fix Guide Tab**: Step-by-step remediation workflow, **Step 1**: "Identify affected conversations", List of conversation IDs with errors: "conv_abc123, conv_def456, conv_ghi789", Bulk action: "Select all errored conversations"; **Step 2**: "Open conversation editor to fix data", "Quick Fix" buttons per conversation: "Open conv_abc123 in Editor" (deep link to `/conversations/{id}/edit`), Opens in new tab preserving context; **Step 3**: "Fix specific field issues", For each error: shows field name, current value (if any), expected format/type, Example: "target_response: Expected non-empty string, Found: null, Fix: Add target response text in conversation editor"; **Step 4**: "Regenerate training file after fixes", Button: "Regenerate Training File" (calls API to rebuild file from fixed conversations), Shows progress: "Regenerating... 156 of 242 conversations processed", Completion: "‚úì Training file regenerated successfully. {X} conversations, {Y} training pairs.", Validation runs automatically after regeneration; **Step 5**: "Retry training job", Button: "Create New Training Job" (pre-selects newly regenerated file), Or: "Return to Job List"
    - **Quick Action Buttons**: Modal footer has action buttons: "Open Conversation Editor" (primary, blue button): Opens `/conversations/{affected_conversation_id}/edit` in new tab, Preserves error modal so user can reference, "Regenerate Training File" (secondary, if user has permissions): Initiates training file regeneration workflow, Disabled if conversations not yet fixed, "Contact Support" (tertiary): Pre-fills support ticket with: Job ID, Training file ID, Error details, Validation log excerpt; "Dismiss" (closes modal, user can retry later)
    - **Pre-Creation Validation Workflow**: During job configuration (before "Create Training Job" clicked), Async validation request: `GET /api/training-files/{file_id}/validate`, Loading state: "Validating training file..." with spinner, Validation response: {valid: true/false, error_count: number, errors: [{...}], warnings: [{...}], metadata: {...}}; If valid: Display success message: "‚úì Training file validated: {X} conversations, {Y} training pairs, all checks passed", Enable "Create Training Job" button; If invalid: Display error summary: "‚ùå Validation failed: {X} critical errors, {Y} warnings", Show first 3 errors inline: "1. Conversation #47: Missing target_response", "2. Conversation #52: Malformed JSON", "3. Conversation #89: Duplicate conversation_id", "View All Errors" button opens full error modal, Disable "Create Training Job" button, Tooltip on disabled button: "Fix {X} validation errors before creating job"
    - **Common Error Types and Fixes**: **Missing Required Field**: Error: "Conversation #{X} missing '{field_name}'", Fix: "Add missing field in conversation editor. Field must be non-empty.", Validation: Field exists and has non-null, non-empty value; **Invalid Data Type**: Error: "Field '{field}' expected {expected_type}, got {actual_type}", Fix: "Convert field to correct type. Example: training_pairs must be array, not string.", Validation: Type matches schema definition; **Malformed JSON**: Error: "Invalid JSON syntax at line {X}: {parse_error}", Fix: "JSON parsing failed. This usually indicates file corruption. Regenerate training file.", Validation: JSON.parse() succeeds without errors; **Encoding Error**: Error: "Invalid UTF-8 character encoding at byte {X}", Fix: "File contains non-UTF-8 characters. Re-export conversations with UTF-8 encoding.", Validation: File reads successfully as UTF-8; **Token Length Exceeded**: Error: "Conversation #{X} exceeds max token length: {tokens} tokens (max: 4096)", Fix: "Shorten conversation text or increase max_sequence_length (may cause OOM).", Validation: Tokenized length ‚â§ max_sequence_length; **Duplicate ID**: Error: "Duplicate conversation_id: '{id}' found at indices {X} and {Y}", Fix: "Ensure all conversation IDs are unique. Edit duplicate conversations to have unique IDs.", Validation: All conversation_ids are unique within training file; **Empty Array**: Error: "training_pairs array is empty for conversation #{X}", Fix: "Add at least one training pair to conversation.", Validation: training_pairs.length ‚â• 1
    - **Validation Error Logging**: All validation errors logged to database: INSERT INTO training_file_validation_errors (training_file_id, job_id, error_type, conversation_index, conversation_id, field_path, error_message, severity, detected_at); Analytics queries: Track most common error types: SELECT error_type, COUNT(*) as count FROM validation_errors GROUP BY error_type ORDER BY count DESC, Result: "Most common: missing_field (47%), malformed_json (23%), token_overflow (18%)", Identify problematic conversation patterns: "Conversations with emotional_arc='Anxiety' have 3x higher validation error rate", Feed insights back to conversation generation quality controls
    - **Automated Fix Suggestions** (future enhancement): For certain error types, system attempts auto-fix: **Missing optional field**: Auto-populate with default value, "scaffolding_metadata.difficulty_level missing ‚Üí auto-set to 'medium'", **Trailing comma in JSON**: Auto-remove to fix parse error, **Encoding issues**: Auto-convert to UTF-8, Preview auto-fixes to user: "We detected {X} fixable errors. Apply suggested fixes? [Preview Fixes] [Apply All] [Manual Fix]", User reviews, approves, system regenerates file with fixes, Validation re-runs automatically after auto-fix
    - **Training File Quality Score**: Based on validation results, calculate quality score: Base score: 100 points, Deduct points for each error: Critical error: -10 points, Warning: -2 points, Info: -0.5 points, Bonus points for: All required fields present (+5), Rich metadata (+5), Human review >20% (+10); Display quality score in training file selector: "Training File Quality: 87/100 (Good - Minor warnings only)", Color coding: 90-100 (green), 70-89 (yellow), <70 (red - recommend fixing errors first); Track quality over time: "Your training file quality improved from 72 to 87 after fixing validation errors"

- **FR3.1.3:** GPU Provisioning Error Handling
  * Description: System shall implement comprehensive GPU provisioning error detection, diagnosis, and recovery mechanisms to handle RunPod API failures including spot instance unavailability, provisioning timeouts, datacenter outages, and quota limits. The system shall present users with intelligent recovery options including automatic retry with exponential backoff, spot-to-on-demand migration, delayed scheduling, and cancellation workflows while displaying real-time datacenter availability metrics, historical provisioning success rates, estimated wait times based on demand patterns, and proactive notifications throughout the provisioning lifecycle to maintain user confidence during infrastructure delays and enable informed decision-making about cost vs availability trade-offs.
  * Impact Weighting: User Experience / Flexibility / Reliability
  * Priority: High
  * User Stories: US3.1.3
  * User Journey: UJ4.3.1 (GPU Provisioning Failures), UJ4.3.2 (Provisioning Recovery Options)
  * Tasks: [T-3.1.3]
  * User Story Acceptance Criteria:
    - Detect GPU provisioning failures from RunPod API
    - Common scenarios:
    - **No spot GPUs available**: "All H100 spot instances are currently in use. High demand."
    - **Spot provisioning timeout**: "Waited 10 minutes, no spot GPU allocated."
    - **Region unavailable**: "RunPod datacenter temporarily unavailable."
    - Error modal displays:
    - **Problem**: "No H100 spot GPUs currently available"
    - **Reason**: "High demand in RunPod datacenter (92% utilization)"
    - **Options**:
      1. **Auto-retry** (spot): "Automatically retry every 5 minutes until GPU available (max 1 hour)"
      2. **Switch to on-demand**: "Start immediately on-demand GPU (+$5/hr, guaranteed availability)"
      3. **Cancel and retry later**: "Cancel job, try again in 30-60 minutes during off-peak hours"
    - **Estimated wait time**: "Historical data shows spot GPUs typically available within 15-30 minutes"
    - If auto-retry selected:
    - Job status: "queued_waiting_for_gpu"
    - Retry every 5 minutes for 1 hour
    - Notification when GPU provisioned and training starts
    - Notification if 1 hour timeout reached: "Still no spot GPU available. Switch to on-demand or cancel?"
    - Track provisioning failure rate to identify patterns (time of day, datacenter congestion)
  * Functional Requirements Acceptance Criteria:
    - **Provisioning Failure Detection**: Monitor RunPod API responses during pod creation, Error detection patterns: HTTP 503 (Service Unavailable), HTTP 429 (Rate Limit/Quota Exceeded), HTTP 404 (Region unavailable), Response body contains: "No available GPUs", "Spot capacity exhausted", "Datacenter maintenance", "Resource quota exceeded", Timeout detection: If pod creation request pending >10 minutes without status change ‚Üí timeout error; Upon detection: UPDATE training_jobs SET status = 'provisioning_failed', error_type = 'GPUProvisioningError', error_subtype = {specific_cause}, provisioning_attempts = provisioning_attempts + 1, last_provisioning_attempt_at = NOW(); INSERT provisioning_errors (job_id, error_type, runpod_response, datacenter_id, gpu_type, pricing_tier, timestamp)
    - **Error Type Classification**: **No Spot Availability** (most common): RunPod API returns: "No spot instances available for H100_PCIE_80GB", Cause: High demand, all spot capacity allocated, datacenter utilization >90%, Historical pattern: Common during US business hours (9 AM - 5 PM PST), weekdays > weekends; **Provisioning Timeout**: Pod creation initiated but stuck in "pending" state >10 minutes, Cause: Datacenter congestion, slow allocation, queueing delays, May resolve if given more time (not permanent failure); **Region/Datacenter Unavailable**: RunPod API returns: "Datacenter temporarily unavailable", "Scheduled maintenance in progress", Cause: Planned maintenance, infrastructure issues, complete unavailability (not just capacity), Recovery: Wait for maintenance completion (ETA provided in API response if available); **Quota Exceeded**: API returns: "Account GPU quota exceeded", "Monthly GPU hours limit reached", Cause: User/team has consumed allocated GPU resources, Requires: Quota increase request, billing tier upgrade; **Rate Limiting**: HTTP 429: "Too many pod creation requests", Cause: Exceeded RunPod API rate limits (e.g., >5 pod creations per minute), Recovery: Automatic backoff and retry after cooldown
    - **Error Modal Design**: Full-screen modal, non-dismissable (must choose recovery option), Header: "GPU Provisioning Failed", Icon: üîå‚ùå or ‚öôÔ∏è‚ö†Ô∏è, Subheader context-aware: "Unable to provision spot GPU" or "Datacenter temporarily unavailable" or "Provisioning timeout"
    - **Problem Statement**: Clear explanation of issue: "No H100 spot GPUs currently available", "Your training job requires an H100 PCIe 80GB GPU, but all spot instances are currently in use.", Visual indicator: GPU availability gauge: "Spot Availability: Low (8% free capacity)", "On-Demand Availability: High (87% free capacity)"
    - **Reason Analysis**: Data-driven explanation of cause: "High demand in RunPod datacenter (92% utilization)", "Typical peak hours: 9 AM - 5 PM PST on weekdays", "Current time: 10:30 AM PST (peak demand period)", Historical context: "Spot availability usually increases after 5 PM (off-peak)", Datacenter status: Fetch real-time from RunPod API: GET /v2/availability, Display: "US-West datacenter: 92% utilized (23 of 25 H100s in use)", "EU-West datacenter: 67% utilized (16 of 24 H100s in use)" (if multi-region supported future)
    - **Recovery Options Section** (presented as large action cards): **Option 1: Auto-Retry (Recommended for spot)**: Title: "Wait for Spot GPU (Auto-Retry)", Description: "Automatically retry provisioning every 5 minutes until GPU becomes available", Details: "Max retry duration: 1 hour (12 attempts)", "You'll be notified when GPU provisioned and training starts", "No action required from you", Estimated wait time: "Historical data shows spot GPUs typically available within 15-30 minutes", Success rate: "82% of retries succeed within 1 hour during peak hours, 96% during off-peak", Cost: "No additional cost beyond standard spot rate ($2.49/hr)", Action button: "Enable Auto-Retry" (primary, blue); **Option 2: Switch to On-Demand**: Title: "Start Immediately (On-Demand GPU)", Description: "Guaranteed GPU availability, start training within 5 minutes", Details: "No waiting, no retry uncertainty", "Guaranteed completion, no spot interruptions", Cost comparison: "Spot: $2.49/hr ‚Üí On-Demand: $7.99/hr (+$5.50/hr)", "Total job cost: Estimated $XX (spot) vs $YY (on-demand) = +$ZZ premium", "Cost increase: {percentage}% more expensive", Recommendation: "Best if urgent deadline or already waited >30 minutes", Action button: "Switch to On-Demand & Start" (secondary, green); **Option 3: Cancel and Retry Later**: Title: "Cancel Job, Try During Off-Peak Hours", Description: "Cancel this job and retry when spot availability is higher", Details: "Spot availability highest: Evenings (5 PM - 11 PM PST), Weekends, Overnight (11 PM - 7 AM)", "Suggested retry times: Tonight at 6 PM (90% availability), Saturday morning (95% availability)", No cost incurred (job never started), Action button: "Cancel Job" (tertiary, gray); **Option 4: Contact Support** (if all else fails): Title: "Request Support Assistance", Description: "Our team can help with custom provisioning or priority allocation", Details: "Response time: <2 hours during business hours", "May be able to reserve GPU capacity for urgent needs", Action button: "Contact Support"
    - **Auto-Retry Workflow**: User selects "Enable Auto-Retry", Modal updates: "Auto-Retry Enabled - Monitoring for Available GPUs", UPDATE training_jobs SET status = 'queued_waiting_for_gpu', auto_retry_enabled = true, auto_retry_started_at = NOW(), auto_retry_max_duration_minutes = 60, auto_retry_interval_minutes = 5; Background job (cron or queue): Every 5 minutes, attempt RunPod pod creation: POST /v2/pods/create with same configuration; Retry attempts logged: INSERT provisioning_retry_log (job_id, attempt_number, timestamp, result); Success: Pod created, job status ‚Üí 'provisioning' ‚Üí 'preprocessing', Notification sent: Email + Slack: "‚úì GPU provisioned! Training started for {job_name}", User redirected to job dashboard; Failure: Log attempt, wait 5 minutes, retry; Timeout (1 hour elapsed, 12 failed attempts): Job status ‚Üí 'provisioning_failed_timeout', Modal updates: "Still No Spot GPU After 1 Hour", Options: "Continue Waiting (extend to 2 hours)", "Switch to On-Demand Now", "Cancel Job"; Notification: "Spot GPU still unavailable after 1 hour. Choose next action: [View Options]"
    - **Real-Time Status Updates** (during auto-retry): Job dashboard shows: Status badge: "Waiting for GPU" (pulsing yellow), Progress indicator: "Auto-retry in progress: Attempt 3 of 12", Time elapsed: "Waiting for 15 minutes (max: 1 hour)", Next retry: "Next attempt in 2m 30s" (countdown timer), Datacenter availability chart: Line graph showing utilization over past 2 hours, trend prediction; Live updates via polling (every 30 seconds): Check job status, update UI if provisioning succeeded, Show availability changes: "Datacenter utilization decreased from 92% to 85% - higher chance of success on next retry"; User can modify strategy during wait: "Switch to On-Demand" button available anytime, "Cancel Auto-Retry" option if user changes mind
    - **Provisioning Failure Analytics**: System tracks provisioning failures across all jobs: Metrics: Failure rate: (failed_provisionings / total_provisioning_attempts) per hour, per day, Average wait time for successful spot provision, Peak failure times: "9-11 AM PST: 42% failure rate, 3-5 PM PST: 38% failure rate", Off-peak success rate: "11 PM - 7 AM: 96% success within 5 minutes"; Insights displayed to users: In job configuration: "‚ö†Ô∏è Peak Demand Period: Spot provisioning success rate currently 58% (10 AM PST). Consider: On-demand GPU for immediate start, Schedule job for evening (95% success rate)"; Dashboard widget: "Spot Availability Forecast" - shows projected availability for next 24 hours based on historical patterns
    - **Proactive Availability Warnings**: Before job creation, check current datacenter utilization: GET /v2/availability, If spot_available_percentage < 20%: Display warning modal: "‚ö†Ô∏è Low Spot Availability Alert", "Current spot utilization: 92% (only 2 of 25 GPUs available)", "Your job may experience provisioning delays (estimated wait: 20-40 minutes)", Options: "Proceed with Spot (may wait)", "Use On-Demand Instead (+$XX)", "Schedule for Later (when availability higher)"; User decision logged for effectiveness analysis
    - **Regional Failover** (future enhancement): If primary datacenter unavailable, auto-failover to secondary region, "US-West datacenter at capacity, automatically trying US-East datacenter...", Transparent to user, may have slight latency differences, Training continues normally once provisioned in alternate region
    - **Priority Provisioning** (premium feature): Enterprise users get priority queue for spot provisioning, "Your account has Priority Provisioning enabled", "Your job will be provisioned before standard queue jobs", "Average wait time reduction: 60% compared to standard provisioning", Additional cost: $0.50/hr premium on top of spot rate
    - **Quota Management**: If quota exceeded error: Display: "GPU Quota Exceeded", "Your account has used {X} of {Y} allocated GPU hours this month", "Remaining quota: 0 hours", Options: "Upgrade Plan (increase quota to {Z} hours)", "Wait Until Next Month (quota resets on {date})", "Contact Sales for Custom Quota"; Quota usage visible in dashboard: "GPU Hours Used: 87 of 100 this month (87%)", Progress bar, forecast: "At current rate, quota will be exhausted in 5 days"

- **FR3.2.1:** Spot Instance Interruption Recovery
  * Description: System shall implement robust automatic checkpoint-based recovery from spot instance interruptions by saving training state every 100 steps to cloud storage, detecting interruption events via RunPod webhooks, immediately provisioning replacement spot instances, downloading latest checkpoints, restoring complete training state including model weights, optimizer state, and random seeds, resuming training from exact interruption point, tracking interruption frequency and downtime, maintaining cost accuracy including recovery overhead, notifying users of interruption and recovery status, and achieving 95%+ successful recovery rate with <10 minute resume time to maximize spot instance cost savings while ensuring training reliability and user confidence.
  * Impact Weighting: Cost Efficiency / Reliability / User Confidence
  * Priority: High
  * User Stories: US3.2.1
  * User Journey: UJ4.4.1 (Spot Interruption Handling), UJ4.4.2 (Automatic Recovery Process)
  * Tasks: [T-3.2.1]
  * User Story Acceptance Criteria:
    - **During training**: Checkpoint saved every 100 steps to Supabase Storage bucket `training-checkpoints`
    - Checkpoint includes: model weights (LoRA adapters), optimizer state, training step, epoch, random seed
    - Checkpoint naming: `{job_id}/checkpoint-step-{step_number}.pt`
    - **On spot interruption**:
    - RunPod sends webhook: "Spot instance interrupted"
    - Job status updates to "interrupted"
    - System initiates recovery immediately
    - **Automatic recovery process**:
      1. Provision new spot instance (same configuration)
      2. Download latest checkpoint from storage
      3. Resume training from last saved step
      4. Update status to "training" (resumed)
      5. Track interruption count
    - **Dashboard display**:
    - Interruption badge: "Interrupted 2√ó (auto-recovered)"
    - Interruption log: "Interrupted at step 850 (6h 23m), resumed at step 850 (6h 32m) - 9 min downtime"
    - Total interruption downtime tracked separately
    - **Success criteria**:
    - Resume within 10 minutes of interruption
    - 95%+ successful recovery rate
    - Cost tracking includes interruption overhead
    - Notification: "Training interrupted at 42% complete. Auto-recovery in progress... [Track Status]"
    - Notification: "Training resumed from checkpoint (Step 850). Estimated completion: 8h 15m remaining."
    - If recovery fails 3 times: Offer option to switch to on-demand instance
  * Functional Requirements Acceptance Criteria:
    - **Checkpoint Saving During Training**: GPU container configured to save checkpoint every 100 training steps, Checkpoint save frequency configurable per preset: Conservative (every 100 steps), Balanced (every 100 steps), Aggressive (every 50 steps - more frequent for longer runs); Checkpoint contents using PyTorch torch.save(): model_state_dict: LoRA adapter weights (all trainable parameters), optimizer_state_dict: Adam optimizer momentum + variance buffers, lr_scheduler_state_dict: Cosine annealing schedule current state, training_state: {current_step, current_epoch, global_step, best_validation_loss, training_loss_history}, random_states: {torch.get_rng_state(), np.random.get_state(), random.getstate()}, configuration: Complete hyperparameter config for verification, metadata: {job_id, training_file_id, checkpoint_step, saved_at_timestamp}; Checkpoint file format: PyTorch .pt file, typical size 400-500MB (LoRA adapters + optimizer state), Compression: gzip compression applied, reduces size by ~30%
    - **Checkpoint Upload to Storage**: After checkpoint saved locally on GPU, upload to Supabase Storage: bucket = 'training-checkpoints', path = '{job_id}/checkpoint-step-{step}.pt', Upload with retry logic: 3 attempts with exponential backoff if network fails, Upload progress webhook: "Checkpoint upload: 42% (180MB of 430MB)", Upload completion webhook: {event: "checkpoint_saved", step: 500, checkpoint_path: "...", file_size_mb: 430, upload_duration_seconds: 28}; Old checkpoints cleanup: Keep last 3 checkpoints only (delete checkpoints older than current - 300 steps), Prevents storage bloat, Configurable retention policy; Upload optimization: Parallel uploads (checkpoint saves while training continues), Async upload doesn't block training loop, Upload bandwidth limit: 50 Mbps to avoid affecting training data transfer
    - **Spot Interruption Detection**: RunPod sends webhook when spot instance reclaimed: POST /api/training/webhook with payload: {event_type: "spot_interruption", job_id: "...", pod_id: "...", interrupted_at_timestamp: "...", last_checkpoint_step: 500, interruption_warning_seconds: 120}; RunPod provides 2-minute warning before termination (spot eviction notice), GPU container receives SIGTERM signal, Graceful shutdown: Save emergency checkpoint (even if not at 100-step boundary), Upload checkpoint with high priority, Send final webhook: "Checkpoint saved, container terminating", Container exits within 120 seconds
    - **Job Status Update on Interruption**: Webhook handler receives interruption event, UPDATE training_jobs SET status = 'interrupted', interrupted_at = NOW(), last_checkpoint_step = {step}, checkpoint_recovery_count = checkpoint_recovery_count + 1; INSERT interruption_log (job_id, interrupted_at_step, interrupted_at_time, checkpoint_available, recovery_initiated_at); User notification (push): "Training interrupted at {percentage}% complete (step {step}). Auto-recovery starting..." (notification doesn't require user action, informational only)
    - **Automatic Recovery Initiation**: Webhook handler immediately triggers recovery workflow (no human intervention), Recovery workflow: async function recoverFromInterruption(job_id), Steps: 1) Verify checkpoint exists in storage: GET storage.getFile('{job_id}/checkpoint-step-{latest_step}.pt'), If checkpoint missing: CRITICAL ERROR, cannot recover, escalate to manual intervention; 2) Provision new spot GPU: POST /v2/pods/create with same config (H100, spot, same datacenter if available), Include checkpoint_path in pod environment variables, Priority: High (recovery jobs get slight priority boost in RunPod queue); 3) Monitor provisioning: Poll pod status every 10 seconds, If provisioning takes >5 minutes: Log delay, continue monitoring, If provisioning fails (no spot available): Wait 2 minutes, retry provisioning, Max 3 provisioning attempts within 10 minutes; 4) On pod ready: Send startup webhook to new pod: "Resume from checkpoint: {path}", Pod downloads checkpoint: GET storage.getFile(checkpoint_path), Download with progress updates, Verify checkpoint integrity: checksum validation; 5) Restore training state: Load model_state_dict into LoRA adapters, Load optimizer_state_dict, Load lr_scheduler_state_dict, Restore random_states (ensures deterministic continuation), Verify current_step matches checkpoint_step; 6) Resume training loop: Start training from step {checkpoint_step + 1}, Send webhook: {event: "training_resumed", resumed_from_step: 500, downtime_minutes: 8.5}, UPDATE training_jobs SET status = 'training', resumed_at = NOW(), total_interruption_downtime_minutes = total + downtime
    - **Recovery Success Tracking**: Target metrics: Resume within 10 minutes of interruption (95th percentile), 95%+ successful recovery rate (recoveries that complete training / total interruptions); Tracking: Calculate recovery_duration_minutes = (resumed_at - interrupted_at).total_minutes(), Log success/failure: INSERT recovery_outcomes (job_id, interruption_number, recovery_duration_minutes, success, failure_reason, timestamp); Dashboard analytics: "Spot Instance Recovery Performance: 96.3% success rate, 8.2 min average recovery time (last 30 days)"; Alert if recovery_rate < 90%: "‚ö†Ô∏è Recovery rate declining, investigate infrastructure issues"
    - **Dashboard Interruption Display**: Interruption badge on job dashboard: "Interrupted 2√ó (auto-recovered)" (orange badge with checkmark), Click badge opens interruption details modal: Timeline view showing each interruption, Per-interruption details: "Interruption #1: Step 500 at 3h 42m ‚Üí Resumed at step 500 at 3h 51m (9 min downtime)", "Interruption #2: Step 1200 at 8h 15m ‚Üí Resumed at step 1200 at 8h 22m (7 min downtime)", Total interruption downtime: "Total downtime: 16 minutes (0.3 hours)", "Active training time: 12h 18m", "Total elapsed time: 12h 34m (including interruptions)"; Visual interruption indicators on loss curve: Vertical dotted lines at interruption steps, Tooltip on hover: "Interrupted at step 500, resumed 9 min later", Loss curve continuous (no gaps) - training resumed seamlessly
    - **Cost Tracking with Interruptions**: Accurate cost calculation including recovery overhead: active_training_hours = (total_elapsed_time - interruption_downtime_minutes) / 60, interruption_overhead_hours = (checkpoint_recovery_count √ó avg_recovery_time_minutes) / 60, total_billable_hours = active_training_hours + interruption_overhead_hours, actual_cost = total_billable_hours √ó gpu_hourly_rate; Cost breakdown display: "GPU Training Time: 12.3 hours √ó $2.49/hr = $30.63", "Interruption Overhead: 0.3 hours √ó $2.49/hr = $0.75 (2 recoveries)", "Total Cost: $31.38"; Compared to estimate: "Estimated: $48-60, Actual: $31.38 (35% under estimate due to fewer interruptions than expected)"
    - **User Notifications**: **On Interruption**: Email + Slack (if not urgent/critical): Subject: "Training Interrupted - Auto-Recovery in Progress", Body: "Your training job {name} was interrupted at {percentage}% complete (step {step}). We're automatically provisioning a new GPU and will resume within 10 minutes. No action needed from you."; Push notification (mobile): "Training interrupted, auto-recovering..." (brief, non-alarming); **On Resume**: Email + Slack: Subject: "Training Resumed Successfully", Body: "Training resumed from step {step}. Downtime: {minutes} min. Estimated completion: {time}.", Estimated completion time recalculated based on remaining steps; Dashboard banner (if user viewing): "‚úì Training resumed from checkpoint. Interruption recovered in 8 min."
    - **Recovery Failure Escalation**: If recovery fails 3 times within 30 minutes: UPDATE training_jobs SET status = 'recovery_failed', recovery_attempts = 3; User notification (urgent): "‚ö†Ô∏è Unable to Recover After 3 Attempts", Email + Slack + push notification; Error modal displayed: "Spot Instance Recovery Failed", "Reason: Unable to provision replacement spot GPU after 3 attempts (datacenter capacity issues)", Options: "Option 1: Continue trying spot (may take hours during peak demand)", "Option 2: Switch to on-demand GPU (guaranteed recovery, +$5.50/hr)", "Option 3: Cancel job and retry later"; Recommended action: "Switch to on-demand GPU recommended for jobs >50% complete (minimal waste)"; User selects option: If on-demand: Provision on-demand GPU, load checkpoint, resume (guaranteed to work), UPDATE training_jobs SET gpu_pricing_tier = 'on_demand', recovery_switched_to_ondemand = true, Cost recalculated with on-demand rate from switch point forward; If continue spot: Continue retry loop indefinitely (or until user cancels), If cancel: Mark job as cancelled, preserve checkpoint for potential future resume
    - **Checkpoint Recovery Rate Analytics**: System-wide tracking: Query: SELECT COUNT(*) as total_interruptions, SUM(CASE WHEN recovery_successful THEN 1 ELSE 0 END) as successful_recoveries FROM interruption_log WHERE created_at > NOW() - INTERVAL '30 days', Calculate recovery_rate = (successful_recoveries / total_interruptions) √ó 100; Display in admin dashboard: "Spot Interruption Recovery: 96.3% success rate, 427 interruptions handled, 411 successful recoveries, 16 required manual intervention", Trend chart: Recovery rate over time (target: maintain >95%); Root cause analysis for failures: "Failed recoveries: 12 due to checkpoint corruption, 3 due to spot unavailability >30 min, 1 due to storage outage"
    - **Optimization Strategies**: Pre-provisioning (future): Predict likely interruption times based on historical patterns, Pre-provision backup spot GPU during high-risk periods, Seamless failover if primary interrupted (0-second downtime); Multi-checkpoint redundancy: Save checkpoints to 2 different cloud storage regions, If primary checkpoint unavailable, fallback to secondary, Increases reliability to 99.9%+; Interruption avoidance: Choose datacenters with historically lower spot interruption rates, "US-West datacenter: 18% interruption rate, US-East: 12% ‚Üí recommend US-East for spot jobs"

- **FR3.2.2:** Manual Checkpoint Resume
  * Description: System shall enable users to manually resume failed training jobs from saved checkpoints by displaying resume options on jobs with available checkpoint files, opening pre-configured resume modal with previous settings, allowing strategic configuration adjustments including GPU type migration, batch size reduction, epoch modification, and learning rate schedule changes, calculating accurate cost estimates for remaining work, creating linked continuation jobs that preserve training history, downloading and validating checkpoint integrity, restoring training state with configuration modifications, and tracking resumed jobs with clear lineage to enable recovery from failures like OOM errors, repeated spot interruptions, or user-initiated improvements while maintaining training continuity and cost efficiency.
  * Impact Weighting: Cost Efficiency / Flexibility / User Control
  * Priority: Medium
  * User Stories: US3.2.2
  * User Journey: UJ4.5.1 (Manual Checkpoint Resume), UJ4.5.2 (Resume Configuration Adjustments)
  * Tasks: [T-3.2.2]
  * User Story Acceptance Criteria:
    - Failed jobs with available checkpoints show "Resume from Checkpoint" button
    - Click opens configuration modal pre-filled with previous settings
    - Allow adjustments:
    - Switch GPU type (spot ‚Üí on-demand)
    - Adjust remaining epochs
    - Change learning rate schedule
    - Modify batch size (if OOM was the issue)
    - Display: "Resume from Step 850 (42% complete). Remaining: 1.5 epochs (~8 hours)"
    - Cost estimate updates based on remaining work
    - Confirmation: "Resume training from last checkpoint with adjusted configuration?"
    - After confirmation:
    - Create new job linked to original job
    - Download checkpoint from storage
    - Continue training from saved step
    - Track as "resumed from job_abc123"
    - Useful scenarios: OOM error ‚Üí reduce batch_size ‚Üí resume; Spot interruption loop ‚Üí switch to on-demand ‚Üí resume
  * Functional Requirements Acceptance Criteria:
    - **Checkpoint Availability Detection**: For jobs with status IN ('failed', 'cancelled', 'recovery_failed'), query checkpoint availability: SELECT checkpoint_path, checkpoint_step, checkpoint_size_mb FROM training_jobs WHERE id = {job_id} AND checkpoint_path IS NOT NULL, Verify checkpoint file exists in storage: HEAD request to Supabase Storage returns 200 OK, If checkpoint available: Display "Resume from Checkpoint" button (prominent, blue, ‚ñ∂Ô∏è icon), Position: Next to "Retry Job" button on failed job details page, Button label: "Resume from Step {checkpoint_step}", Tooltip: "Continue training from last saved checkpoint ({checkpoint_step_percentage}% complete)", If no checkpoint: Button hidden, explanation: "No checkpoint available. This job failed before first checkpoint (step 100)."
    - **Resume Configuration Modal**: Click "Resume from Checkpoint" opens full-screen modal, Modal header: "Resume Training from Checkpoint", Subheader: "Continue training from step {checkpoint_step} ({percentage}% complete)", Close button (X) - can be dismissed unlike review modal
    - **Original Job Summary Section**: Card showing original job details: Job name: "{original_job_name}", Status: "Failed" (red badge) with failure reason, Original configuration: Preset (Conservative/Balanced/Aggressive), GPU type (Spot/On-Demand), Key hyperparameters: rank={r}, lr={lr}, epochs={epochs}, batch_size={batch_size}, Training progress before failure: "Completed {checkpoint_step} of {total_steps} steps ({percentage}%)", "Training time elapsed: {hours}h {minutes}m", "Cost spent so far: ${cost}", Checkpoint details: "Last checkpoint: Step {step}, saved {time_ago} ago", "Checkpoint size: {size}MB"
    - **Remaining Work Calculation**: Calculate remaining work: remaining_steps = total_steps - checkpoint_step, remaining_epochs = total_epochs - completed_epochs + partial_epoch_fraction, estimated_remaining_hours = remaining_steps √ó avg_seconds_per_step_from_previous_job / 3600; Display prominently: "Remaining Work: {remaining_steps} steps (~{remaining_epochs} epochs)", "Estimated Duration: {hours}h - {hours_max}h", "Estimated Cost: ${min} - ${max} (based on {gpu_type})"
    - **Configuration Adjustment Options**: **GPU Type Selection** (most common adjustment): Toggle: [Spot Instance] [On-Demand Instance], Pre-selected based on context: If original failed due to spot interruptions (‚â•3): Default to on-demand with explanation "Recommended: Switch to on-demand to avoid further interruptions", If original OOM failure: Keep same GPU type, If original spot with <2 interruptions: Default to spot (cost-effective); Cost impact displayed: "Cost change: Spot $XX-YY ‚Üí On-Demand $ZZ-WW (+$AA premium for guaranteed completion)"; **Batch Size Adjustment** (for OOM recovery): If original failure type = 'OutOfMemoryError': Show batch_size adjustment UI (slider or dropdown), Current: batch_size = {original_batch_size}, Suggested: batch_size = {suggested_reduced_batch_size} (calculated to fit in VRAM), Explanation: "Reducing batch_size from {X} to {Y} will prevent OOM error", VRAM estimate display: "Estimated VRAM: {original}GB (exceeded 80GB) ‚Üí {new}GB (fits within capacity)", Can't increase batch_size (only reduce or keep same); **Epochs Adjustment** (optional optimization): Allow reducing remaining epochs: "Complete fewer epochs to finish faster/cheaper", Slider: "Resume with {remaining_epochs} epochs" ‚Üí adjustable down to 1 epoch, Can't increase total epochs beyond original configuration (would require retraining from start), Display impact: "Reducing to 1 epoch: Duration -6h, Cost -$XX"; **Learning Rate Schedule** (advanced, optional): Option to restart LR schedule or continue from checkpoint schedule, Radio buttons: "Continue LR schedule from checkpoint" (default, recommended), "Restart LR schedule (warmup again)", Explanation tooltip: "Continuing schedule maintains training continuity. Restarting may help if training stalled."; **Hyperparameters Locked**: Display locked parameters (cannot change): "LoRA Rank (r): {X} - Cannot change (model architecture must match checkpoint)", "Target Modules: {...} - Locked", "LoRA Alpha: {Y} - Locked", Explanation: "These parameters define model architecture and cannot be changed when resuming from checkpoint."
    - **Cost Estimate Update**: Real-time cost calculation as user adjusts config: estimated_cost = remaining_hours √ó gpu_hourly_rate[selected_gpu_type], Display updates within 300ms of any change, Cost comparison: "Original job cost: ${spent_so_far}", "Estimated additional cost: ${remaining_cost}", "Total projected cost: ${total} vs original estimate ${original_estimate}", Cost savings indicator if switching to spot: "Switching to spot saves $XX vs original on-demand plan"
    - **Resume Confirmation Section**: Checkbox (required): "‚òê I understand this will create a new training job that continues from step {checkpoint_step}", Summary: "Configuration Changes: {list of changes from original}", Example: "GPU type: Spot ‚Üí On-Demand, Batch size: 4 ‚Üí 2, Remaining epochs: 1.5", Warning if no changes: "‚ÑπÔ∏è You haven't modified the configuration. If the original job failed, it may fail again with the same settings.", Recommendation if applicable: "üí° Suggestion: {context-specific advice}", Example: "For OOM error, we recommend reducing batch_size to 2 (current: unchanged at 4)"
    - **Action Buttons**: "Resume Training" (primary, green button): Disabled until checkbox checked, Creates continuation job (workflow below); "Reset to Original Configuration" (secondary): Reverts all changes to original job settings; "Cancel" (tertiary): Closes modal, no job created
    - **Job Creation Workflow**: On "Resume Training" click: INSERT INTO training_jobs (id, name, description, training_file_id, configuration, status, created_by, ...) VALUES (new_uuid(), '{original_job_name} - Resumed', 'Resumed from {original_job_id} at step {checkpoint_step}', ...); Link jobs: INSERT INTO training_job_continuations (original_job_id, continuation_job_id, resumed_from_step, resumed_from_checkpoint_path, configuration_changes); UPDATE original_job SET has_continuation = true, continuation_job_id = {new_job_id}; Job configuration includes: gpu_pricing_tier: {selected_gpu_type}, batch_size: {adjusted_batch_size}, remaining_epochs: {adjusted_epochs}, checkpoint_resume_path: '{checkpoint_path}', resume_from_step: {checkpoint_step}, inherited_configuration: {original_config}; Redirect to new job details page: /training-jobs/{new_job_id}
    - **Container Startup with Checkpoint Resume**: New job provisions GPU (spot or on-demand as selected), Container receives environment variables: RESUME_FROM_CHECKPOINT=true, CHECKPOINT_PATH='{checkpoint_path}', RESUME_FROM_STEP={checkpoint_step}; Container downloads checkpoint from storage: GET /storage/training-checkpoints/{path}, Verify checksum integrity, Extract checkpoint contents; Load training state: Load model weights into LoRA adapters (must match rank/target_modules), Load optimizer state, Load LR scheduler state (or reinitialize if "restart schedule" selected), Restore random states for reproducibility, Verify configuration compatibility: checkpoint rank matches config rank, checkpoint target_modules match config; Apply configuration changes: If batch_size changed: Use new batch_size for data loaders, If GPU type changed: Already provisioned correct GPU type, If epochs adjusted: Update total_steps calculation, Update LR scheduler accordingly; Resume training loop: Start from step {checkpoint_step + 1}, Continue epoch progression, Send webhook: "Training resumed from checkpoint {step}", Continue normal training workflow
    - **Dashboard Display for Resumed Jobs**: Job details page shows: Status badge: "Training (Resumed from {original_job_name})", Resume indicator: "‚ñ∂Ô∏è Resumed from step {checkpoint_step} of original job", Link to original job: "View original job: {original_job_name}", Configuration changes highlighted: "Modified configuration: GPU type: Spot ‚Üí On-Demand, Batch size: 4 ‚Üí 2"; Progress tracking: Progress bar shows continuation only (0-100% of remaining work), Overall progress label: "Overall: 42% ‚Üí 100% (resumed at 42%)", Step counter: "Step 850 ‚Üí 2000 (resumed from 850)"; Cost tracking: "Original job cost: $XX.XX (first 42%)", "Continuation cost: $YY.YY (current, remaining 58%)", "Total cost: $ZZ.ZZ (both jobs combined)"; Loss curve displays: Combined loss curve showing both original and continuation training, Vertical marker at resume point: "Resumed here from checkpoint", Tooltip: "Training resumed at step 850 after {failure_reason}", Continuous curve (no break) - training continuity maintained
    - **Common Resume Scenarios**: **Scenario 1: OOM Error Recovery**: Original: Aggressive preset, batch_size=4, r=32 ‚Üí OOM at step 450, Resume: Reduce batch_size to 2, keep other params, switch to on-demand for reliability, Result: Training completes successfully, total cost $XX (original $YY wasted + continuation $ZZ); **Scenario 2: Repeated Spot Interruptions**: Original: Spot GPU, interrupted 3 times, recovery failed, Resume: Switch to on-demand GPU (guaranteed completion), keep all other config, Result: Training completes without interruption, pays premium but job finishes; **Scenario 3: User Cancellation then Resume**: Original: User cancelled at 60% to adjust strategy, Resume: Continue with same config, just want to finish, Result: Completes remaining 40%, minimal waste; **Scenario 4: Budget Optimization**: Original: On-demand GPU, user realizes cost too high, pauses/cancels, Resume: Switch to spot GPU for remaining work, accept interruption risk for cost savings, Result: Saves $XX on remaining training
    - **Resume Success Tracking**: Track resumed job outcomes: INSERT INTO resume_outcomes (original_job_id, continuation_job_id, failure_reason_original, configuration_changes, success, final_cost, timestamp); Analytics: Resume success rate: (successful_resumes / total_resumes) √ó 100, Target: >90% of resumed jobs complete successfully, Common successful patterns: "OOM + batch_size reduction: 96% success rate", "Spot interruption loop + on-demand switch: 99% success rate", "User cancellation + same config resume: 94% success rate"; Display insights to users: "Similar resume scenarios have 96% success rate with your configuration changes"
    - **Limitations and Edge Cases**: Cannot resume if: Checkpoint corrupted (checksum fails), Checkpoint older than 30 days (expired/deleted), Configuration incompatible (trying to change locked params), Training file deleted/modified (data inconsistency); Error handling: If checkpoint download fails: "Unable to download checkpoint. File may have been deleted.", Option to retry or cancel, If checkpoint incompatible: "Checkpoint configuration doesn't match. Cannot resume with different LoRA rank.", Must start new job from scratch; Warning if: Original job very old (>7 days): "‚ö†Ô∏è Original job is 12 days old. Training file or system may have changed. Recommend creating new job instead of resuming.", User can proceed anyway or create fresh job

- **FR3.3.1:** One-Click Retry with Same Configuration
  * Description: System shall implement streamlined one-click retry functionality for failed training jobs by cloning complete configurations, automatically incrementing retry counters, linking retry jobs to original failures for traceability, displaying confirmation modals with failure analysis and fresh cost estimates, providing optional configuration editing before retry initiation, automatically queuing and starting retried jobs, tracking retry success rates per error type, and optimizing for transient failures like network timeouts and provisioning delays to maximize productivity and minimize configuration overhead while maintaining clear job lineage and enabling rapid recovery from temporary infrastructure issues.
  * Impact Weighting: Productivity / User Experience / Time Savings
  * Priority: Medium
  * User Stories: US3.3.1
  * User Journey: UJ4.7.1 (One-Click Job Retry), UJ4.7.2 (Retry Success Tracking)
  * Tasks: [T-3.3.1]
  * User Story Acceptance Criteria:
    - Failed jobs show "Retry Job" button
    - Click creates new job with identical configuration:
    - Same training file
    - Same hyperparameter preset
    - Same GPU selection (spot/on-demand)
    - Same job name with suffix " (Retry #2)"
    - Confirmation modal displays:
    - **Original job**: Name, failure reason, elapsed time before failure
    - **Retry configuration**: Complete configuration summary
    - **Cost estimate**: Fresh estimate for new attempt
    - Option to edit configuration before retrying
    - After confirmation:
    - Create new job in "queued" status
    - Link to original job for reference
    - Start training automatically
    - Useful for transient errors: Network timeouts, GPU provisioning delays, spot interruptions without checkpoints
    - Track retry count per job: "This is retry #2 of job_abc123"
    - Success rate metric: "85% of retried jobs complete successfully"
  * Functional Requirements Acceptance Criteria:
    - **Retry Button Display**: "Retry Job" button appears on failed job details page, Position: Header area, next to job name, prominent placement, Styling: Secondary action button (blue/gray), icon: üîÑ, Label: "Retry Job" (simple, clear), Enabled for jobs with status IN ('failed', 'cancelled'), Disabled tooltip for successful jobs: "Job completed successfully, no retry needed", For jobs with checkpoints: "Retry Job" button appears alongside "Resume from Checkpoint" button (user can choose which approach)
    - **Button Click Confirmation Modal**: Click opens confirmation modal (not immediate retry to prevent accidents), Modal header: "Retry Training Job?", Subheader: "Create a new job with identical configuration from the failed job", Modal dismissable (can cancel)
    - **Original Job Summary Section**: Card showing failure context: Job name: "{original_job_name}", Status: "Failed" (red badge), Failure reason: "{error_type}: {error_message}" (e.g., "OutOfMemoryError: CUDA out of memory"), Failed at: "Step {step} ({percentage}% complete)", Elapsed time before failure: "{hours}h {minutes}m", Cost spent: "${cost}", Failure timestamp: "Failed {relative_time} ago (Dec 15, 2025 at 2:34 PM)", Link: "View original job details"
    - **Retry Configuration Display**: Shows complete configuration being reused: Training file: "{training_file_name}" ({conversation_count} conversations), Hyperparameter preset: "{preset_name}" badge, GPU type: "Spot H100" or "On-Demand H100", Key hyperparameters: "Rank: {r}, Learning Rate: {lr}, Epochs: {epochs}, Batch Size: {batch_size}", All metadata: Same tags, same client/project assignment, same description/notes; Visual indicator: "‚úì Configuration cloned from original job", Explanation: "This retry will use the exact same settings. For adjusted retry, use 'Retry with Suggested Fix' button."
    - **Fresh Cost Estimate**: Calculate new cost estimate (not reusing original estimate): estimated_duration = (total_steps √ó avg_seconds_per_step) / 3600 based on historical data, estimated_cost = estimated_duration √ó gpu_hourly_rate[gpu_type], Display: "Estimated duration: {min}h - {max}h", "Estimated cost: ${min} - ${max}", Comparison to original: "Original estimate: ${original_est}", "Original spent before failure: ${spent}", Warning if high cost: "‚ö†Ô∏è This retry will cost an additional ${estimate}. Consider using 'Retry with Suggested Fix' to reduce cost."
    - **Retry Reasoning Display**: Explanation of when simple retry makes sense: "Retry recommended for: ‚úì Transient network errors (timeout, connection reset), ‚úì Temporary GPU provisioning delays, ‚úì Spot interruptions without saved checkpoint (<step 100), ‚úì Infrastructure hiccups (datacenter transient issues)"; "Retry may not help for: ‚úó OutOfMemoryError (likely to fail again with same config), ‚úó Dataset validation errors (data issue not fixed), ‚úó Repeated spot interruption loops (same risk), ‚Üí For these errors, use 'Retry with Suggested Fix' or 'Resume from Checkpoint'"
    - **Configuration Edit Option**: Link/button: "Edit Configuration Before Retry", Click opens job configuration form pre-filled with settings from original job, User can modify: GPU type, hyperparameters, batch size, epochs, training file, job name, metadata, After edits, proceeds to normal job creation flow (not one-click anymore, but user requested customization); If no edits made: "Edit Configuration" option subtle/secondary, emphasize "Retry with Same Config" as primary action
    - **Job Naming Convention**: New job name auto-generated: If original job not a retry: "{original_job_name} (Retry #1)", If original job is already a retry: Increment counter: "Job Name (Retry #2)" ‚Üí "Job Name (Retry #3)", Parse retry counter from job name, increment, Editable: User can change job name in confirmation modal if desired, Default description: "Retry of job {original_job_id} which failed with {error_type}"
    - **Action Buttons**: "Retry Job" (primary, blue button): Enabled by default, Click creates retry job (workflow below), Loading state while creating: "Creating retry job..."; "Edit Configuration" (secondary, link or button): Opens configuration editor; "Cancel" (tertiary): Closes modal, returns to job details
    - **Retry Job Creation Workflow**: On "Retry Job" confirm: Deep clone original job configuration: SELECT * FROM training_jobs WHERE id = {original_job_id}, Create new job record: INSERT INTO training_jobs (id, name, description, training_file_id, configuration, gpu_pricing_tier, created_by, retry_of_job_id, retry_attempt_number, status) VALUES (new_uuid(), '{name} (Retry #{attempt})', 'Retry of {original_id}', {same_training_file}, {cloned_config}, {same_gpu_type}, {same_user}, {original_job_id}, {attempt_number}, 'queued'); Link retry to original: UPDATE original_job SET has_retry = true, latest_retry_job_id = {new_job_id}; Audit log: INSERT INTO job_retry_log (original_job_id, retry_job_id, retry_attempt, reason = 'manual_retry', timestamp); Auto-start: No additional confirmation needed, job moves to queue, provisions GPU automatically; Redirect: User navigated to new retry job details page: /training-jobs/{new_retry_job_id}
    - **Retry Job Display**: New job details page shows: Status: "Queued" (starting soon), Retry indicator badge: "Retry #2 of original job", Link to original: "Original job: {original_job_name} [View]", Configuration display: "Configuration cloned from: {original_job_id}", If original failed: "Original failure: {error_type} at step {step}", Expectation: "This retry uses identical configuration. Success depends on whether original error was transient."; Job progress: Standard monitoring once training starts, Independent progress tracking (0-100% of new attempt, not continuing from original)
    - **Retry Count Tracking**: Database field: retry_attempt_number INT, retry_of_job_id UUID (foreign key to original job), Job history shows retry chain: "Original Job ‚Üí Retry #1 ‚Üí Retry #2 ‚Üí Retry #3", Each job links to previous/next in retry chain, UI displays: "This is retry attempt #2", "Previous attempt: {job_id} (failed with {error})", "Next retry (if exists): {job_id} (status: {status})"
    - **Retry Success Tracking**: For each retry, track outcome: INSERT INTO job_retry_outcomes (original_job_id, retry_job_id, retry_attempt, original_error_type, retry_successful, retry_final_status, retry_cost, timestamp); Calculate success rates: Query: SELECT error_type, COUNT(*) as total_retries, SUM(CASE WHEN retry_successful THEN 1 ELSE 0 END) as successful_retries FROM retry_outcomes GROUP BY error_type, Compute: success_rate = (successful_retries / total_retries) √ó 100; Display insights: "85% of retried jobs complete successfully overall", "For {error_type}: {X}% retry success rate", "Your retry history: 7 of 9 retries successful (78%)"; Use data to guide users: If success rate <50% for error type: "‚ö†Ô∏è Jobs failing with {error_type} have only {X}% retry success rate. Consider 'Retry with Suggested Fix' instead.", If success rate >80%: "‚úì Jobs failing with {error_type} have {X}% retry success rate. Retry recommended."
    - **Transient Error Optimization**: Identify transient vs persistent errors: **Transient** (high retry success): Network timeout errors: 92% success, Spot interruption before checkpoint: 88% success, Provisioning delays: 94% success, Webhook delivery failures: 96% success; **Persistent** (low retry success): OutOfMemoryError: 12% success (config issue), Dataset validation errors: 8% success (data issue), Authentication failures: 5% success (credentials issue); Recommendation logic: If error_type IN (transient_errors): Primary button: "Retry Job" (recommended), Secondary: "Retry with Suggested Fix", Message: "This error is usually transient. Retry recommended."; If error_type IN (persistent_errors): Primary button: "Retry with Suggested Fix" (recommended), Secondary: "Retry Job" (not recommended), Warning: "‚ö†Ô∏è This error typically requires configuration changes. Simple retry may fail again."
    - **Retry Limit Enforcement**: Prevent infinite retry loops: Max retries per original job: 5 attempts, After 5 failed retries: Disable "Retry Job" button, Display: "Maximum retry limit reached (5 attempts). Contact support for assistance.", Alternative: "Create New Job" (fresh start, not linked as retry); Warning at attempt 3+: "This is retry attempt #3. Consider reviewing configuration or contacting support if repeated failures occur."; Admin override: Support team can bypass retry limit for special cases
    - **Cost Tracking Across Retries**: Aggregate cost of retry chain: total_retry_chain_cost = SUM(cost) for all jobs in retry chain, Display on original job: "Total cost including retries: ${total}", "Original attempt: ${cost1}, Retry #1: ${cost2}, Retry #2: ${cost3}", "Total retries cost: ${retry_total} (additional expense)"; Budget impact: Retries count toward monthly budget, Alert if excessive retry costs: "You've spent ${X} on retries this month. Review failure patterns to reduce retry costs."; Analytics: "Team retry costs: ${X} this month (Y% of total training costs)", "Cost savings opportunity: Reduce retry costs by improving configuration quality"
    - **Batch Retry** (future enhancement): Select multiple failed jobs, "Retry Selected Jobs" button, Confirmation: "Retry {X} jobs with same configurations?", Creates {X} retry jobs simultaneously, Progress indicator: "Creating retries: 5 of 12 complete", Use case: Infrastructure outage affected many jobs, one-click retry all

- **FR3.3.2:** Retry with Suggested Adjustments
  * Description: System shall implement intelligent retry functionality with context-aware configuration suggestions by analyzing failure patterns, generating evidence-based parameter adjustments, displaying configuration diffs with change highlights, providing confidence ratings for each suggestion based on historical success rates, enabling users to accept recommended fixes or manually edit configurations, tracking suggestion effectiveness to refine recommendation algorithms through machine learning, and optimizing for common failure modes like OOM errors, timeout issues, and spot interruption loops to maximize retry success rates and user learning while reducing trial-and-error cycles and unnecessary costs.
  * Impact Weighting: Success Rate / Learning / User Guidance
  * Priority: Medium
  * User Stories: US3.3.2
  * User Journey: UJ4.8.1 (Guided Error Recovery), UJ4.8.2 (Learning from Suggestions)
  * Tasks: [T-3.3.2]
  * User Story Acceptance Criteria:
    - For specific error types, offer "Retry with Suggested Fix" button
    - **OOM Error Suggestions**:
    - Reduce batch_size: 4 ‚Üí 2
    - Switch to Conservative preset
    - Highlight changes: "batch_size: 4 ~~‚Üí~~ **2**"
    - **Timeout Error Suggestions**:
    - Reduce epochs: 4 ‚Üí 3
    - Switch to Balanced preset
    - Increase checkpoint frequency
    - **Spot Interruption Loop Suggestions** (if interrupted >3 times):
    - Switch to on-demand instance
    - Accept higher cost for reliability
    - Confirmation modal shows diff of configuration changes
    - User can accept suggested fixes or manually edit
    - After retry with suggestions:
    - Track success rate of suggested fixes
    - Learn from patterns to improve future suggestions
    - Example: "Your previous job failed with OOM error. Retry with batch_size=2 (suggested) for 95% success rate?"
  * Functional Requirements Acceptance Criteria:
    - **Suggested Fix Button Display**: "Retry with Suggested Fix" button appears on failed job details page for supported error types, Position: Next to "Retry Job" button (if both present), or as primary action if suggestions available, Styling: Primary action button (green/blue), icon: üîß‚ú®, Label: "Retry with Suggested Fix" (implies intelligence/help), Enabled for: OutOfMemoryError, Spot Interruption Loop (‚â•3 interruptions), Provisioning Timeout, Dataset Validation Errors (some types), Training Timeout, Loss Plateau issues; Badge indicator: "Recommended" badge overlay on button for high-confidence suggestions
    - **Suggestion Generation Engine**: When job fails, trigger suggestion analysis: python def generate_suggestions(job_id, error_type, configuration, context): suggestions = []; Analyze error type and context: **For OutOfMemoryError**: Calculate VRAM overflow amount: shortage_gb = estimated_vram - 80; Generate suggestions ordered by effectiveness: If batch_size > 1: suggest reduce_batch_size (primary fix, 96% success rate), If rank > 16: suggest reduce_rank / switch to Conservative preset (95% success rate), If sequence_length > 2048: suggest reduce_max_length (80% success rate); **For Spot Interruption Loop** (‚â•3 interruptions): Primary: Switch to on-demand GPU (99% success rate), Secondary: Try different datacenter (if multi-region available, 75% success rate), Tertiary: Schedule for off-peak hours (85% success rate overnight); **For Provisioning Timeout**: Primary: Switch to on-demand (100% success, guaranteed), Secondary: Auto-retry with exponential backoff (82% success), Tertiary: Try different GPU type (if H100 unavailable, try A100, 70% success); **For Loss Plateau**: Reduce learning rate by 50% (68% improves training), Increase training steps/epochs by 25% (72% achieves better convergence), Switch to different optimizer (Adam ‚Üí AdamW, 64% improvement); Return suggestions ranked by: Confidence (historical success rate), Impact (how much it addresses root cause), Cost (prefer lower-cost solutions first)
    - **Retry with Suggested Fix Modal**: Click button opens comprehensive modal, Modal header: "Retry with Intelligent Fixes", Subheader: "We've analyzed your failure and recommend these changes", Icon: üîß‚ú® or ü§ñ
    - **Failure Analysis Section**: Card showing error diagnosis: Error Type: "{error_type}" (badge), Root Cause Analysis: "Your configuration exceeded GPU memory capacity by ~{X}GB", Specific Issue: "batch_size=4 with rank=32 requires ~92GB VRAM (H100 capacity: 80GB)", Impact: "Training could not proceed past step {step}", Context data: Historical pattern: "OOM errors with this configuration fail 94% of the time", Your history: "2 of your last 3 jobs with Aggressive preset had OOM errors"
    - **Suggested Fixes Section** (ranked list): **Fix #1 - Primary Recommendation** (highlighted, most prominent): Title: "Reduce Batch Size" (badge: "Recommended"), Change visualization: "batch_size: 4 ‚Üí 2" (strikethrough old value, bold new value), Diff highlighting: RED "4" crossed out, GREEN "2" highlighted, Explanation: "Reducing batch size from 4 to 2 will reduce VRAM usage by ~12GB, bringing total to 72GB (within 80GB capacity)", Impact: "+ Higher likelihood of success", "+ Prevents OOM error", "- Training will take ~15% longer (~2 hours)", Cost impact: "Cost: $48-60 (similar to original estimate)", Confidence: "95% success rate" (large, green badge), Visual confidence bar: ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñë 95%, Historical data: "427 similar jobs succeeded with this fix (95% success)", Checkbox: "‚òê Apply this fix" (checked by default); **Fix #2 - Alternative** (secondary prominence): Title: "Switch to Conservative Preset", Change: "Preset: Aggressive ‚Üí Conservative", Detailed changes: "rank: 32 ‚Üí 8", "learning_rate: 0.0003 ‚Üí 0.0002", "batch_size: 4 ‚Üí 2", Explanation: "Conservative preset uses lower rank and batch size, significantly reducing memory requirements", Impact: "+ Very safe, 98% success rate", "- Lower model capacity, may reduce quality by ~5-10%", "- Training faster (~10h instead of 14h)", Cost: "$35-45 (cheaper than original)", Confidence: "98% success rate", Checkbox: "‚òê Apply this fix" (unchecked by default, user can select instead of Fix #1); **Fix #3 - Advanced** (least prominent, collapsible): Title: "Manual Configuration Adjustments", Description: "Fine-tune parameters yourself", Button: "Edit Configuration Manually" (opens config editor with pre-filled values from original job)
    - **Configuration Diff Viewer**: Interactive diff display showing all changes, Side-by-side or inline comparison: "Original Configuration | Suggested Configuration", Changed parameters highlighted: "batch_size: 4 | 2" (old value RED, new value GREEN), Unchanged parameters grayed out: "epochs: 3" (same), Explanation tooltips: Hover over each changed parameter shows: "Why this change?", "What's the impact?", "What's the confidence?"; Visual summary: "3 parameters changed, 7 unchanged", "Estimated success rate increase: 12% ‚Üí 95%"
    - **Cost & Duration Impact**: Recalculated estimates based on suggested configuration: "Original Estimate: $48-60, 12-14 hours", "Suggested Configuration: $52-65, 13-15 hours (+$4, +1h)", Explanation: "Longer duration due to smaller batch size, but highly likely to succeed", Trade-off visualization: Success Probability: 12% ‚Üí 95% ‚úì, Duration: 12h ‚Üí 13h ‚Üë, Cost: $48 ‚Üí $52 ‚Üë, Overall: "Worth the slight increase for 95% success vs 12%"
    - **Multiple Fix Selection**: User can select multiple complementary fixes: Checkboxes allow combining: "‚òë Reduce batch_size", "‚òê Switch to Conservative preset", "‚òê Switch to on-demand GPU", System validates compatibility: If incompatible: "Cannot combine 'Conservative preset' with 'Reduce batch_size only' (Conservative already includes reduced batch_size)", Auto-adjust: If user selects Conservative preset, uncheck individual batch_size change (redundant); Combined impact calculation: Show cumulative effect of all selected fixes, "Combined success rate: 97%", "Combined cost: $55-70"
    - **Confidence Rating Methodology**: Historical success rate calculation: Query retry outcomes: SELECT COUNT(*) as total, SUM(CASE WHEN success THEN 1 ELSE 0 END) as successes FROM retry_outcomes WHERE original_error_type = {error_type} AND suggested_fix_applied = {fix_type}; Confidence = (successes / total) √ó 100; Confidence tiers: 90-100% = "Very High Confidence" (dark green), 75-89% = "High Confidence" (green), 60-74% = "Medium Confidence" (yellow), <60% = "Low Confidence" (orange), experimental fix; Minimum sample size: Require ‚â•20 historical cases before showing confidence, If <20 cases: "Confidence: Experimental (limited data)", Encourage user: "Help us improve by trying this fix and reporting results"
    - **User Decision Actions**: "Apply Suggested Fixes & Retry" (primary, green button): Disabled until ‚â•1 fix selected, Click creates retry job with suggested configuration applied, Loading state: "Creating retry job with suggested fixes..."; "Edit Manually" (secondary button): Opens full configuration editor, Pre-fills with suggested values (user can further adjust), Proceeds to standard job creation flow; "Retry Without Changes" (tertiary, link): Falls back to one-click retry (same as FR3.3.1), Warning: "Not recommended - original configuration likely to fail again"; "Cancel" (close modal)
    - **Retry Job Creation with Suggestions**: On "Apply Suggested Fixes & Retry": Clone original job configuration, Apply selected fixes to configuration object: If "reduce_batch_size" selected: config.batch_size = suggested_batch_size, If "switch_preset" selected: Apply all preset parameters (rank, lr, batch_size, etc.), If "switch_gpu_type" selected: config.gpu_pricing_tier = 'on_demand', Create new training job: INSERT training_jobs (id, name, configuration, retry_of_job_id, suggested_fixes_applied, ...) VALUES (...); Track which fixes applied: INSERT INTO applied_suggestions (retry_job_id, original_job_id, suggestion_type, suggestion_details, confidence_at_time, timestamp); Auto-start job (queues immediately); Redirect to new retry job page
    - **Retry Job Display with Suggestions**: New job details page shows: Status: "Queued (with suggested fixes)", Retry indicator: "Intelligent Retry of {original_job_name}", Applied Fixes section: "Applied Suggestions: ‚Ä¢ Batch size reduced: 4 ‚Üí 2 (95% confidence), ‚Ä¢ GPU switched: Spot ‚Üí On-Demand (99% confidence)", Expected success rate: "Based on similar fixes: 95% likely to succeed", Link: "View original failed job: {original_job_name}"; Progress tracking: Standard monitoring, Special attention flag: "Monitoring for previously failed step ({failed_step})...", Success notification: If training passes previously failed step: "‚úì Passed step {failed_step} successfully! Suggestion worked.", If completes successfully: "‚úì Training completed! Suggested fixes resolved the issue."
    - **Suggestion Effectiveness Tracking**: After retry completes (success or failure): UPDATE applied_suggestions SET retry_outcome = {status}, retry_successful = {true/false}, actual_cost = {cost}, actual_duration = {duration}, resolved_original_error = {true if different error or success}; Calculate suggestion effectiveness: For each suggestion_type: success_rate = successful_retries / total_retries_with_suggestion, Compare to baseline retry (no suggestions): improvement_rate = suggested_success_rate - simple_retry_success_rate; Use data to refine future suggestions: If fix consistently successful (>90%): Promote to "Recommended" tier, Increase confidence display, If fix rarely successful (<50%): Demote or remove from suggestions, Flag for engineering review: "batch_size=1 suggestion only 45% effective, investigate"
    - **Learning & Improvement Loop**: Machine learning-driven suggestion refinement (future): Feed retry outcomes back into recommendation engine, Train model to predict: Which fixes most likely to work for given error + configuration, Optimal parameter values (not just "reduce batch_size" but "reduce to X specifically"), Expected success rate per suggested configuration; A/B testing: Randomly assign some users alternative suggestions, Compare effectiveness, Roll out winning suggestions to all users; User feedback: "Was this suggestion helpful? [Yes] [No, I needed different fix]", If "No": "What fix worked for you?" (free text or dropdown), Use feedback to improve suggestions
    - **Error-Specific Suggestion Templates**: **OutOfMemoryError**: Primary: Reduce batch_size (calculate exact safe value), Secondary: Switch to Conservative preset, Tertiary: Reduce sequence length if applicable; **Spot Interruption Loop**: Primary: Switch to on-demand, Secondary: Schedule for off-peak hours, Tertiary: Accept longer completion time with continued retries; **Provisioning Timeout**: Primary: Switch to on-demand (guaranteed), Secondary: Auto-retry with backoff, Tertiary: Wait and retry during off-peak; **Dataset Validation Error**: Primary: Fix specific data issues (provide editing links), Secondary: Remove invalid conversations from training file, Tertiary: Regenerate training file; **Loss Plateau**: Primary: Reduce learning rate, Secondary: Increase training duration, Tertiary: Switch optimizer; **Training Timeout**: Primary: Reduce epochs, Secondary: Reduce dataset size, Tertiary: Switch to faster preset
    - **Suggestion Explanation & Education**: Each suggestion includes educational content: "Why does this help?" section: "Reducing batch_size decreases the amount of data processed simultaneously, which reduces memory consumption. Smaller batches fit within GPU memory limits.", "Trade-offs to consider": "Smaller batches mean more gradient updates per epoch, which can actually improve model quality but takes longer.", "Learn more": Link to documentation "Understanding Batch Size in LoRA Training"; Help users understand not just what to change, but why: Builds user expertise over time, Reduces repeat errors in future jobs, Empowers users to make informed decisions
    - **Team-Wide Suggestion Insights**: Manager/admin view shows: "Top Suggested Fixes This Month: Reduce batch_size (applied 23 times, 95% success), Switch to on-demand (applied 12 times, 100% success)", "Most Effective Suggestions: On-demand GPU for spot loops: 100% success, Batch size reduction for OOM: 95% success", "Suggestion Adoption Rate: 67% of users accept suggested fixes (vs 33% retry without changes)", "Cost Savings from Suggestions: ${X} saved by preventing repeated failures"; Team training opportunities: "Top suggestion needed: Reduce batch_size (18 times this month)", "Training recommendation: Educate team on VRAM management to reduce OOM errors"
