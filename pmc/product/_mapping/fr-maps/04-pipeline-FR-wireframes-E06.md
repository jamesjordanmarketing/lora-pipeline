# LoRA Pipeline - Functional Requirements
**Version:** 2.0.0  
**Date:** 12/16/2025  
**Category:** Design System Platform
**Product Abbreviation:** pipeline

**Source References:**
- Seed Story: `pmc\product\00-pipeline-seed-story.md`
- Overview Document: `pmc\product\01-pipeline-overview.md`
- User Stories: `pmc\product\02-pipeline-user-stories.md`


## 6. Model Quality Validation

- **FR6.1.1:** Calculate Perplexity Improvement
  * Description: System shall implement comprehensive automated perplexity evaluation during training finalization by testing both baseline Llama 3 70B and trained model with LoRA adapters on identical held-out validation sets, calculating perplexity scores using cross-entropy loss over token predictions, computing improvement percentages, applying quality threshold classifications (production-ready â‰¥30%, acceptable 20-29%, below threshold <20%), displaying results with visual comparisons, exporting detailed metrics, and tracking perplexity trends across training runs to provide objective, quantifiable measures of model improvement that serve as primary quality gates for production deployment and client deliverables.
  * Impact Weighting: Quality Assurance / Objective Measurement / Client Proof
  * Priority: High
  * User Stories: US6.1.1
  * User Journey: UJ5.1.1 (Perplexity Evaluation), UJ5.1.2 (Quality Gate Assessment)
  * Tasks: [T-6.1.1]
  * User Story Acceptance Criteria:
    - **Automatic Perplexity Calculation** (runs during training finalization):
    - Test baseline Llama 3 70B on validation set (20% held-out data, ~48 conversations)
    - Test trained model (base + LoRA adapters) on same validation set
    - Calculate perplexity for both models
    - Compute improvement percentage: ((baseline - trained) / baseline) Ã— 100%
    - **Results Display** on job details page:
    - Baseline perplexity: 24.5 (Llama 3 70B without fine-tuning)
    - Trained perplexity: 16.8 (Llama 3 70B + LoRA adapters)
    - Improvement: **31.4%** (green badge if â‰¥30%, yellow if 20-29%, red if <20%)
    - Interpretation: "31% lower perplexity indicates significantly better prediction quality"
    - **Target Threshold**:
    - Production-ready: â‰¥30% improvement
    - Acceptable: 20-29% improvement
    - Needs retry: <20% improvement
    - **Quality Badge**:
    - "âœ“ Production Ready" (â‰¥30% improvement)
    - "âš  Acceptable Quality" (20-29% improvement)
    - "âœ— Below Threshold" (<20% improvement)
    - Include perplexity chart: Bar chart comparing baseline vs trained
    - Export perplexity data with validation report
    - Perplexity trend: Track across multiple training runs to identify improvements
  * Functional Requirements Acceptance Criteria:
    - **Automatic Triggering**: Perplexity evaluation initiates automatically during finalization stage, After final training epoch completes and final adapter saved, Before job marked as 'completed', Job status updates to 'running_validation' with substatus "Calculating perplexity...", Duration estimate: 10-20 minutes for 48 validation conversations
    - **Validation Set Selection**: Training file split: 80% training (194 conversations), 20% validation (48 conversations), Split deterministic: Same conversations always in validation set (seeded by training_file_id), Validation set stored in database during preprocessing: validation_conversation_ids JSONB array in training_jobs table, Validation conversations not used during training (held out), Stratified sampling: Proportional representation of personas, emotional arcs, topics in validation set
    - **Baseline Model Evaluation**: Load base Llama 3 70B model (no adapters): Use cached model if available, 4-bit quantization for consistency with training, Run inference on all 48 validation conversations: For each conversation: For each training pair (prompt_context + user_query â†’ target_response), Tokenize prompt and target with Llama 3 tokenizer, Calculate log probabilities: model(prompt) â†’ logits for target tokens, Compute cross-entropy loss: -log(P(target | prompt)), Store per-token losses; Aggregate perplexity: total_loss = SUM(cross_entropy_loss for all tokens in all validation pairs), total_tokens = COUNT(tokens in all target_responses), average_loss = total_loss / total_tokens, baseline_perplexity = exp(average_loss); Typical baseline range: 20-30 for financial advice domain (Llama 3 70B general knowledge)
    - **Trained Model Evaluation**: Load trained model: Base Llama 3 70B + trained LoRA adapters (adapter_model.bin), Merge adapters into base model for inference, Run identical validation evaluation: Same 48 conversations, Same prompts, Same scoring methodology, Calculate trained_perplexity = exp(average_loss_trained); Expected trained range: 12-20 (significant improvement from fine-tuning)
    - **Improvement Calculation**: Compute improvement percentage: improvement_pct = ((baseline_perplexity - trained_perplexity) / baseline_perplexity) Ã— 100, Example: (24.5 - 16.8) / 24.5 = 0.314 = 31.4% improvement, Positive improvement = trained model better (lower perplexity), Negative improvement = trained model worse (regression, rare), Store in database: UPDATE training_jobs SET baseline_perplexity = 24.5, trained_perplexity = 16.8, perplexity_improvement_pct = 31.4, perplexity_calculated_at = NOW()
    - **Quality Threshold Classification**: Apply thresholds: IF improvement_pct >= 30: quality_tier = 'production_ready', badge_color = green, badge_text = "âœ“ Production Ready", recommendation = "Model exceeds quality threshold. Ready for client delivery."; ELSIF improvement_pct >= 20: quality_tier = 'acceptable', badge_color = yellow, badge_text = "âš  Acceptable Quality", recommendation = "Model meets minimum quality standards. Review with team before delivery."; ELSE: quality_tier = 'below_threshold', badge_color = red, badge_text = "âœ— Below Threshold", recommendation = "Model quality insufficient. Consider retraining with more data or different configuration."; Store: UPDATE training_jobs SET perplexity_quality_tier = quality_tier
    - **Results Display on Job Details Page**: **Perplexity Section** (dedicated card on dashboard): Card header: "Model Quality: Perplexity Improvement", Quality badge: Large prominent badge with tier icon and text, Results grid (3 columns): Column 1 - Baseline Perplexity: "24.5" (large number), "Llama 3 70B without fine-tuning" (subtitle), Info tooltip: "Lower perplexity = better. Perplexity measures how surprised the model is by the validation data."; Column 2 - Trained Perplexity: "16.8" (large number, green if improved), "Llama 3 70B + LoRA adapters" (subtitle), Difference indicator: "â†“ 7.7 points lower"; Column 3 - Improvement: "31.4%" (huge, bold, green), "Production Ready" (quality tier), Target: "Target: â‰¥30%" (met); Interpretation text: "31% lower perplexity indicates significantly better prediction quality", "This model is ready for production deployment and client delivery."
    - **Perplexity Comparison Chart**: Bar chart visualization: X-axis: "Baseline" vs "Trained", Y-axis: Perplexity score (0-30 scale), Baseline bar: Blue, height 24.5, Trained bar: Green, height 16.8, Improvement arrow: Red downward arrow showing 31.4% reduction, Target line: Horizontal dashed line at 30% improvement threshold (production-ready marker); Chart interactive: Hover shows exact values, Click to expand full-screen; Chart export: Download as PNG image for reports
    - **Detailed Metrics Expandable Section**: "View Detailed Perplexity Breakdown" link expands section showing: Total validation pairs evaluated: 187 pairs across 48 conversations, Total tokens scored: 23,456 tokens, Average tokens per response: 125 tokens, Baseline metrics: Total loss: 76,234, Average loss: 3.25, Perplexity: exp(3.25) = 24.5; Trained metrics: Total loss: 68,012, Average loss: 2.90, Perplexity: exp(2.90) = 16.8; Improvement: Absolute: -0.35 loss units, Relative: 31.4% perplexity reduction; Per-conversation min/max: Best improvement: Conversation #42 (45% improvement), Worst improvement: Conversation #7 (18% improvement), Link to per-conversation breakdown (FR6.1.2)
    - **Quality Gate Decision Workflow**: If quality_tier = 'below_threshold': Display warning banner: "âš ï¸ Model Quality Below Threshold", "Perplexity improvement (X%) below target (30%). This model may not meet production standards.", Recommendations: "Consider: Adding more training data (+20-30 conversations), Retraining with different hyperparameters (try Conservative preset), Reviewing training data quality (low-quality data degrades performance), Extending training (add 1-2 epochs)"; Actions: [Retry Training with Suggestions] [Review Training Data] [Accept Anyway (with justification)]; Require manager approval for delivery if below threshold; If quality_tier = 'acceptable': Display caution banner: "Model Quality Acceptable (20-30% improvement)", "Meets minimum standards but below optimal threshold.", Recommendation: "Review with team before client delivery. Consider A/B testing or gradual rollout."; If quality_tier = 'production_ready': Display success banner: "âœ“ Model Quality Exceeds Standards", "Ready for immediate production deployment."; Auto-approve for delivery (no manual review needed)
    - **Export Perplexity Data**: "Export Perplexity Report" button: Generates comprehensive PDF report: Cover page: Job name, training date, quality tier badge, Executive summary: Overall improvement, quality tier, recommendation, Detailed metrics: Baseline/trained perplexity, improvement %, validation set details, Per-conversation breakdown table (if >20 conversations, top/bottom 10 shown), Visual charts: Bar chart, improvement trend chart (if multiple runs), Methodology: Explanation of perplexity calculation, Validation set composition; CSV export option: Job ID, Baseline Perplexity, Trained Perplexity, Improvement %, Quality Tier, Validation Pairs, Total Tokens, Calculated At; Export includes in validation artifact bundle: training-job-{id}-validation-report.pdf
    - **Perplexity Trend Tracking**: Store historical perplexity data: INSERT INTO perplexity_history (job_id, training_file_id, baseline_perplexity, trained_perplexity, improvement_pct, quality_tier, timestamp); Query trends for same training file across multiple training runs: SELECT job_id, improvement_pct, quality_tier FROM perplexity_history WHERE training_file_id = {id} ORDER BY timestamp DESC; Display trend chart on job details: Line graph showing improvement % over time (X-axis: job creation date, Y-axis: improvement %), Identify improvements: "Previous run: 28% improvement, This run: 31% improvement (â†‘ 3 percentage points)", Show optimal configuration: "Best result: 34% improvement (Balanced preset, 3 epochs, job #abc123)"; Use trends to guide future training: "ðŸ’¡ Insight: Balanced preset consistently achieves 30-35% improvement for this training file. Recommend using Balanced for future runs."
    - **Team Analytics**: Aggregate perplexity metrics across team: Average perplexity improvement: 28.5% across 47 completed jobs, Distribution: Production-ready (â‰¥30%): 23 jobs (49%), Acceptable (20-29%): 19 jobs (40%), Below threshold (<20%): 5 jobs (11%); Identify patterns: Best-performing training files: High-quality datasets with 30+ conversations typically achieve 32-38% improvement, Problematic configurations: Aggressive preset with <200 conversations: only 18% average improvement (insufficient data), Improvement correlation: More training data (>250 conversations) correlates with +5-8% higher improvement; Team goals: Target: 75% of jobs reach production-ready tier (currently 49%), Action plan: Improve training data quality, Provide hyperparameter guidance, Share best practices from high-performing jobs

- **FR6.1.2:** Perplexity by Category Analysis
  * Description: System shall implement granular perplexity analysis segmented by conversation scaffolding categories including persona types, emotional arcs, and training topics by calculating category-specific baseline and trained perplexity scores, computing per-category improvements, identifying high-performing and underperforming segments, generating visual heatmaps showing persona-emotional arc intersections, producing actionable recommendations for training data augmentation, and exporting detailed breakdowns to enable data-driven iteration, targeted dataset improvements, and strategic training file optimization based on quantified quality gaps across different conversation types.
  * Impact Weighting: Quality Insights / Data-Driven Iteration / Targeted Improvement
  * Priority: Medium
  * User Stories: US6.1.2
  * User Journey: UJ5.2.1 (Category-Level Quality Analysis), UJ5.2.2 (Training Data Optimization)
  * Tasks: [T-6.1.2]
  * User Story Acceptance Criteria:
    - **Perplexity by Persona**:
    - Table: Persona, Baseline Perplexity, Trained Perplexity, Improvement %
    - Example: "Anxious Investor: 26.3 â†’ 15.2 (42% improvement)"
    - Highlight personas with best/worst improvement
    - **Perplexity by Emotional Arc**:
    - Triumph arc: 23.1 â†’ 15.8 (32% improvement)
    - Struggle-to-Success: 25.7 â†’ 17.2 (33% improvement)
    - Identify arcs needing more training coverage
    - **Perplexity by Training Topic**:
    - Retirement Planning: 22.5 â†’ 14.9 (34% improvement)
    - Tax Strategies: 28.3 â†’ 19.1 (32% improvement)
    - Identify topics with lower improvement (need more training data)
    - **Visual Heatmap**:
    - Persona (rows) Ã— Emotional Arc (columns)
    - Cell color: Green (high improvement), Yellow (medium), Red (low)
    - Quick identification of weak areas
    - **Recommendations**:
    - "Add 10+ more 'Pragmatic Skeptic + Anxiety' conversations for better coverage"
    - "Tax Strategies topic shows lower improvement - consider adding specialized training data"
    - Export detailed category analysis as CSV
    - Use insights to improve future training datasets
  * Functional Requirements Acceptance Criteria:
    - **Category Data Extraction**: During perplexity calculation, track metadata for each validation conversation: conversation_id, persona, emotional_arc, primary_topic (from scaffolding_metadata), baseline_perplexity_this_conversation, trained_perplexity_this_conversation; Store in perplexity_by_conversation table: INSERT perplexity_by_conversation (job_id, conversation_id, persona, emotional_arc, topic, baseline_ppl, trained_ppl, improvement_pct); Aggregate by categories after all validation conversations evaluated
    - **Perplexity by Persona Analysis**: Query aggregation: SELECT persona, AVG(baseline_ppl) as avg_baseline, AVG(trained_ppl) as avg_trained, AVG(improvement_pct) as avg_improvement, COUNT(*) as conversation_count FROM perplexity_by_conversation WHERE job_id = {id} GROUP BY persona ORDER BY avg_improvement DESC; Results table display: **Persona | Baseline PPL | Trained PPL | Improvement | Count**; Example rows: "Anxious Investor | 26.3 | 15.2 | 42.2% â†‘ | 12 convos", "Pragmatic Skeptic | 24.1 | 18.5 | 23.2% | 8 convos", "Hopeful Planner | 23.8 | 16.1 | 32.4% | 10 convos", "Overwhelmed Procrastinator | 27.5 | 19.8 | 28.0% | 9 convos", "Detail-Oriented Analyzer | 22.9 | 15.7 | 31.4% | 9 convos"; Visual indicators: Green highlight: Improvement â‰¥35% (excellent), Yellow: 25-34% (good), Red: <25% (needs improvement), Best performer badge: "ðŸ† Best" next to highest improvement persona, Worst performer warning: "âš ï¸ Needs Attention" next to lowest; Sort options: "Sort by: [Improvement â–¼] [Baseline PPL] [Trained PPL] [Count]"
    - **Perplexity by Emotional Arc Analysis**: Similar aggregation: GROUP BY emotional_arc; Results table: **Emotional Arc | Baseline PPL | Trained PPL | Improvement | Count**; Example: "Triumph | 23.1 | 15.8 | 31.6% | 14 convos", "Struggle-to-Success | 25.7 | 17.2 | 33.1% | 12 convos", "Anxiety | 27.2 | 18.9 | 30.5% | 11 convos", "Skepticism-to-Trust | 24.5 | 16.3 | 33.5% | 11 convos"; Identify underperforming arcs: If improvement <25%: Flag as "Needs more training coverage", Example: "Anxiety arc: 22% improvement (below average 31%). Consider adding 5-10 more Anxiety conversations to improve model performance on anxious clients."
    - **Perplexity by Training Topic Analysis**: Aggregation: GROUP BY primary_topic; Results table: **Topic | Baseline PPL | Trained PPL | Improvement | Count**; Example: "Retirement Planning | 22.5 | 14.9 | 33.8% | 16 convos", "Tax Strategies | 28.3 | 19.1 | 32.5% | 10 convos", "Investment Advice | 24.7 | 16.2 | 34.4% | 12 convos", "Debt Management | 26.1 | 18.4 | 29.5% | 10 convos"; Topic-specific insights: High baseline perplexity = inherently complex topic (e.g., Tax Strategies: 28.3), Even with training, may remain higher than simpler topics, Focus on improvement %, not absolute perplexity; Flag topics with low improvement: "Debt Management: 29.5% improvement (below 30% target). Consider: Adding 5+ more debt-focused conversations, Reviewing quality of existing debt conversations, Ensuring diverse debt scenarios (student loans, credit cards, mortgages)"
    - **Visual Heatmap: Persona Ã— Emotional Arc**: Matrix visualization: Rows: All personas (5-8 personas), Columns: All emotional arcs (4-6 arcs), Cells: Color-coded by improvement %; Cell coloring: Green (dark): â‰¥40% improvement, Green (light): 35-39%, Yellow: 30-34%, Orange: 25-29%, Red: <25%, Gray: No data (persona-arc combination not in validation set); Cell contents: Display improvement %: "42%", Show conversation count: "(3)", Hover tooltip: "Anxious Investor + Triumph: 42.2% improvement, 3 conversations, Baseline: 26.8, Trained: 15.5"; Heatmap insights: Quick visual scan identifies: Strong areas (green clusters): Model performs well, Weak areas (red cells): Need more training data for this combination, Data gaps (gray cells): Combinations not represented in training/validation; Interactive: Click cell to view specific conversations in that category, Filter: "Show only cells with <30% improvement" (highlights problems)
    - **Data Gap Identification**: Analyze validation set coverage: Count conversations per persona: Min: 7, Max: 15, Average: 10; Count conversations per emotional arc: Min: 9, Max: 15, Average: 11; Count conversations per persona-arc combination: Many combinations: 0-2 conversations (insufficient coverage), Few combinations: 5+ conversations (good coverage); Flag combinations with <3 conversations: "âš ï¸ Low Coverage: Pragmatic Skeptic + Anxiety (2 conversations)", "Model may not generalize well to this combination due to limited training data"
    - **Actionable Recommendations Generation**: Algorithm analyzes category results and generates specific recommendations: **Recommendation Type 1: Low Improvement Categories**: IF persona_improvement < 25%: "Add 10+ more '{persona}' conversations to improve model performance for this client type (currently {X}% improvement, target â‰¥30%)"; IF emotional_arc_improvement < 25%: "Focus on '{emotional_arc}' scenarios. Current improvement ({X}%) below target. Add 5-8 conversations with this arc."; IF topic_improvement < 28%: "'{topic}' shows lower improvement ({X}%). Consider: Adding specialized training data for this topic, Reviewing existing conversations for quality issues, Consulting subject matter expert to improve conversation realism"; **Recommendation Type 2: Data Gaps**: IF persona_arc_combination_count < 3: "Low coverage for '{persona} + {emotional_arc}' (only {count} conversations). Add 5+ conversations with this combination to improve generalization."; **Recommendation Type 3: Best Practices**: Identify highest-performing combinations: "'{persona} + {emotional_arc}' achieves {X}% improvement (best performance). This combination serves as quality benchmark. Use similar conversation structures for other categories."; **Recommendation Type 4: Sample Size Issues**: IF persona_count < 8: "Only {count} '{persona}' conversations in validation set. Consider adding more to increase statistical confidence."; Display recommendations section: "ðŸ“Š Data-Driven Recommendations for Next Training Run:", Numbered list of 5-10 actionable recommendations, Prioritized by impact: Critical (blocks production), High impact (significantly improves quality), Medium impact (incremental improvement); Each recommendation includes: Issue description, Current metrics, Target metrics, Specific action, Expected impact
    - **Category Analysis Export**: "Export Category Analysis" button: CSV format includes all granular data: **Persona-level**: persona, baseline_ppl, trained_ppl, improvement_pct, conversation_count; **Emotional Arc-level**: emotional_arc, baseline_ppl, trained_ppl, improvement_pct, conversation_count; **Topic-level**: topic, baseline_ppl, trained_ppl, improvement_pct, conversation_count; **Persona-Arc Matrix**: All combinations with improvement scores; **Per-Conversation Detail**: conversation_id, persona, emotional_arc, topic, baseline_ppl, trained_ppl, improvement_pct; Filename: `training-job-{id}-category-analysis-{timestamp}.csv`; PDF report includes: Summary tables for each category type, Heatmap visualization (full-color rendered), Recommendations section, Statistical analysis (standard deviations, confidence intervals if sufficient sample size)
    - **Insights for Future Training**: System learns from category analysis across multiple jobs: Store category performance: INSERT INTO category_performance_history (training_file_id, job_id, persona, emotional_arc, topic, improvement_pct); Aggregate insights: "Across all training runs for this file:", "Anxious Investor persona: Average 38% improvement (consistently strong)", "Pragmatic Skeptic persona: Average 24% improvement (consistently weak - needs more data)", "Triumph arc: Average 33% improvement (strong)", "Anxiety arc: Average 27% improvement (acceptable but could improve)"; Proactive suggestions during training file creation: When user creates new training file: "ðŸ’¡ Insight: Based on historical data, 'Pragmatic Skeptic' conversations typically show lower improvement. Consider adding 15+ Pragmatic Skeptic conversations (currently have 8) to achieve optimal results."; Smart training file recommendations: "Recommended persona distribution for optimal quality: Anxious Investor: 15-20%, Pragmatic Skeptic: 20-25% (needs more to reach parity), Hopeful Planner: 15-20%, Overwhelmed Procrastinator: 15-20%, Detail-Oriented Analyzer: 15-20%"
    - **Comparison Across Training Runs**: If user has multiple completed jobs with same training file: Display trend charts: "Perplexity Improvement by Persona Over Time", Line graph: X-axis: Job date, Y-axis: Improvement %, Separate line per persona, Identify improvements: "Pragmatic Skeptic improvement increased from 22% (first run) to 28% (this run) after adding 10 more conversations", Or identify regressions: "Anxious Investor dropped from 42% to 35% - investigate why (different hyperparameters? data quality change?)"; Use comparisons to validate training strategies: "Adding 10 Pragmatic Skeptic conversations improved this persona's performance by +6 percentage points. Strategy validated."

- **FR6.2.1:** Run Emotional Intelligence Benchmarks
  * Description: System shall implement comprehensive emotional intelligence evaluation using curated 50-scenario test suite covering empathy detection, emotional awareness, supportive responses, and conflict handling with difficulty stratification, generating responses from both baseline and trained models for side-by-side comparison, scoring responses across empathy, clarity, and appropriateness dimensions using automated LLM-as-judge methodology with human validation sampling, calculating aggregate improvements, presenting before/after examples showcasing quality gains, assigning quality tier badges based on improvement thresholds, and exporting detailed evaluation results to provide objective EI metrics for client proof, sales enablement, and quality assurance.
  * Impact Weighting: Client Proof / Quality Assurance / Sales Enablement
  * Priority: High
  * User Stories: US6.2.1
  * User Journey: UJ5.3.1 (EI Benchmark Evaluation), UJ5.3.2 (Client Quality Demonstration)
  * Tasks: [T-6.2.1]
  * User Story Acceptance Criteria:
    - **Test Suite**: Curated set of 50 emotional intelligence scenarios
    - Categories: Empathy detection (15 scenarios), Emotional awareness (15), Supportive responses (10), Conflict handling (10)
    - Difficulty levels: Easy (20), Medium (20), Hard (10)
    - Cover personas: Anxious Investor, Pragmatic Skeptic, Hopeful Planner, etc.
    - **Validation Process**:
    - Run baseline Llama 3 70B on all 50 scenarios
    - Run trained model on same 50 scenarios
    - Capture responses for side-by-side comparison
    - **Human Evaluation** (or automated LLM-as-judge):
    - Score each response 1-5 on:
    - Empathy: Recognizes and validates emotions
    - Clarity: Clear, understandable explanations
    - Appropriateness: Tone matches situation
    - Calculate aggregate scores: Baseline avg vs Trained avg
    - **Results Display**:
    - Overall Emotional Intelligence Score: 3.2/5 (baseline) â†’ 4.5/5 (trained) = **41% improvement**
    - Empathy subscore: 3.1 â†’ 4.6 (48% improvement)
    - Clarity subscore: 3.4 â†’ 4.5 (32% improvement)
    - Appropriateness subscore: 3.1 â†’ 4.4 (42% improvement)
    - **Before/After Examples**:
    - Display 10 best improvements
    - Scenario prompt, baseline response, trained response, improvement notes
    - Example: "Scenario: Client anxious about market volatility. Baseline response: Generic advice. Trained response: Empathetic acknowledgment + specific reassurance + action plan."
    - **Quality Badge**:
    - "âœ“ Exceptional EI" (â‰¥40% improvement)
    - "âœ“ Strong EI" (30-39% improvement)
    - "âš  Moderate EI" (20-29% improvement)
    - "âœ— Needs Improvement" (<20% improvement)
    - Include EI validation in training completion report
    - Export full evaluation results (all 50 scenarios) as CSV
  * Functional Requirements Acceptance Criteria:
    - **EI Test Suite Management**: Test suite stored in database: ei_test_scenarios table with fields (scenario_id, category, difficulty, persona, prompt, ideal_response_characteristics, evaluation_criteria), 50 scenarios curated by domain experts covering financial advisor emotional intelligence situations, Scenario categories: Empathy Detection (15): Recognizing client emotions from subtle cues, Emotional Awareness (15): Demonstrating understanding of client emotional state, Supportive Responses (10): Providing comfort and reassurance, Conflict Handling (10): De-escalating tense situations; Difficulty stratification: Easy (20): Clear emotional signals, straightforward responses needed, Medium (20): Nuanced emotions, requires balance of multiple factors, Hard (10): Complex emotional situations, high stakes, multiple conflicting needs; Test suite versioning: v1.0 initial suite, periodic updates with new scenarios based on real client interactions
    - **Baseline Model Evaluation**: After perplexity calculation completes, run EI benchmarks, Load baseline Llama 3 70B (no adapters), For each of 50 scenarios: Generate response: model(scenario_prompt), Max tokens: 250, Temperature: 0.7 (slight randomness for natural responses), Store baseline_response in ei_evaluation_results table; Processing time: ~10-15 minutes for 50 scenarios
    - **Trained Model Evaluation**: Load trained model (base + LoRA adapters), Generate 50 responses with identical settings, Store trained_response; Side-by-side storage: scenario_id, baseline_response, trained_response for comparison
    - **Automated LLM-as-Judge Scoring**: Use GPT-4 or Claude as evaluator (cost-effective, consistent): For each scenario response pair: Evaluation prompt template: "You are evaluating financial advisor responses for emotional intelligence. Rate the following response on a 1-5 scale for each dimension: [Scenario Context: {scenario}], [Response to Evaluate: {response}], [Dimensions], Empathy (1-5): Does the response recognize and validate the client's emotions?, Criteria: 1=Ignores emotions, 3=Acknowledges emotions, 5=Deeply validates and reflects emotions, Clarity (1-5): Is the explanation clear and understandable?, Criteria: 1=Confusing/jargon-heavy, 3=Clear but basic, 5=Crystal clear with examples, Appropriateness (1-5): Does the tone match the situation?, Criteria: 1=Tone mismatch, 3=Appropriate tone, 5=Perfectly calibrated tone, [Provide scores in JSON format: {empathy: X, clarity: Y, appropriateness: Z, reasoning: '...'}]"; Send to LLM API, parse JSON response, Store scores; Repeat for both baseline and trained responses; Processing: ~20-30 minutes for 100 responses (50 baseline + 50 trained)
    - **Human Validation Sampling** (quality check): Randomly sample 10 of 50 scenarios (20%), Human reviewer independently scores the 10 scenario responses, Compare human scores vs LLM-as-judge scores, Calculate inter-rater reliability (Cohen's kappa or correlation), If reliability >0.7: LLM-as-judge validated, use for all scenarios, If reliability <0.7: Flag for review, may need human scoring for more scenarios; Periodic human validation ensures LLM scoring remains accurate
    - **Aggregate Score Calculation**: For each dimension (empathy, clarity, appropriateness): baseline_avg = AVG(baseline_scores for dimension), trained_avg = AVG(trained_scores for dimension), improvement_pct = ((trained_avg - baseline_avg) / baseline_avg) Ã— 100; Overall EI score: overall_baseline = AVG(empathy, clarity, appropriateness scores), overall_trained = AVG(empathy, clarity, appropriateness scores), overall_improvement_pct = ((overall_trained - overall_baseline) / overall_baseline) Ã— 100; Store: UPDATE training_jobs SET ei_baseline_score = overall_baseline, ei_trained_score = overall_trained, ei_improvement_pct = overall_improvement_pct
    - **Results Display on Job Details**: **EI Benchmark Card**: Header: "Emotional Intelligence Benchmarks", Quality badge: Large tier badge based on improvement %; Overall EI Score section: "3.2/5 â†’ 4.5/5 = 41% improvement" (large, prominent), Visualization: Horizontal bar showing baseline (gray) vs trained (green) with improvement arrow; Dimension Breakdown (3 sub-cards): Empathy: "3.1/5 â†’ 4.6/5 (48% improvement) â†‘", Clarity: "3.4/5 â†’ 4.5/5 (32% improvement) â†‘", Appropriateness: "3.1/5 â†’ 4.4/5 (42% improvement) â†‘"; Radar chart: 3-axis radar (empathy, clarity, appropriateness), Baseline (blue outline), Trained (green filled), Visual shows improvement across all dimensions
    - **Quality Tier Classification**: IF ei_improvement_pct >= 40: tier = 'exceptional', badge = "âœ“ Exceptional EI", color = dark_green, message = "Model demonstrates exceptional emotional intelligence improvements. Ideal for client-facing applications."; ELSIF ei_improvement_pct >= 30: tier = 'strong', badge = "âœ“ Strong EI", color = green, message = "Model shows strong emotional intelligence. Ready for production."; ELSIF ei_improvement_pct >= 20: tier = 'moderate', badge = "âš  Moderate EI", color = yellow, message = "Model meets minimum EI standards. Review before client deployment."; ELSE: tier = 'needs_improvement', badge = "âœ— Needs Improvement", color = red, message = "EI improvement insufficient. Consider retraining with more emotionally-focused training data."
    - **Before/After Examples Section**: Display top 10 improvements (highest score delta): For each example: Scenario prompt (truncated): "Client anxious about market volatility asks: 'Should I sell everything?'", Baseline response (card): "Market volatility is normal. Diversification helps manage risk. Consider your long-term goals.", Baseline scores: "Empathy: 2/5, Clarity: 3/5, Appropriateness: 3/5, Total: 2.7/5"; Trained response (card): "I hear your concern - market volatility can be really unsettling, especially when your financial future feels at stake. Let's take a breath together and look at your specific situation. First, selling everything now could lock in losses. Your portfolio is diversified across [sectors], which cushions volatility. We built this strategy for exactly these moments. What if we review your goals and timeline together? Often, staying the course is the wisest path, but I want to make sure you feel confident in that decision.", Trained scores: "Empathy: 5/5 (validates emotions explicitly), Clarity: 4/5 (clear explanation with specifics), Appropriateness: 5/5 (reassuring yet honest tone), Total: 4.7/5"; Improvement: "+2.0 points (74% improvement)", Improvement notes: "Trained model explicitly validates anxiety, provides specific portfolio details, offers collaborative problem-solving, uses reassuring language while being realistic"; "View All 50 Scenarios" link expands to full comparison table
    - **Category-Level Analysis**: Break down by scenario category: Empathy Detection (15 scenarios): 3.0 â†’ 4.6 (53% improvement) - Strongest area, Emotional Awareness (15): 3.3 â†’ 4.5 (36% improvement), Supportive Responses (10): 3.1 â†’ 4.4 (42% improvement), Conflict Handling (10): 3.4 â†’ 4.3 (26% improvement) - Weakest area; Insights: "Model excels at empathy detection and supportive responses but shows lower improvement in conflict handling. Consider adding more conflict resolution training scenarios."; Difficulty-Level Analysis: Easy (20): 3.8 â†’ 4.7 (24% improvement), Medium (20): 3.1 â†’ 4.5 (45% improvement), Hard (10): 2.6 â†’ 4.2 (62% improvement); Insight: "Model shows greatest improvement on hard scenarios, suggesting strong learning of complex emotional intelligence patterns"
    - **Export EI Evaluation Results**: CSV export includes all 50 scenarios: Columns: scenario_id, category, difficulty, prompt, baseline_response, trained_response, baseline_empathy, baseline_clarity, baseline_appropriateness, baseline_total, trained_empathy, trained_clarity, trained_appropriateness, trained_total, improvement_points, improvement_pct, improvement_notes; Filename: `training-job-{id}-ei-benchmarks-{timestamp}.csv`; PDF Report: Executive summary with overall scores and tier, Category breakdown with insights, Top 10 before/after examples (formatted nicely), Full scenario comparison table (appendix), Methodology explanation, Recommendations for future training; Include in validation artifact bundle
    - **Client Sales Enablement**: EI benchmark results designed for client presentations: Professional PDF report with Bright Run branding, Executive summary suitable for C-suite: "Your custom AI financial advisor achieved 41% improvement in emotional intelligence, exceeding industry benchmarks. This translates to more empathetic client interactions, higher satisfaction, and stronger trust."; Before/after examples demonstrate tangible value: "See the difference: Your AI now recognizes client anxiety and responds with validation and specific reassurance, unlike generic baseline responses."; Benchmarking context: "Target: 30% improvement, Your model: 41% improvement (37% above target), Industry average: 28% improvement"; Use case scenarios: "This level of emotional intelligence enables your AI to: Handle anxious clients during market downturns, Build trust with skeptical new clients, Provide appropriate reassurance without minimizing concerns, De-escalate tense situations professionally"; ROI connection: "Higher EI = Higher client satisfaction = Better retention = Increased lifetime value"
    - **Integration with Perplexity Results**: Combined quality dashboard shows: Technical Quality (Perplexity): 31% improvement, Emotional Quality (EI): 41% improvement, Overall Quality Score: (31 + 41) / 2 = 36% (weighted average or custom formula); Holistic quality badge: "âœ“ Exceptional Quality (36% overall improvement)", Production readiness: Both perplexity AND EI meet thresholds â†’ green light for deployment; If one metric below threshold: "âš  Mixed Quality: Perplexity excellent (31%) but EI needs improvement (18%). Consider retraining with emotionally-focused conversations."

- **FR6.2.2:** Emotional Intelligence Regression Detection
  * Description: System shall implement comprehensive regression detection by comparing trained model EI scores against baseline scores scenario-by-scenario, identifying instances where training degraded performance, classifying regression severity (minor <10%, moderate 10-20%, major >20%), analyzing regression patterns across personas and categories, implementing quality gates blocking delivery for excessive regressions, generating root cause analysis reports, providing corrective action recommendations, and maintaining regression audit trails to protect against catastrophic forgetting, ensure training improvements are genuine across all scenarios, and prevent deployment of models with degraded emotional intelligence in critical areas.
  * Impact Weighting: Quality Assurance / Risk Mitigation / Client Protection
  * Priority: Medium
  * User Stories: US6.2.2
  * User Journey: UJ5.4.1 (Regression Detection), UJ5.4.2 (Quality Gate Enforcement)
  * Tasks: [T-6.2.2]
  * User Story Acceptance Criteria:
    - Regression detection: Identify scenarios where trained_score < baseline_score
    - **Regression Report**:
    - Number of regressions: 3 of 50 scenarios (6%)
    - List affected scenarios with details
    - Example: "Scenario #23: Baseline 4.2/5, Trained 3.8/5 (-10% regression)"
    - **Root Cause Analysis**:
    - Identify patterns: Which personas? Which emotional arcs? Which topics?
    - Example: "2 of 3 regressions involve 'Pragmatic Skeptic' persona - may need more training data"
    - **Severity Classification**:
    - Minor regression: <10% decrease, overall score still â‰¥4/5
    - Moderate regression: 10-20% decrease, score drops below 4/5
    - Major regression: >20% decrease, score drops below 3/5
    - **Quality Gate**:
    - Allow delivery if: <10% scenarios show regression, no major regressions
    - Block delivery if: â‰¥10% scenarios show regression or any major regression
    - Warning: "3 minor regressions detected. Review before client delivery."
    - **Corrective Actions**:
    - Add more training data for affected scenarios
    - Adjust hyperparameters (reduce learning rate, increase regularization)
    - Retry training with different configuration
    - Include regression analysis in validation report
  * Functional Requirements Acceptance Criteria:
    - **Regression Detection Logic**: For each of 50 EI scenarios: Calculate score_delta = trained_score - baseline_score, IF score_delta < 0: Flag as regression, Calculate regression_pct = (ABS(score_delta) / baseline_score) Ã— 100, Store: INSERT ei_regressions (job_id, scenario_id, baseline_score, trained_score, regression_pct, severity); Count total regressions: regression_count / 50 scenarios = regression_rate
    - **Severity Classification**: For each regression: IF regression_pct < 10 AND trained_score >= 4.0: severity = 'minor', risk = 'low', message = "Minor regression, overall quality still high"; ELSIF regression_pct >= 10 AND regression_pct < 20: severity = 'moderate', risk = 'medium', message = "Moderate regression, review scenario before deployment"; ELSIF regression_pct >= 20 OR trained_score < 3.0: severity = 'major', risk = 'high', message = "Major regression, critical quality degradation detected"; Store severity classification
    - **Regression Report Display**: Regression Summary Card on validation dashboard: Header: "Regression Detection", Total regressions: "3 of 50 scenarios (6%)", Severity breakdown: "Minor: 2, Moderate: 1, Major: 0", Overall status badge: Green if <10% and no major, Yellow if 10-15% or 1 moderate, Red if >15% or any major; Detailed Regression List (table): Scenario ID | Category | Baseline Score | Trained Score | Regression % | Severity; Example rows: "#23 | Empathy | 4.2/5 | 3.8/5 | -10% | Moderate ðŸŸ¡", "#37 | Conflict | 3.9/5 | 3.6/5 | -8% | Minor ðŸŸ¢", "#42 | Supportive | 4.1/5 | 3.9/5 | -5% | Minor ðŸŸ¢"; Click scenario row expands details: Scenario prompt, Baseline response, Trained response, Score breakdown per dimension, Regression reasoning: "Trained response less empathetic, more clinical tone"
    - **Root Cause Pattern Analysis**: Aggregate regressions by category: Query: SELECT category, COUNT(*) as regression_count FROM ei_regressions GROUP BY category ORDER BY regression_count DESC; Example results: "Conflict Handling: 2 regressions (67% of all regressions)", "Empathy Detection: 1 regression", Insight: "Most regressions in Conflict Handling category suggests model struggles with this scenario type. May need more conflict-focused training data."; Aggregate by persona: "Pragmatic Skeptic: 2 regressions (out of 12 scenarios with this persona = 17% regression rate)", "Anxious Investor: 1 regression (out of 15 scenarios = 7%)", Insight: "Pragmatic Skeptic scenarios show 2.4Ã— higher regression rate than average. Model may be overfitting to other personas at expense of Skeptics."; Aggregate by difficulty: "Hard scenarios: 0 regressions", "Medium: 2 regressions", "Easy: 1 regression", Insight: "Regressions mostly in medium-difficulty scenarios. May indicate model overcomplicates simple situations."
    - **Quality Gate Enforcement**: Quality gate rules: PASS (Green Light - Allow Delivery): regression_rate < 10% (fewer than 5 of 50 scenarios), AND no major regressions, AND overall_ei_improvement >= 20%; WARN (Yellow Light - Requires Review): regression_rate >= 10% AND < 15%, OR 1-2 moderate regressions, OR overall_ei_improvement 15-19%; BLOCK (Red Light - Block Delivery): regression_rate >= 15% (8+ scenarios regressed), OR any major regression (>20% degradation), OR overall_ei_improvement < 15%; Quality gate display: Banner at top of validation results: "âœ“ Quality Gate: PASSED - Model approved for delivery" (green), "âš  Quality Gate: REVIEW REQUIRED - 3 minor regressions detected. Team review recommended before client delivery." (yellow), "âœ— Quality Gate: BLOCKED - Excessive regressions detected (8 scenarios). Model not approved for production." (red); If BLOCKED: Disable "Approve for Delivery" button, Require: addressing regressions, or manager override with justification; If WARN: "Approve for Delivery" enabled but requires acknowledgment: "â˜ I have reviewed the 3 regression scenarios and approve delivery despite minor quality concerns"
    - **Corrective Action Recommendations**: System generates specific recommendations based on regression patterns: **Recommendation 1: Training Data Augmentation**: "Add 10-15 more '{category}' scenarios to training data, focusing on '{persona}' client type (2 of 3 regressions involve this combination)"; **Recommendation 2: Hyperparameter Adjustment**: "Current learning rate (0.0003) may be too high, causing overfitting to certain patterns. Retry with Conservative preset (lr=0.0002) to reduce overfitting."; **Recommendation 3: Training Data Quality Review**: "Review quality of existing '{category}' training conversations. Regressions may indicate inconsistent or contradictory training examples."; **Recommendation 4: Extended Training**: "Model shows good overall improvement but some regressions. Consider adding 1 additional epoch with reduced learning rate for fine-tuning."; **Recommendation 5: Accept Trade-off**: "Overall improvement (41%) far exceeds minor regressions (3 scenarios, -5-10%). Trade-off acceptable for production deployment. Monitor real-world performance."; Each recommendation includes: Issue description, Root cause hypothesis, Specific action, Expected impact, Priority (critical/high/medium/low)
    - **Regression Trend Tracking**: Store regression data across multiple training runs: INSERT ei_regression_history (training_file_id, job_id, regression_count, regression_rate, major_regression_count, timestamp); Track trends: "Previous run: 2 regressions (4%), This run: 3 regressions (6%) - increased", "After adding 10 Pragmatic Skeptic conversations: Pragmatic Skeptic regression rate 17% â†’ 8% (improved)"; Display trend chart: X-axis: Job date, Y-axis: Regression rate %, Line shows regression rate over time, Target line at 10% threshold; Insights: "Regression rate increasing over time may indicate training data quality declining or model complexity increasing.", "Successful mitigation: Adding targeted training data reduced regressions from 12% to 6%"
    - **Regression Audit Trail**: All regressions logged for compliance and analysis: Table: ei_regression_audit with fields: job_id, scenario_id, detected_at, severity, reviewed_by (user_id), review_notes, resolution_action, resolved_at; Manager review workflow: If WARN or BLOCK status: Assign to quality manager for review, Manager reviews regressed scenarios: Views baseline vs trained responses, Assesses whether regression is acceptable trade-off, Documents decision: "Regression acceptable - overall EI improvement (41%) outweighs 3 minor regressions. Approved for delivery.", Or: "Regression unacceptable - retrain with corrective actions applied."; Audit trail shows: Who approved delivery despite regressions, What justification provided, When approval granted
    - **Catastrophic Forgetting Detection**: Extreme case: If overall_ei_score decreases (trained < baseline): Flag as "Catastrophic Forgetting - Training Degraded Model Quality", Display: "ðŸš¨ Critical Issue: Trained model performs worse than baseline (3.2 â†’ 2.8, -13% overall)", Automatic BLOCK status, no override allowed, Require complete retraining: "This model cannot be deployed. Training process may have issues: Excessive learning rate, Contradictory training data, Insufficient training data, Model capacity too small"; Rare but critical check to prevent deploying worse-than-baseline models
    - **Export Regression Report**: Include in validation PDF report: Regression Summary section with counts and severity breakdown, Detailed Regression List table with all regressed scenarios, Root Cause Analysis with identified patterns, Corrective Action Recommendations prioritized list, Quality Gate Status and approval/block decision; CSV export: regression_details.csv with all regressed scenarios and scores; Use in continuous improvement: "Regression Analysis Report: Pattern identified - Conflict Handling scenarios consistently show higher regression rates across multiple training runs. Action: Create specialized Conflict Handling training module with 25+ scenarios."

- **FR6.3.1:** Financial Knowledge Retention Test
  * Description: System shall implement comprehensive financial knowledge retention testing using 100-question curated test suite covering taxes, retirement, investing, and insurance across difficulty levels, evaluating both baseline and trained models with objective multiple-choice grading, calculating retention rates as ratio of trained accuracy to baseline accuracy, applying threshold classifications (passed â‰¥95%, warning 90-94%, failed <90%), analyzing failed questions to identify knowledge gaps, implementing quality gates blocking deployment for retention below 90%, and providing corrective action recommendations to prevent catastrophic forgetting and ensure fine-tuning enhances domain-specific capabilities without degrading foundational financial knowledge.
  * Impact Weighting: Quality Assurance / Risk Mitigation / Client Trust
  * Priority: Medium
  * User Stories: US6.3.1
  * User Journey: UJ5.5.1 (Knowledge Retention Testing), UJ5.5.2 (Catastrophic Forgetting Prevention)
  * Tasks: [T-6.3.1]
  * User Story Acceptance Criteria:
    - **Test Suite**: 100 financial knowledge questions
    - Categories: Taxes (25), Retirement planning (25), Investing (25), Insurance (25)
    - Difficulty: Basic (40), Intermediate (40), Advanced (20)
    - Multiple choice format (A/B/C/D) for objective grading
    - **Validation Process**:
    - Run baseline Llama 3 70B on all 100 questions
    - Run trained model on same 100 questions
    - Compare accuracy: Correct answers / Total questions
    - **Results Display**:
    - Baseline accuracy: 87% (87/100 correct)
    - Trained accuracy: 85% (85/100 correct)
    - Retention rate: **98%** (85/87 = 97.7%, rounds to 98%)
    - Verdict: "âœ“ Passed" (â‰¥95% retention threshold)
    - **Acceptable Thresholds**:
    - âœ“ Passed: â‰¥95% retention (trained accuracy â‰¥ 95% of baseline)
    - âš  Warning: 90-94% retention (minor knowledge loss)
    - âœ— Failed: <90% retention (catastrophic forgetting detected)
    - **Failed Questions Analysis**:
    - If retention <95%, list questions where trained model failed but baseline passed
    - Identify knowledge gaps: "5 retirement planning questions regressed"
    - Recommendations: "Retrain with lower learning rate to prevent overfitting"
    - **Quality Gate**:
    - Block delivery if retention <90%
    - Require review if retention 90-94%
    - Auto-approve if retention â‰¥95%
    - Include retention test results in validation report
    - Export detailed question-by-question results as CSV
  * Functional Requirements Acceptance Criteria:
    - **Knowledge Test Suite Management**: 100 financial knowledge questions stored in database: knowledge_test_questions table with fields (question_id, category, difficulty, question_text, option_a, option_b, option_c, option_d, correct_answer, explanation, source); Categories: Taxes (25 questions): Tax brackets, deductions, retirement account tax implications, Retirement Planning (25): 401k rules, Social Security, RMDs, withdrawal strategies, Investing (25): Asset allocation, risk management, diversification, market fundamentals, Insurance (25): Life insurance types, policy features, estate planning implications; Difficulty distribution: Basic (40): Fundamental concepts any financial professional should know, Intermediate (40): Practical application, nuanced understanding, Advanced (20): Complex scenarios, deep expertise; Multiple-choice format: 4 options (A/B/C/D), single correct answer, Objective grading (eliminates scoring subjectivity); Test suite curated by certified financial planners, validated for accuracy and relevance
    - **Baseline Model Evaluation**: Load baseline Llama 3 70B (no adapters), For each of 100 questions: Prompt format: "Answer the following financial question. Select the correct option (A, B, C, or D). Question: {question_text}, Options: A) {option_a}, B) {option_b}, C) {option_c}, D) {option_d}, Answer:", Extract model's selected option from response (parse for A/B/C/D), Compare to correct_answer, Store: baseline_answer, baseline_correct (boolean); Calculate baseline_accuracy = (correct_answers / 100) Ã— 100; Typical baseline: 85-90% for Llama 3 70B (strong foundational financial knowledge)
    - **Trained Model Evaluation**: Load trained model (base + LoRA adapters), Run identical 100 questions with same prompts, Extract trained_answer, trained_correct for each question, Calculate trained_accuracy = (correct_answers / 100) Ã— 100; Expected trained: 83-90% (should maintain similar accuracy, slight drop acceptable)
    - **Retention Rate Calculation**: Retention rate = (trained_accuracy / baseline_accuracy) Ã— 100, Example: Baseline 87%, Trained 85% â†’ Retention = (85 / 87) Ã— 100 = 97.7% â‰ˆ 98%; Interpretation: Retention rate measures how well model preserves baseline knowledge, 100% = perfect retention (same accuracy), >100% = improved knowledge (rare but possible), <90% = significant forgetting (problematic); Store: UPDATE training_jobs SET knowledge_baseline_accuracy = 87.0, knowledge_trained_accuracy = 85.0, knowledge_retention_rate = 97.7
    - **Threshold Classification**: IF retention_rate >= 95: status = 'passed', badge = "âœ“ Passed", color = green, message = "Model retains 95%+ of baseline financial knowledge. No catastrophic forgetting detected."; ELSIF retention_rate >= 90: status = 'warning', badge = "âš  Warning", color = yellow, message = "Minor knowledge loss detected (90-94% retention). Review before deployment."; ELSE: status = 'failed', badge = "âœ— Failed", color = red, message = "Significant knowledge loss detected (<90% retention). Catastrophic forgetting may have occurred."
    - **Results Display**: Knowledge Retention Card on validation dashboard: Header: "Financial Knowledge Retention Test", Status badge: Large badge with retention verdict; Accuracy Comparison section: Baseline Accuracy: "87/100 (87%)", Trained Accuracy: "85/100 (85%)", Difference: "â†“ 2 questions (-2%)" visual indicator; Retention Rate: Huge number: "98%" (color-coded: green if â‰¥95%, yellow if 90-94%, red if <90%), Target: "Target: â‰¥95% (passed)", Interpretation: "Model retained 98% of baseline financial knowledge"; Category Breakdown (4 sub-sections): Taxes: "Baseline 84% (21/25), Trained 80% (20/25), Retention 95%", Retirement: "Baseline 88% (22/25), Trained 84% (21/25), Retention 95%", Investing: "Baseline 88% (22/25), Trained 88% (22/25), Retention 100%", Insurance: "Baseline 88% (22/25), Trained 88% (22/25), Retention 100%"; Visual bar chart: 4 bars (one per category), Each shows baseline (gray) vs trained (green or red) accuracy
    - **Failed Questions Analysis**: If retention < 95%: Display "Knowledge Gap Analysis" section, List questions where trained_correct = false AND baseline_correct = true (regressions): Question ID, Category, Difficulty, Question (truncated), Baseline Answer, Trained Answer, Correct Answer; Example: "#23 | Taxes | Intermediate | Tax implications of Roth conversions... | C (correct) | B (incorrect) | C"; Count regressions by category: "5 retirement planning questions regressed (baseline correct, trained incorrect)", "2 tax questions regressed"; Identify patterns: "Most regressions in Intermediate Retirement questions", "Advanced questions: No regressions (model maintains deep knowledge)", "Basic questions: 3 regressions (concerning - should be easiest to retain)"; Root cause hypothesis: IF regress ions mostly in basic questions: "Model may be overthinking simple concepts. Consider more diverse training data.", IF regressions in specific category: "Retirement planning: 5 regressions. Training data may lack retirement diversity, causing overfitting to training topics.", IF regressions random: "Random pattern suggests overfitting. Retry with lower learning rate or more regularization."
    - **Corrective Action Recommendations**: Based on retention rate and patterns: IF retention < 90: Recommendation 1: "Retry training with Conservative preset (lower learning rate reduces overfitting)", Recommendation 2: "Add dropout/regularization to prevent catastrophic forgetting", Recommendation 3: "Include knowledge retention examples in training data (mix general financial Qs with Elena-specific conversations)"; IF retention 90-94% AND category-specific regression: "Add 10-15 training conversations covering {regressed_category} topics", "{Category} retention only 88%. Balance training data to include more {category} scenarios."; IF retention 95-97%: "Model passes threshold but slight improvement possible. Consider: Adding knowledge-preserving loss term, Mixing in general financial Q&A during training"; Each recommendation includes expected impact and implementation guidance
    - **Quality Gate Enforcement**: Quality gate rules: PASS (â‰¥95% retention): Auto-approve, no manager review needed, Model safe for deployment; WARN (90-94% retention): Require manager review and approval, Manager assesses: Severity of knowledge gaps, Business criticality of regressed topics, Overall model improvement (perplexity + EI) vs slight knowledge loss, Manager approves or rejects deployment; BLOCK (<90% retention): Automatic deployment block, Cannot override (too risky to deploy model with significant knowledge loss), Must retrain with corrective actions; Quality gate banner displays retention status and approval requirements
    - **Export Knowledge Test Results**: CSV export: knowledge_retention_details.csv, Columns: question_id, category, difficulty, question_text, baseline_answer, trained_answer, correct_answer, baseline_correct, trained_correct, regression (boolean), explanation; All 100 questions with full details; PDF Report: Executive summary with retention rate and verdict, Category-level accuracy comparison table, Regressed questions list (if any) with explanations, Recommendations section, Appendix: Full 100-question results table; Include in validation artifact bundle
    - **Integration with Other Validation Metrics**: Combined quality scorecard: Perplexity Improvement: 31% âœ“, Emotional Intelligence: 41% âœ“, Knowledge Retention: 98% âœ“, Overall Quality: "Exceptional - All metrics pass thresholds"; If knowledge retention fails but others pass: "âš  Mixed Quality: Excellent EI and perplexity but knowledge retention concerns. Address before deployment."; Holistic quality decision requires all metrics passing their respective thresholds
    - **Knowledge Retention Trend Tracking**: Store historical data: INSERT knowledge_retention_history (job_id, training_file_id, baseline_accuracy, trained_accuracy, retention_rate, category_breakdown, timestamp); Track trends for training file: "Previous run: 96% retention, This run: 98% retention (â†‘ improved)", "After adjusting hyperparameters: Retention increased from 88% to 98%"; Identify configurations that preserve knowledge best: "Balanced preset: Average 97% retention across 12 jobs", "Aggressive preset: Average 91% retention (higher forgetting risk)"; Use insights to guide future training: "ðŸ’¡ Recommendation: Use Balanced preset for this training file - historically maintains 97% knowledge retention while achieving 32% perplexity improvement"

- **FR6.3.2:** Domain-Specific Knowledge Probes
  * Description: System shall implement client-customizable domain-specific knowledge validation accepting client-provided test suites (50-100 questions) covering regulatory compliance, product-specific knowledge, and business-specific requirements in CSV or JSON formats, running identical evaluations on baseline and trained models, calculating retention rates with elevated thresholds for compliance-critical content (100% retention required), flagging knowledge regressions in regulated areas, blocking deployment for compliance knowledge degradation, and providing specialized validation for financial regulations, proprietary products, and client-specific workflows to ensure models maintain business-critical accuracy while gaining conversational capabilities.
  * Impact Weighting: Quality Assurance / Compliance / Business-Specific Validation
  * Priority: Low (Future Enhancement - Client-Specific)
  * User Stories: US6.3.2
  * User Journey: UJ5.6.1 (Custom Knowledge Validation - Future)
  * Tasks: [T-6.3.2]
  * User Story Acceptance Criteria:
    - **Custom Test Suite**: Client-provided domain knowledge questions (50-100)
    - Financial regulations (SEC, FINRA rules)
    - Product-specific knowledge (401k vs Roth IRA differences)
    - Compliance requirements (disclosure language)
    - Allow clients to upload custom test suite (CSV or JSON format)
    - Run baseline vs trained model comparison
    - Report retention rate for domain-specific knowledge
    - Flag any regressions in critical knowledge areas
    - Example: "Compliance knowledge: 92% retention (acceptable threshold: 100% for regulatory content)"
    - If compliance knowledge drops <100%: Block delivery, require retraining
    - Use case: Ensure AI doesn't give incorrect tax advice or violate regulations after training
  * Functional Requirements Acceptance Criteria:
    - **Custom Test Suite Upload Interface**: Dashboard section: "Custom Knowledge Validation", Upload button: "Upload Custom Test Suite", Accepted formats: CSV (columns: question_id, category, criticality, question, option_a, option_b, option_c, option_d, correct_answer, explanation), JSON (structured array of question objects); Validation on upload: Check required fields present, Verify answer options valid (A/B/C/D), Confirm at least 20 questions (minimum for statistical validity); Store: custom_knowledge_tests table linked to client/team_id
    - **Criticality Classification**: Each question tagged with criticality level: Compliance (100% retention required): SEC/FINRA regulations, legal disclosures, fiduciary requirements, High (â‰¥98% retention): Product features, pricing, eligibility rules, Medium (â‰¥95% retention): Best practices, general procedures, Low (â‰¥90% retention): Nice-to-know information; Client defines criticality per question during upload; System enforces differential thresholds based on criticality
    - **Evaluation Process**: Runs automatically during finalization (after standard knowledge test), Load custom test suite for client/team, Evaluate baseline and trained models identically to FR6.3.1, Calculate per-category retention rates, Apply criticality-based thresholds; Quality gate: BLOCK if any compliance question fails (trained incorrect when baseline correct), WARN if high-criticality retention <98%, PASS if all thresholds met
    - **Compliance-Specific Reporting**: Separate "Compliance Knowledge" card on validation dashboard, Red/green status: "âœ“ All Compliance Knowledge Retained" or "âœ— Compliance Regression Detected", List any failed compliance questions with remediation requirements, Audit trail: Log all compliance evaluations for regulatory review; Export compliance-specific report for legal/compliance team review
    - **Use Cases**: Financial Firm: Upload 50 FINRA compliance questions ensuring AI doesn't violate regulations post-training, Insurance Company: Validate product knowledge retention (policy types, coverage limits, exclusions), Robo-Advisor: Test algorithmic trading rules and risk assessment knowledge, Tax Preparation Service: Verify current tax code knowledge maintained after personality training; Premium Feature: Available for Enterprise tier clients, Included in compliance-focused packages

- **FR6.4.1:** Elena Morales Voice Consistency Scoring
  * Description: System shall implement brand voice consistency evaluation using Elena Morales 10-characteristic rubric (warmth, directness, education-first, pragmatic optimism, question-driven, storytelling, action-oriented, patience, humor, confidence) by generating 30 diverse scenario responses from trained model, scoring each response across all characteristics using human evaluation or LLM-as-judge, calculating per-characteristic and overall averages, applying quality thresholds (excellent â‰¥90%, strong 85-89%, acceptable 80-84%), displaying before/after comparisons demonstrating voice improvement, flagging weak characteristics, and exporting detailed scorecards to validate brand alignment, ensure personality consistency, and provide client proof of brand voice acquisition.
  * Impact Weighting: Brand Alignment / Client Satisfaction / Quality Differentiation
  * Priority: Medium
  * User Stories: US6.4.1
  * User Journey: UJ5.7.1 (Brand Voice Validation)
  * Tasks: [T-6.4.1]
  * User Story Acceptance Criteria:
    - **Elena Morales Voice Rubric** (10 characteristics, each scored 1-5):
      1. Warmth & Empathy: Genuine emotional connection
      2. Directness & Clarity: Avoids jargon, gets to the point
      3. Education-First Approach: Explains "why" behind advice
      4. Pragmatic Optimism: Realistic yet hopeful tone
      5. Question-Driven: Asks clarifying questions
      6. Storytelling: Uses relatable examples
      7. Action-Oriented: Provides concrete next steps
      8. Patience: Never rushes or dismisses concerns
      9. Humor (appropriate): Light touches when suitable
      10. Confidence: Authoritative yet humble
    - **Evaluation Process**:
    - Generate 30 responses from trained model (diverse scenarios)
    - Human evaluators score each response on 10 characteristics
    - Calculate average score per characteristic
    - Calculate overall voice consistency: Average of 10 characteristic scores
    - **Results Display**:
    - Overall voice consistency: 4.3/5 (**86% alignment**, target â‰¥85%)
    - Per-characteristic breakdown:
    - Warmth & Empathy: 4.5/5 (excellent)
    - Directness: 4.2/5 (strong)
    - Education-First: 4.1/5 (good)
    - Pragmatic Optimism: 4.6/5 (excellent)
    - ... (remaining characteristics)
    - Flag characteristics scoring <3/5: "Humor: 2.8/5 (needs improvement)"
    - **Quality Badge**:
    - "âœ“ Excellent Brand Alignment" (â‰¥4.5/5, 90%+)
    - "âœ“ Strong Brand Alignment" (â‰¥4.25/5, 85-89%)
    - "âš  Acceptable Alignment" (â‰¥4.0/5, 80-84%)
    - "âœ— Needs Improvement" (<4.0/5, <80%)
    - **Before/After Examples**:
    - Show 5 responses demonstrating brand voice improvement
    - Baseline: Generic financial advice
    - Trained: Elena Morales style (warm, educational, action-oriented)
    - Include voice consistency report in validation PDF
    - Export detailed scoring (30 responses Ã— 10 characteristics) as CSV
  * Functional Requirements Acceptance Criteria:
    - **Voice Rubric Definition**: 10 Elena Morales voice characteristics stored in database with scoring criteria (1-5 scale) and evaluation guidelines; Each characteristic includes: Name, Description, Scoring rubric (what qualifies as 1/2/3/4/5), Example responses for each score level, Relative importance weight (all weighted equally for Elena, but customizable per client); Rubric validated by brand team and domain experts
    - **Scenario Generation for Evaluation**: Create 30 diverse test scenarios covering: All persona types (6 scenarios each for 5 personas), All emotional arcs (5 scenarios each for 6 arcs), Various topics (retirement, taxes, investing, insurance, debt, estate), Difficulty range (easy, medium, hard), Edge cases (difficult clients, ambiguous situations, emotionally charged); Scenarios designed to elicit brand voice characteristics (e.g., scenario requiring patience, scenario requiring storytelling)
    - **Response Generation**: Load trained model (base + LoRA adapters), Generate response for each of 30 scenarios, Temperature: 0.8 (allows personality to show), Max tokens: 300 (full response), Store generated responses; Baseline comparison optional: Generate baseline responses for side-by-side comparison, Demonstrates voice acquisition (generic â†’ Elena-specific)
    - **Evaluation Method**: **Option 1 - Human Evaluation**: 2-3 human evaluators (brand team members, domain experts) independently score each response, Score all 10 characteristics per response (300 total scores per evaluator), Average across evaluators for final scores, Inter-rater reliability calculated; **Option 2 - LLM-as-Judge** (faster, more scalable): Use GPT-4/Claude as evaluator, Provide rubric and scoring criteria in prompt, Request JSON-formatted scores with reasoning, Human validation sampling (10 responses) to verify LLM accuracy; Hybrid approach: LLM scores all 30, human reviews 10 for validation
    - **Score Aggregation**: Per-characteristic average: warmth_avg = AVG(warmth_scores across 30 responses), Repeat for all 10 characteristics, Overall voice consistency score = AVG(all 10 characteristic averages), Percentage alignment = (overall_score / 5.0) Ã— 100; Store: UPDATE training_jobs SET voice_consistency_score = 4.3, voice_alignment_pct = 86, voice_characteristic_scores = {warmth: 4.5, directness: 4.2, ...}
    - **Results Display**: Brand Voice Consistency Card on validation dashboard, Header: "Elena Morales Voice Alignment", Overall Score: "4.3/5 (86% alignment)" large, prominent, Quality badge based on thresholds; Radar Chart Visualization: 10-axis radar (one per characteristic), Scores plotted on each axis (1-5 scale), Filled area shows voice profile, Target line at 4.0 (80% threshold); Characteristic Breakdown Table: 10 rows (one per characteristic), Columns: Characteristic | Avg Score | Rating | Status, Example: "Warmth & Empathy | 4.5/5 | Excellent | âœ“", "Humor | 2.8/5 | Needs Work | âš "; Flag weak characteristics (<3.0): Highlighted in red, "âš  Humor characteristic below threshold. Consider adding humorous training examples."
    - **Quality Tier Classification**: IF voice_consistency_score >= 4.5 (90%): tier = 'excellent', badge = "âœ“ Exceptional Brand Alignment"; ELSIF >= 4.25 (85%): tier = 'strong', badge = "âœ“ Strong Brand Alignment"; ELSIF >= 4.0 (80%): tier = 'acceptable', badge = "âš  Acceptable Alignment"; ELSE: tier = 'needs_improvement', badge = "âœ— Needs Improvement"
    - **Before/After Examples**: Display 5 best voice improvements, Side-by-side comparison: Scenario prompt, Baseline response (generic), Trained response (Elena voice), Voice characteristics demonstrated highlighted, Score improvement shown; Example: Scenario: "Client worried about market downturn", Baseline (3.2/5): "Market corrections are normal. Diversify your portfolio and stay invested long-term.", Trained (4.7/5): "I hear that worry - market dips can feel scary, especially when it's your hard-earned money at stake. Let me walk you through what's happening and why staying calm is your best move right now. Your diversified portfolio is actually built for moments like this..." Highlights: Warmth (validates emotion), Education-first (explains reasoning), Action-oriented (concrete guidance)
    - **Export Voice Consistency Data**: CSV: voice_consistency_details.csv, 30 responses Ã— 10 characteristics matrix, Columns: response_id, scenario, [characteristic_1...characteristic_10], overall_score; PDF Report: Elena Morales Voice Profile (radar chart), Overall alignment score and tier, Per-characteristic breakdown table, Top 5 before/after examples, Weak characteristics and recommendations, Brand team sign-off section; Include in validation artifact bundle for client delivery

- **FR6.4.2:** Client Brand Customization (Future)
  * Description: System shall implement customizable brand voice evaluation framework enabling clients to define proprietary voice rubrics (5-15 characteristics), upload characteristic descriptions with scoring criteria and examples, configure relative importance weights, run evaluations using client-specific rubrics, generate customized brand alignment reports, and validate diverse brand personalities (formal/informal, conservative/progressive, technical/accessible) to enable Bright Run platform serving multiple clients with distinct brand identities while maintaining consistent quality assurance methodology and providing white-label validation reports matching client brand guidelines.
  * Impact Weighting: Client Customization / Brand Protection / Competitive Differentiation
  * Priority: Low (Future Enhancement - Premium Feature)
  * User Stories: US6.4.2
  * User Journey: UJ5.8.1 (Custom Brand Definition - Future)
  * Tasks: [T-6.4.2]
  * User Story Acceptance Criteria:
    - Allow clients to define custom brand voice rubric (5-15 characteristics)
    - Client provides characteristic descriptions and scoring criteria
    - Example custom characteristics: "Conservative tone", "Risk-aware language", "Formal communication style"
    - Run evaluation using client's rubric
    - Generate brand alignment report customized to client's brand
    - Use case: Financial firm with formal, conservative brand (different from Elena Morales' warm, approachable style)
    - Enables: Bright Run to serve diverse clients with different brand personalities
  * Functional Requirements Acceptance Criteria:
    - **Custom Rubric Builder Interface**: Dashboard section: "Brand Voice Configuration", "Create Custom Rubric" button opens builder, Add characteristic form: Name (required), Description (required), Scoring criteria for 1-5 scale (required), Example responses for each score level (optional), Relative weight 1-10 (default: 5); Supports 5-15 characteristics (enforced range for validity), Save rubric linked to client/team account; Library of common characteristics with templates: Formality, Risk-awareness, Technical depth, Humor level, Urgency/pace, Authority tone, Client empowerment vs directive guidance
    - **Evaluation Execution**: Uses identical methodology as FR6.4.1 but with client's custom rubric, Generates 30 responses, Scores using custom characteristics, Calculates weighted average if weights specified, Produces brand-specific report with client logo and terminology; Example: Conservative Financial Firm rubric: "Risk Disclosure" (weight 10), "Formal Tone" (weight 8), "Regulatory Compliance Language" (weight 10), "Technical Accuracy" (weight 7), "Conservative Recommendations" (weight 9); Evaluation ensures model aligns with firm's conservative, compliance-focused brand vs Elena's warm, approachable style
    - **Use Cases**: Enterprise financial institutions with established brand guidelines, Robo-advisors with distinct market positioning, Insurance companies with formal compliance requirements, FinTech startups with casual, tech-forward voice; Premium feature: Available for Enterprise tier, Included in white-label packages, Enables Bright Run serving diverse client brands simultaneously
